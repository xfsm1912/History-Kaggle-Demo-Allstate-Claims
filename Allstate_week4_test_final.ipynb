{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Allstate_week4_test_20200417_BinWang.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uIXd1GVOugV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "# from sklearn import preprocessing, pipeline, metrics, grid_search, cross_validation\n",
        "from sklearn import preprocessing, pipeline, metrics\n",
        "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_validate, cross_val_score\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from scipy import sparse\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyFMPJkUQhk-",
        "colab_type": "text"
      },
      "source": [
        "grid_search: GridSearchCV \\\\\n",
        "cross_validation: cross_validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMba70kCO8QH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_mae(labels,preds,lift=200):\n",
        "    return mean_absolute_error(np.exp(labels)-lift, np.exp(preds)-lift)\n",
        "\n",
        "def logregobj(labels, preds):\n",
        "    con = 2\n",
        "    x =preds-labels\n",
        "    grad =con*x / (np.abs(x)+con)\n",
        "    hess =con**2 / (np.abs(x)+con)**2\n",
        "    return grad, hess \n",
        "\n",
        "\n",
        "log_mae_scorer = metrics.make_scorer(log_mae, greater_is_better = False)\n",
        "\n",
        "def search_model(train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
        "    ##Grid Search for the best model\n",
        "    model = GridSearchCV(estimator  = est,\n",
        "                                     param_grid = param_grid,\n",
        "                                     scoring    = log_mae_scorer,\n",
        "                                     verbose    = 10,\n",
        "                                     n_jobs  = n_jobs,\n",
        "                                     iid        = True,\n",
        "                                     refit    = refit,\n",
        "                                     cv      = cv)\n",
        "    # Fit Grid Search Model\n",
        "    model.fit(train_x, train_y)\n",
        "    print(\"params:\\n\")\n",
        "    print(model.cv_results_.__getitem__('params'))\n",
        "    print(\"mean test scores:\\n\")\n",
        "    print(model.cv_results_.__getitem__('mean_test_score'))\n",
        "    print(\"std test scores:\\n\")\n",
        "    print(model.cv_results_.__getitem__('std_test_score'))\n",
        "    print(\"Best score: %0.3f\" % model.best_score_)\n",
        "    print(\"Best parameters set:\", model.best_params_)\n",
        "    # print(\"Scores:\", model.grid_scores_)\n",
        "    print(\"**********************************************\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "def xg_eval_mae(yhat, dtrain, lift=200):\n",
        "    y = dtrain.get_label()\n",
        "    return 'mae', mean_absolute_error(np.exp(y)-lift, np.exp(yhat)-lift)\n",
        "\n",
        "def xgb_logregobj(preds, dtrain):\n",
        "    con = 2\n",
        "    labels = dtrain.get_label()\n",
        "    x =preds-labels\n",
        "    grad =con*x / (np.abs(x)+con)\n",
        "    hess =con**2 / (np.abs(x)+con)**2\n",
        "    return grad, hess\n",
        "\n",
        "\n",
        "def search_model_mae (train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
        "##Grid Search for the best model\n",
        "    model = GridSearchCV(estimator  = est,\n",
        "                          param_grid = param_grid,\n",
        "                          scoring    = 'neg_mean_absolute_error',\n",
        "                          verbose    = 10,\n",
        "                          n_jobs  = n_jobs,\n",
        "                          iid        = True,\n",
        "                          refit    = refit,\n",
        "                          cv      = cv)\n",
        "    # Fit Grid Search Model\n",
        "    model.fit(train_x, train_y)\n",
        "    print(\"Best score: %0.3f\" % model.best_score_)\n",
        "    print(\"Best parameters set:\", model.best_params_)\n",
        "    # print(\"Scores:\", model.grid_scores_)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-_fwxo6ck9q",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYfH1lXacug8",
        "colab_type": "code",
        "outputId": "82045467-7461-4334-814c-4620e9dd895e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DV33cCjculk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/Kaggle_Allstate/data/train.csv /home/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqN3Yju0dbyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/Kaggle_Allstate/data/test.csv /home/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvo1O_Vucjgm",
        "colab_type": "code",
        "outputId": "11fef41e-41a4-4c75-a3cc-d1b067480f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load data\n",
        "start = time.time() \n",
        "train_data = pd.read_csv('/data/train.csv')\n",
        "train_size=train_data.shape[0]\n",
        "print (\"Loading train data finished in %0.3fs\" % (time.time() - start))        \n",
        "\n",
        "test_data = pd.read_csv('/data/test.csv')\n",
        "print (\"Loading test data finished in %0.3fs\" % (time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading train data finished in 2.776s\n",
            "Loading test data finished in 4.473s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64YxhaBucjvU",
        "colab_type": "code",
        "outputId": "9ab51847-cead-4805-f315-bc0f1f33a282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "train_data.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>cat1</th>\n",
              "      <th>cat2</th>\n",
              "      <th>cat3</th>\n",
              "      <th>cat4</th>\n",
              "      <th>cat5</th>\n",
              "      <th>cat6</th>\n",
              "      <th>cat7</th>\n",
              "      <th>cat8</th>\n",
              "      <th>cat9</th>\n",
              "      <th>cat10</th>\n",
              "      <th>cat11</th>\n",
              "      <th>cat12</th>\n",
              "      <th>cat13</th>\n",
              "      <th>cat14</th>\n",
              "      <th>cat15</th>\n",
              "      <th>cat16</th>\n",
              "      <th>cat17</th>\n",
              "      <th>cat18</th>\n",
              "      <th>cat19</th>\n",
              "      <th>cat20</th>\n",
              "      <th>cat21</th>\n",
              "      <th>cat22</th>\n",
              "      <th>cat23</th>\n",
              "      <th>cat24</th>\n",
              "      <th>cat25</th>\n",
              "      <th>cat26</th>\n",
              "      <th>cat27</th>\n",
              "      <th>cat28</th>\n",
              "      <th>cat29</th>\n",
              "      <th>cat30</th>\n",
              "      <th>cat31</th>\n",
              "      <th>cat32</th>\n",
              "      <th>cat33</th>\n",
              "      <th>cat34</th>\n",
              "      <th>cat35</th>\n",
              "      <th>cat36</th>\n",
              "      <th>cat37</th>\n",
              "      <th>cat38</th>\n",
              "      <th>cat39</th>\n",
              "      <th>...</th>\n",
              "      <th>cat92</th>\n",
              "      <th>cat93</th>\n",
              "      <th>cat94</th>\n",
              "      <th>cat95</th>\n",
              "      <th>cat96</th>\n",
              "      <th>cat97</th>\n",
              "      <th>cat98</th>\n",
              "      <th>cat99</th>\n",
              "      <th>cat100</th>\n",
              "      <th>cat101</th>\n",
              "      <th>cat102</th>\n",
              "      <th>cat103</th>\n",
              "      <th>cat104</th>\n",
              "      <th>cat105</th>\n",
              "      <th>cat106</th>\n",
              "      <th>cat107</th>\n",
              "      <th>cat108</th>\n",
              "      <th>cat109</th>\n",
              "      <th>cat110</th>\n",
              "      <th>cat111</th>\n",
              "      <th>cat112</th>\n",
              "      <th>cat113</th>\n",
              "      <th>cat114</th>\n",
              "      <th>cat115</th>\n",
              "      <th>cat116</th>\n",
              "      <th>cont1</th>\n",
              "      <th>cont2</th>\n",
              "      <th>cont3</th>\n",
              "      <th>cont4</th>\n",
              "      <th>cont5</th>\n",
              "      <th>cont6</th>\n",
              "      <th>cont7</th>\n",
              "      <th>cont8</th>\n",
              "      <th>cont9</th>\n",
              "      <th>cont10</th>\n",
              "      <th>cont11</th>\n",
              "      <th>cont12</th>\n",
              "      <th>cont13</th>\n",
              "      <th>cont14</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>B</td>\n",
              "      <td>C</td>\n",
              "      <td>E</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>B</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>I</td>\n",
              "      <td>E</td>\n",
              "      <td>G</td>\n",
              "      <td>J</td>\n",
              "      <td>G</td>\n",
              "      <td>BU</td>\n",
              "      <td>BC</td>\n",
              "      <td>C</td>\n",
              "      <td>AS</td>\n",
              "      <td>S</td>\n",
              "      <td>A</td>\n",
              "      <td>O</td>\n",
              "      <td>LB</td>\n",
              "      <td>0.726300</td>\n",
              "      <td>0.245921</td>\n",
              "      <td>0.187583</td>\n",
              "      <td>0.789639</td>\n",
              "      <td>0.310061</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.335060</td>\n",
              "      <td>0.30260</td>\n",
              "      <td>0.67135</td>\n",
              "      <td>0.83510</td>\n",
              "      <td>0.569745</td>\n",
              "      <td>0.594646</td>\n",
              "      <td>0.822493</td>\n",
              "      <td>0.714843</td>\n",
              "      <td>2213.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>C</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "      <td>T</td>\n",
              "      <td>L</td>\n",
              "      <td>F</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>I</td>\n",
              "      <td>K</td>\n",
              "      <td>K</td>\n",
              "      <td>BI</td>\n",
              "      <td>CQ</td>\n",
              "      <td>A</td>\n",
              "      <td>AV</td>\n",
              "      <td>BM</td>\n",
              "      <td>A</td>\n",
              "      <td>O</td>\n",
              "      <td>DP</td>\n",
              "      <td>0.330514</td>\n",
              "      <td>0.737068</td>\n",
              "      <td>0.592681</td>\n",
              "      <td>0.614134</td>\n",
              "      <td>0.885834</td>\n",
              "      <td>0.438917</td>\n",
              "      <td>0.436585</td>\n",
              "      <td>0.60087</td>\n",
              "      <td>0.35127</td>\n",
              "      <td>0.43919</td>\n",
              "      <td>0.338312</td>\n",
              "      <td>0.366307</td>\n",
              "      <td>0.611431</td>\n",
              "      <td>0.304496</td>\n",
              "      <td>1283.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>C</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>L</td>\n",
              "      <td>O</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>E</td>\n",
              "      <td>F</td>\n",
              "      <td>H</td>\n",
              "      <td>F</td>\n",
              "      <td>A</td>\n",
              "      <td>AB</td>\n",
              "      <td>DK</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>AF</td>\n",
              "      <td>A</td>\n",
              "      <td>I</td>\n",
              "      <td>GK</td>\n",
              "      <td>0.261841</td>\n",
              "      <td>0.358319</td>\n",
              "      <td>0.484196</td>\n",
              "      <td>0.236924</td>\n",
              "      <td>0.397069</td>\n",
              "      <td>0.289648</td>\n",
              "      <td>0.315545</td>\n",
              "      <td>0.27320</td>\n",
              "      <td>0.26076</td>\n",
              "      <td>0.32446</td>\n",
              "      <td>0.381398</td>\n",
              "      <td>0.373424</td>\n",
              "      <td>0.195709</td>\n",
              "      <td>0.774425</td>\n",
              "      <td>3005.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>C</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "      <td>T</td>\n",
              "      <td>I</td>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>I</td>\n",
              "      <td>K</td>\n",
              "      <td>K</td>\n",
              "      <td>BI</td>\n",
              "      <td>CS</td>\n",
              "      <td>C</td>\n",
              "      <td>N</td>\n",
              "      <td>AE</td>\n",
              "      <td>A</td>\n",
              "      <td>O</td>\n",
              "      <td>DJ</td>\n",
              "      <td>0.321594</td>\n",
              "      <td>0.555782</td>\n",
              "      <td>0.527991</td>\n",
              "      <td>0.373816</td>\n",
              "      <td>0.422268</td>\n",
              "      <td>0.440945</td>\n",
              "      <td>0.391128</td>\n",
              "      <td>0.31796</td>\n",
              "      <td>0.32128</td>\n",
              "      <td>0.44467</td>\n",
              "      <td>0.327915</td>\n",
              "      <td>0.321570</td>\n",
              "      <td>0.605077</td>\n",
              "      <td>0.602642</td>\n",
              "      <td>939.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>H</td>\n",
              "      <td>D</td>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>A</td>\n",
              "      <td>P</td>\n",
              "      <td>F</td>\n",
              "      <td>J</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>E</td>\n",
              "      <td>K</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>H</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>Y</td>\n",
              "      <td>BM</td>\n",
              "      <td>A</td>\n",
              "      <td>K</td>\n",
              "      <td>CK</td>\n",
              "      <td>0.273204</td>\n",
              "      <td>0.159990</td>\n",
              "      <td>0.527991</td>\n",
              "      <td>0.473202</td>\n",
              "      <td>0.704268</td>\n",
              "      <td>0.178193</td>\n",
              "      <td>0.247408</td>\n",
              "      <td>0.24564</td>\n",
              "      <td>0.22089</td>\n",
              "      <td>0.21230</td>\n",
              "      <td>0.204687</td>\n",
              "      <td>0.202213</td>\n",
              "      <td>0.246011</td>\n",
              "      <td>0.432606</td>\n",
              "      <td>2763.85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 132 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id cat1 cat2 cat3 cat4  ...    cont11    cont12    cont13    cont14     loss\n",
              "0   1    A    B    A    B  ...  0.569745  0.594646  0.822493  0.714843  2213.18\n",
              "1   2    A    B    A    A  ...  0.338312  0.366307  0.611431  0.304496  1283.60\n",
              "2   5    A    B    A    A  ...  0.381398  0.373424  0.195709  0.774425  3005.09\n",
              "3  10    B    B    A    B  ...  0.327915  0.321570  0.605077  0.602642   939.85\n",
              "4  11    A    B    A    B  ...  0.204687  0.202213  0.246011  0.432606  2763.85\n",
              "\n",
              "[5 rows x 132 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJiTy5Yvd3IS",
        "colab_type": "text"
      },
      "source": [
        "## Merge train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wbC_reMcj-e",
        "colab_type": "code",
        "outputId": "83bfe60d-0afe-4880-dbc0-e31f506cac99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "full_data=pd.concat([train_data\n",
        "                       ,test_data])\n",
        "del( train_data, test_data)\n",
        "print (\"Full Data set created.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full Data set created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWMZPzEAd5Ty",
        "colab_type": "text"
      },
      "source": [
        "## Group features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvO4Swrtcjmw",
        "colab_type": "code",
        "outputId": "8c6ac5ee-9b8f-45af-e909-f92c5980ea2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "data_types = full_data.dtypes  \n",
        "cat_cols = list(data_types[data_types=='object'].index)\n",
        "num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
        "\n",
        "id_col = 'id'\n",
        "target_col = 'loss'\n",
        "num_cols.remove('id')\n",
        "num_cols.remove('loss')\n",
        "\n",
        "print (\"Categorical features:\", cat_cols)\n",
        "print ( \"Numerica features:\", num_cols)\n",
        "print ( \"ID: %s, target: %s\" %( id_col, target_col))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categorical features: ['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat30', 'cat31', 'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39', 'cat40', 'cat41', 'cat42', 'cat43', 'cat44', 'cat45', 'cat46', 'cat47', 'cat48', 'cat49', 'cat50', 'cat51', 'cat52', 'cat53', 'cat54', 'cat55', 'cat56', 'cat57', 'cat58', 'cat59', 'cat60', 'cat61', 'cat62', 'cat63', 'cat64', 'cat65', 'cat66', 'cat67', 'cat68', 'cat69', 'cat70', 'cat71', 'cat72', 'cat73', 'cat74', 'cat75', 'cat76', 'cat77', 'cat78', 'cat79', 'cat80', 'cat81', 'cat82', 'cat83', 'cat84', 'cat85', 'cat86', 'cat87', 'cat88', 'cat89', 'cat90', 'cat91', 'cat92', 'cat93', 'cat94', 'cat95', 'cat96', 'cat97', 'cat98', 'cat99', 'cat100', 'cat101', 'cat102', 'cat103', 'cat104', 'cat105', 'cat106', 'cat107', 'cat108', 'cat109', 'cat110', 'cat111', 'cat112', 'cat113', 'cat114', 'cat115', 'cat116']\n",
            "Numerica features: ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\n",
            "ID: id, target: loss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Qggc_pd9WD",
        "colab_type": "code",
        "outputId": "b6b85b75-e909-4940-ea28-e53db4541540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "LBL = preprocessing.LabelEncoder()\n",
        "start=time.time()\n",
        "for cat_col in cat_cols:\n",
        "#     print (\"Factorize feature %s\" % (cat))\n",
        "    full_data[cat_col] = LBL.fit_transform(full_data[cat_col])\n",
        "print ('Label enconding finished in %f seconds' % (time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label enconding finished in 18.921697 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSPNZ_0Id9e-",
        "colab_type": "code",
        "outputId": "62e27b52-5079-4ce2-e975-94e4fb010afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
        "start=time.time()\n",
        "full_data_sparse=OHE.fit_transform(full_data[cat_cols])\n",
        "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
        "\n",
        "print (full_data_sparse.shape)\n",
        "\n",
        "## it should be (313864, 1176)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One-hot-encoding finished in 3.006921 seconds\n",
            "(313864, 1176)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn1E4gxLeOcN",
        "colab_type": "code",
        "outputId": "a1642dc7-ce39-4a14-9a12-cc331e138546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from scipy.stats import skew, boxcox\n",
        "skewed_cols = full_data[num_cols].apply(lambda x: skew(x.dropna()))\n",
        "print (skewed_cols.sort_values())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cont2    -0.311146\n",
            "cont3    -0.007023\n",
            "cont14    0.250673\n",
            "cont11    0.281139\n",
            "cont12    0.291997\n",
            "cont10    0.352116\n",
            "cont13    0.376138\n",
            "cont4     0.417559\n",
            "cont6     0.458413\n",
            "cont1     0.513205\n",
            "cont8     0.673237\n",
            "cont5     0.679610\n",
            "cont7     0.825889\n",
            "cont9     1.067247\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGxfoG6feOlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
        "for skewed_col in skewed_cols:\n",
        "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmbWjHs1eUNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SSL = preprocessing.StandardScaler()\n",
        "# for num_col in num_cols:\n",
        "#     full_data[num_col] = full_data[num_col].values.reshape(-1, 1)\n",
        "#     full_data[num_col] = SSL.fit_transform(full_data[num_col])\n",
        "\n",
        "full_data[num_cols] = SSL.fit_transform(full_data[num_cols])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pDtydEYeUe7",
        "colab_type": "code",
        "outputId": "8d9cbd49-de0b-492c-b49a-b9bf709b1288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from scipy import sparse\n",
        "full_data_sparse = sparse.hstack((full_data_sparse,full_data[num_cols])).tocsr()\n",
        "print (full_data_sparse.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(313864, 1190)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCu3WPNEeUpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shift = 200\n",
        "full_cols = cat_cols + num_cols\n",
        "train_x = full_data_sparse[:train_size]\n",
        "test_x = full_data_sparse[train_size:]\n",
        "train_y = np.log(full_data[:train_size].loss.values + 200)\n",
        "ID = full_data.id[:train_size].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xGtwn7TeU1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Initialize data\n",
        "# lift = 200\n",
        "\n",
        "# full_cols = num_cols + cat_cols\n",
        "# train_x = full_data[full_cols][:train_size].values\n",
        "# test_x = full_data[full_cols][train_size:].values\n",
        "# train_y = np.log(full_data[:train_size].loss.values + lift)\n",
        "# ID = full_data.id[:train_size].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCxuj3-3lJTS",
        "colab_type": "code",
        "outputId": "2d678abc-fde4-4542-9a3a-03792701bd65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_x.shape)\n",
        "print(test_x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(188318, 1190)\n",
            "(125546, 1190)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVtGeMczlg6S",
        "colab_type": "text"
      },
      "source": [
        "## gridresearch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLvGpxO6lJ_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNcZ1l0lriG",
        "colab_type": "code",
        "outputId": "b313a449-de3a-4754-a201-c5f443f088b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# time consuming, use gpu to tune\n",
        "param_grid = {'objective':[logregobj],\n",
        "              'learning_rate':[0.01, 0.02, 0.04, 0.06, 0.08],\n",
        "              'n_estimators':[1500],\n",
        "              'max_depth': [9],\n",
        "              'min_child_weight':[50],\n",
        "              'subsample': [0.78],\n",
        "              'colsample_bytree':[0.67],\n",
        "              'gamma':[0.9],\n",
        "              'nthread': [-1],\n",
        "              'seed' : [1234]}\n",
        "\n",
        "model = search_model(train_x,\n",
        "                     train_y,\n",
        "                     xgb.XGBRegressor(tree_method='gpu_hist', gpu_id=0),\n",
        "                     param_grid,\n",
        "                     n_jobs = 1,\n",
        "                     cv = 4,\n",
        "                     refit = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.01, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:46:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.01, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1137.840, total=  53.3s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.01, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   53.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:47:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.01, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1141.557, total=  52.6s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.01, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.8min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:48:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.01, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1137.325, total=  52.6s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.01, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.6min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:49:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.01, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1136.673, total=  53.2s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.02, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  3.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:50:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.02, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1134.393, total=  50.8s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.02, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:51:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.02, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1139.094, total=  51.0s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.02, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  5.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:52:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.02, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1134.547, total=  52.7s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.02, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  6.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:53:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.02, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1133.511, total=  52.4s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.04, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  7.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:53:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.04, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1140.857, total=  51.9s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.04, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  7.8min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:54:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.04, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1146.169, total=  51.7s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.04, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n",
            "[12:55:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.04, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1139.953, total=  51.4s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.04, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n",
            "[12:56:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.04, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1140.862, total=  50.9s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.06, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n",
            "[12:57:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.06, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1151.583, total=  51.1s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.06, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n",
            "[12:58:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.06, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1154.851, total=  51.2s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.06, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n",
            "[12:59:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.06, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1152.099, total=  51.2s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.06, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n",
            "[12:59:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.06, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1150.131, total=  50.8s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.08, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n",
            "[13:00:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.08, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1162.135, total=  51.5s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.08, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n",
            "[13:01:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.08, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1162.181, total=  51.5s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.08, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n",
            "[13:02:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.08, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1158.940, total=  50.8s\n",
            "[CV] colsample_bytree=0.67, gamma=0.9, learning_rate=0.08, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78 \n",
            "[13:03:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV]  colsample_bytree=0.67, gamma=0.9, learning_rate=0.08, max_depth=9, min_child_weight=50, n_estimators=1500, nthread=-1, objective=<function logregobj at 0x7f2efe02c048>, seed=1234, subsample=0.78, score=-1159.648, total=  50.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 17.2min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
            "  \"removed in 0.24.\", FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[13:04:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "params:\n",
            "\n",
            "[{'colsample_bytree': 0.67, 'gamma': 0.9, 'learning_rate': 0.01, 'max_depth': 9, 'min_child_weight': 50, 'n_estimators': 1500, 'nthread': -1, 'objective': <function logregobj at 0x7f2efe02c048>, 'seed': 1234, 'subsample': 0.78}, {'colsample_bytree': 0.67, 'gamma': 0.9, 'learning_rate': 0.02, 'max_depth': 9, 'min_child_weight': 50, 'n_estimators': 1500, 'nthread': -1, 'objective': <function logregobj at 0x7f2efe02c048>, 'seed': 1234, 'subsample': 0.78}, {'colsample_bytree': 0.67, 'gamma': 0.9, 'learning_rate': 0.04, 'max_depth': 9, 'min_child_weight': 50, 'n_estimators': 1500, 'nthread': -1, 'objective': <function logregobj at 0x7f2efe02c048>, 'seed': 1234, 'subsample': 0.78}, {'colsample_bytree': 0.67, 'gamma': 0.9, 'learning_rate': 0.06, 'max_depth': 9, 'min_child_weight': 50, 'n_estimators': 1500, 'nthread': -1, 'objective': <function logregobj at 0x7f2efe02c048>, 'seed': 1234, 'subsample': 0.78}, {'colsample_bytree': 0.67, 'gamma': 0.9, 'learning_rate': 0.08, 'max_depth': 9, 'min_child_weight': 50, 'n_estimators': 1500, 'nthread': -1, 'objective': <function logregobj at 0x7f2efe02c048>, 'seed': 1234, 'subsample': 0.78}]\n",
            "mean test scores:\n",
            "\n",
            "[-1138.34881393 -1135.38630941 -1141.96013447 -1152.1660183\n",
            " -1160.72604273]\n",
            "std test scores:\n",
            "\n",
            "[1.89799198 2.17677319 2.45785789 1.71008846 1.45404226]\n",
            "Best score: -1135.386\n",
            "Best parameters set: {'colsample_bytree': 0.67, 'gamma': 0.9, 'learning_rate': 0.02, 'max_depth': 9, 'min_child_weight': 50, 'n_estimators': 1500, 'nthread': -1, 'objective': <function logregobj at 0x7f2efe02c048>, 'seed': 1234, 'subsample': 0.78}\n",
            "**********************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3M94S-slruy",
        "colab_type": "code",
        "outputId": "1c6371a4-f519-4958-a971-90a59fc4de0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "\n",
        "# here just an example, please use the optimal parameters from last gridsearch cell\n",
        "rgr = xgb.XGBRegressor(tree_method='gpu_hist',\n",
        "                       gpu_id=0,\n",
        "                      seed = 1234, \n",
        "                      learning_rate = 0.02, # smaller, better results, more time\n",
        "                      n_estimators = 1500, # Number of boosted trees to fit. \n",
        "                      max_depth=9, # the maximum depth of a tree\n",
        "                      min_child_weight=50,\n",
        "                      colsample_bytree=0.67, # the fraction of columns to be randomly samples for each tree\n",
        "                      subsample=0.78, # the fraction of observations to be randomly samples for each tree\n",
        "                      gamma=0.9, # Minimum loss reduction required to make a further partition on a leaf node of the tree, \n",
        "                       # the larger, the more conservative \n",
        "                      nthread = -1, # Number of parallel threads used to run xgboost.\n",
        "                      silent = False # Whether to print messages while running boosting.\n",
        "                      )\n",
        "rgr.fit(train_x, train_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14:09:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "             colsample_bynode=1, colsample_bytree=0.67, gamma=0.9, gpu_id=0,\n",
              "             importance_type='gain', learning_rate=0.02, max_delta_step=0,\n",
              "             max_depth=9, min_child_weight=50, missing=None, n_estimators=1500,\n",
              "             n_jobs=1, nthread=-1, objective='reg:linear', random_state=0,\n",
              "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=1234,\n",
              "             silent=False, subsample=0.78, tree_method='gpu_hist', verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr66Y4yUlr9m",
        "colab_type": "code",
        "outputId": "f9260bf1-c70b-4539-e0df-438c6b30d3fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_y = np.exp(rgr.predict(test_x)) - 200\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['id'] = full_data[train_size:].id\n",
        "results['loss'] = pred_y\n",
        "results.to_csv(\"/data/sub.csv\", index=False)\n",
        "print (\"Submission created.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submission created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px9uzkuglsqX",
        "colab_type": "code",
        "outputId": "65466426-bb6c-4f58-9503-5ffdf081d080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.bar(range(len(rgr.feature_importances_)), rgr.feature_importances_)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK75uQHpltOS",
        "colab_type": "code",
        "outputId": "64183a72-9f60-4590-eff0-bf5e94e2b965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "xgb.plot_importance(rgr,max_num_features=5,importance_type='weight')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2ec00e35c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asVkrV_lltZn",
        "colab_type": "code",
        "outputId": "4bd868b9-63bc-45af-8665-de55d2ad035d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.argsort(rgr.feature_importances_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 594,  467,  471, ..., 1047, 1051, 1053])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7t-dewAC4z3",
        "colab_type": "text"
      },
      "source": [
        "## Parameter tuning\n",
        "### XGBoost manual greedy tuning \n",
        "\n",
        "xgb max_depth must be >=0 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzU7YBQZO6lE",
        "colab_type": "code",
        "outputId": "50ee3ccd-f772-4d83-92fe-fc9b56e7c1e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "import copy\n",
        "kfolds = 5\n",
        "random_state = 42\n",
        "\n",
        "# skf = StratifiedKFold(n_splits=kfolds, shuffle=True, random_state=random_state)\n",
        "skf = KFold(n_splits=kfolds, random_state=1234)\n",
        "skf_ids = list(skf.split(train_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ePKHTIXy6a",
        "colab_type": "text"
      },
      "source": [
        "lgb vs xgb:\n",
        "bagging_freq = Not have this feature \\\\\n",
        "num_leaves = max_leaves \\\\\n",
        "max_depth = max_depth \\\\\n",
        "min_gain_to_split = gamma \\\\\n",
        "feature_fraction = colsample_bytree \\\\\n",
        "bagging_fraction = subsample \\\\\n",
        "min_sum_hessian_in_leaf = min_child_weight \\\\\n",
        "lambda_l2 = lambda \\\\\n",
        "lambda_l1 = alpha \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul1CIglBC4GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_xgb_params = {}\n",
        "default_xgb_params[\"learning_rate\"] = 0.05\n",
        "default_xgb_params[\"metrics\"] = 'rmse'\n",
        "default_xgb_params[\"subsample\"] = 1\n",
        "default_xgb_params[\"seed\"] = 1234\n",
        "default_xgb_params[\"objective\"] = 'reg:squarederror'\n",
        "default_xgb_params[\"tree_method\"] = 'gpu_hist'\n",
        "default_xgb_params[\"gpu_id\"] = 0\n",
        "\n",
        "\n",
        "params_xgb_space = {}\n",
        "params_xgb_space['max_leaves'] = [3, 7, 15, 31, 63, 127, 255] # the performance of =3 is equalt to others \n",
        "params_xgb_space['max_depth'] = [3 ,4 ,5 ,6 ,7 ,8] \n",
        "params_xgb_space['gamma'] = [0, 0.1, 0.3, 1, 1.5, 2, 3]\n",
        "params_xgb_space['colsample_bytree'] = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "params_xgb_space['subsample'] = [0.2, 0.4, 0.6, 0.8, 1]\n",
        "params_xgb_space['min_child_weight'] = [1, 5, 10, 30, 100]\n",
        "params_xgb_space['lambda'] = [0, 0.01, 0.1, 1, 10, 100]\n",
        "params_xgb_space['alpha'] = [0, 0.01, 0.1, 1, 10, 100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "753WdFV6DJjJ",
        "colab_type": "code",
        "outputId": "120e3423-4628-42b7-e4a5-c33fbcd4df85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "greater_is_better = False\n",
        "\n",
        "best_xgb_params = copy.copy(default_xgb_params)\n",
        "\n",
        "for p in params_xgb_space:\n",
        "    print (\"\\n Tuning parameter %s in %s\" % (p, params_xgb_space[p]))\n",
        "\n",
        "    params = best_xgb_params\n",
        "    scores = []    \n",
        "    for v in params_xgb_space[p]:\n",
        "        print ('\\n    %s: %s' % (p, v), end=\"\\n\")\n",
        "        params[p] = v\n",
        "        xgb_cv = xgb.cv(params,\n",
        "                        xgb.DMatrix(train_x,\n",
        "                                    label=train_y\n",
        "                                    ),\n",
        "                        num_boost_round=100000,\n",
        "                        nfold=kfolds,\n",
        "                        folds=list(skf.split(train_y)),\n",
        "                        stratified=False,\n",
        "                        early_stopping_rounds=50,\n",
        "                        verbose_eval=500)\n",
        "\n",
        "        best_xgb_score = min(xgb_cv['test-rmse-mean'])\n",
        "        best_xgb_iteration = len(xgb_cv['test-rmse-mean'])\n",
        "        print (', best_score: %f, best_iteration: %d' % (best_xgb_score, best_xgb_iteration))\n",
        "        scores.append([v, best_xgb_score])\n",
        "    # best param value in the space\n",
        "    best_param_value = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][0]\n",
        "    best_param_score = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][1]\n",
        "    best_xgb_params[p] = best_param_value\n",
        "    print (\"Best %s is %s with a score of %f\" %(p, best_param_value, best_param_score))\n",
        "\n",
        "print ('\\n Best manually tuned parameters:', best_xgb_params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Tuning parameter max_leaves in [3, 7, 15, 31, 63, 127, 255]\n",
            "\n",
            "    max_leaves: 3\n",
            "[0]\ttrain-rmse:6.97233+0.00038087\ttest-rmse:6.97235+0.00171519\n",
            "[500]\ttrain-rmse:0.507721+0.000434257\ttest-rmse:0.508607+0.00176234\n",
            "[1000]\ttrain-rmse:0.497247+0.000384104\ttest-rmse:0.498502+0.00188388\n",
            "[1500]\ttrain-rmse:0.493531+0.00031482\ttest-rmse:0.495256+0.00198871\n",
            "[2000]\ttrain-rmse:0.491486+0.000318544\ttest-rmse:0.493565+0.00204347\n",
            "[2500]\ttrain-rmse:0.489996+0.000341298\ttest-rmse:0.492374+0.00205447\n",
            "[3000]\ttrain-rmse:0.48884+0.000404373\ttest-rmse:0.491515+0.00209161\n",
            "[3500]\ttrain-rmse:0.487611+0.000619928\ttest-rmse:0.490569+0.00201428\n",
            "[4000]\ttrain-rmse:0.486636+0.000579663\ttest-rmse:0.489881+0.00211701\n",
            "[4500]\ttrain-rmse:0.485912+0.000651174\ttest-rmse:0.489394+0.00213503\n",
            "[5000]\ttrain-rmse:0.485276+0.000671661\ttest-rmse:0.488979+0.00205816\n",
            "[5500]\ttrain-rmse:0.484565+0.000668323\ttest-rmse:0.488482+0.00193314\n",
            "[6000]\ttrain-rmse:0.484025+0.000650182\ttest-rmse:0.488138+0.00195439\n",
            "[6500]\ttrain-rmse:0.483331+0.000504109\ttest-rmse:0.48766+0.00202745\n",
            "[7000]\ttrain-rmse:0.482717+0.000408593\ttest-rmse:0.48724+0.00207674\n",
            "[7500]\ttrain-rmse:0.482228+0.000337482\ttest-rmse:0.486946+0.00220487\n",
            "[8000]\ttrain-rmse:0.481811+0.000338226\ttest-rmse:0.486719+0.00222648\n",
            "[8500]\ttrain-rmse:0.481352+0.000335242\ttest-rmse:0.486452+0.00219164\n",
            "[9000]\ttrain-rmse:0.480984+0.000291293\ttest-rmse:0.486273+0.00221847\n",
            "[9500]\ttrain-rmse:0.480685+0.000281101\ttest-rmse:0.486138+0.00221849\n",
            "[10000]\ttrain-rmse:0.480311+0.000285491\ttest-rmse:0.48594+0.00222746\n",
            "[10500]\ttrain-rmse:0.479991+0.0003263\ttest-rmse:0.485786+0.00216818\n",
            "[11000]\ttrain-rmse:0.479673+0.000345987\ttest-rmse:0.485649+0.00215406\n",
            "[11500]\ttrain-rmse:0.479386+0.000368149\ttest-rmse:0.485543+0.00215392\n",
            "[12000]\ttrain-rmse:0.479019+0.000431799\ttest-rmse:0.48536+0.00213868\n",
            "[12500]\ttrain-rmse:0.478679+0.00039185\ttest-rmse:0.48518+0.00210001\n",
            "[13000]\ttrain-rmse:0.478354+0.000407841\ttest-rmse:0.48503+0.00215179\n",
            "[13500]\ttrain-rmse:0.478045+0.000395464\ttest-rmse:0.484894+0.00213837\n",
            "[14000]\ttrain-rmse:0.477716+0.000363448\ttest-rmse:0.484719+0.00215416\n",
            "[14500]\ttrain-rmse:0.47737+0.000422314\ttest-rmse:0.484519+0.00210345\n",
            "[15000]\ttrain-rmse:0.477096+0.000413851\ttest-rmse:0.48441+0.00210143\n",
            ", best_score: 0.484383, best_iteration: 15057\n",
            "\n",
            "    max_leaves: 7\n",
            "[0]\ttrain-rmse:6.97215+0.000381023\ttest-rmse:6.97215+0.00169251\n",
            "[500]\ttrain-rmse:0.488764+0.000622937\ttest-rmse:0.491672+0.00168101\n",
            "[1000]\ttrain-rmse:0.480907+0.000581192\ttest-rmse:0.486106+0.001814\n",
            "[1500]\ttrain-rmse:0.477033+0.000642342\ttest-rmse:0.484158+0.00182645\n",
            "[2000]\ttrain-rmse:0.474191+0.000510496\ttest-rmse:0.483038+0.00199281\n",
            "[2500]\ttrain-rmse:0.472057+0.000471158\ttest-rmse:0.482434+0.00205865\n",
            "[3000]\ttrain-rmse:0.470099+0.00050932\ttest-rmse:0.481924+0.00206918\n",
            "[3500]\ttrain-rmse:0.468386+0.000457741\ttest-rmse:0.481598+0.0021147\n",
            "[4000]\ttrain-rmse:0.466858+0.000512051\ttest-rmse:0.481362+0.0020808\n",
            "[4500]\ttrain-rmse:0.465447+0.000523219\ttest-rmse:0.481216+0.00210642\n",
            ", best_score: 0.481163, best_iteration: 4667\n",
            "\n",
            "    max_leaves: 15\n",
            "[0]\ttrain-rmse:6.972+0.000380891\ttest-rmse:6.97201+0.00170083\n",
            "[500]\ttrain-rmse:0.477349+0.000471442\ttest-rmse:0.484796+0.00212442\n",
            "[1000]\ttrain-rmse:0.468664+0.000460724\ttest-rmse:0.481379+0.00217881\n",
            "[1500]\ttrain-rmse:0.463294+0.000465362\ttest-rmse:0.480383+0.00221686\n",
            "[2000]\ttrain-rmse:0.458953+0.000503563\ttest-rmse:0.47989+0.00223921\n",
            ", best_score: 0.479850, best_iteration: 2073\n",
            "\n",
            "    max_leaves: 31\n",
            "[0]\ttrain-rmse:6.97191+0.000379552\ttest-rmse:6.97192+0.00170656\n",
            "[500]\ttrain-rmse:0.46614+0.000459506\ttest-rmse:0.481212+0.00203226\n",
            "[1000]\ttrain-rmse:0.455009+0.000375445\ttest-rmse:0.479353+0.00212921\n",
            "[1500]\ttrain-rmse:0.447216+0.000459636\ttest-rmse:0.478949+0.00219465\n",
            ", best_score: 0.478941, best_iteration: 1507\n",
            "\n",
            "    max_leaves: 63\n",
            "[0]\ttrain-rmse:6.97191+0.000379552\ttest-rmse:6.97192+0.00170656\n",
            "[500]\ttrain-rmse:0.455768+0.000371586\ttest-rmse:0.479939+0.00213578\n",
            "[1000]\ttrain-rmse:0.443642+0.000404211\ttest-rmse:0.478847+0.00222673\n",
            ", best_score: 0.478772, best_iteration: 1177\n",
            "\n",
            "    max_leaves: 127\n",
            "[0]\ttrain-rmse:6.97191+0.000379552\ttest-rmse:6.97192+0.00170656\n",
            "[500]\ttrain-rmse:0.455561+0.000285453\ttest-rmse:0.479798+0.00225483\n",
            "[1000]\ttrain-rmse:0.443657+0.000239968\ttest-rmse:0.478723+0.00235376\n",
            ", best_score: 0.478643, best_iteration: 1190\n",
            "\n",
            "    max_leaves: 255\n",
            "[0]\ttrain-rmse:6.97191+0.000379552\ttest-rmse:6.97192+0.00170656\n",
            "[500]\ttrain-rmse:0.455561+0.000285453\ttest-rmse:0.479798+0.00225483\n",
            "[1000]\ttrain-rmse:0.443657+0.000239968\ttest-rmse:0.478723+0.00235376\n",
            ", best_score: 0.478643, best_iteration: 1190\n",
            "Best max_leaves is 127 with a score of 0.478643\n",
            "\n",
            " Tuning parameter max_depth in [3, 4, 5, 6, 7, 8]\n",
            "\n",
            "    max_depth: 3\n",
            "[0]\ttrain-rmse:6.97209+0.000380231\ttest-rmse:6.9721+0.00169505\n",
            "[500]\ttrain-rmse:0.48668+0.000403452\ttest-rmse:0.490237+0.0018713\n",
            "[1000]\ttrain-rmse:0.479104+0.000463979\ttest-rmse:0.485286+0.00185967\n",
            "[1500]\ttrain-rmse:0.475248+0.000519143\ttest-rmse:0.48359+0.00185294\n",
            "[2000]\ttrain-rmse:0.472516+0.000473523\ttest-rmse:0.482716+0.00192644\n",
            "[2500]\ttrain-rmse:0.470284+0.000519698\ttest-rmse:0.482146+0.00192496\n",
            "[3000]\ttrain-rmse:0.468313+0.000514376\ttest-rmse:0.481738+0.00194393\n",
            "[3500]\ttrain-rmse:0.466505+0.000567732\ttest-rmse:0.481408+0.00196109\n",
            "[4000]\ttrain-rmse:0.464848+0.000536642\ttest-rmse:0.481189+0.0019696\n",
            ", best_score: 0.481125, best_iteration: 4147\n",
            "\n",
            "    max_depth: 4\n",
            "[0]\ttrain-rmse:6.97201+0.000379499\ttest-rmse:6.97202+0.00169483\n",
            "[500]\ttrain-rmse:0.476639+0.000445288\ttest-rmse:0.484372+0.00216464\n",
            "[1000]\ttrain-rmse:0.468562+0.000386629\ttest-rmse:0.481269+0.00225855\n",
            "[1500]\ttrain-rmse:0.463644+0.000446893\ttest-rmse:0.480335+0.00227868\n",
            "[2000]\ttrain-rmse:0.459769+0.00042834\ttest-rmse:0.479895+0.00233744\n",
            "[2500]\ttrain-rmse:0.456324+0.000490524\ttest-rmse:0.479612+0.00232484\n",
            "[3000]\ttrain-rmse:0.453301+0.000518923\ttest-rmse:0.479487+0.00231336\n",
            ", best_score: 0.479485, best_iteration: 2955\n",
            "\n",
            "    max_depth: 5\n",
            "[0]\ttrain-rmse:6.97195+0.000379658\ttest-rmse:6.97196+0.00169098\n",
            "[500]\ttrain-rmse:0.466846+0.000602303\ttest-rmse:0.481499+0.00199521\n",
            "[1000]\ttrain-rmse:0.457176+0.000416589\ttest-rmse:0.479603+0.00206991\n",
            "[1500]\ttrain-rmse:0.450701+0.000399668\ttest-rmse:0.479125+0.00207514\n",
            ", best_score: 0.478949, best_iteration: 1855\n",
            "\n",
            "    max_depth: 6\n",
            "[0]\ttrain-rmse:6.97191+0.000379552\ttest-rmse:6.97192+0.00170656\n",
            "[500]\ttrain-rmse:0.455561+0.000285453\ttest-rmse:0.479798+0.00225483\n",
            "[1000]\ttrain-rmse:0.443657+0.000239968\ttest-rmse:0.478723+0.00235376\n",
            ", best_score: 0.478643, best_iteration: 1190\n",
            "\n",
            "    max_depth: 7\n",
            "[0]\ttrain-rmse:6.97188+0.000379107\ttest-rmse:6.9719+0.00169514\n",
            "[500]\ttrain-rmse:0.442142+0.000315649\ttest-rmse:0.478846+0.00211458\n",
            ", best_score: 0.478522, best_iteration: 812\n",
            "\n",
            "    max_depth: 8\n",
            "[0]\ttrain-rmse:6.97186+0.000379625\ttest-rmse:6.97188+0.00168618\n",
            "[500]\ttrain-rmse:0.434659+0.000104197\ttest-rmse:0.47881+0.00211725\n",
            ", best_score: 0.478636, best_iteration: 753\n",
            "Best max_depth is 7 with a score of 0.478522\n",
            "\n",
            " Tuning parameter gamma in [0, 0.1, 0.3, 1, 1.5, 2, 3]\n",
            "\n",
            "    gamma: 0\n",
            "[0]\ttrain-rmse:6.97188+0.000379107\ttest-rmse:6.9719+0.00169514\n",
            "[500]\ttrain-rmse:0.442142+0.000315649\ttest-rmse:0.478846+0.00211458\n",
            ", best_score: 0.478522, best_iteration: 812\n",
            "\n",
            "    gamma: 0.1\n",
            "[0]\ttrain-rmse:6.97188+0.000379107\ttest-rmse:6.9719+0.00169514\n",
            "[500]\ttrain-rmse:0.442142+0.000315649\ttest-rmse:0.478846+0.00211458\n",
            ", best_score: 0.478522, best_iteration: 812\n",
            "\n",
            "    gamma: 0.3\n",
            "[0]\ttrain-rmse:6.97188+0.000379107\ttest-rmse:6.9719+0.00169514\n",
            "[500]\ttrain-rmse:0.442142+0.000315649\ttest-rmse:0.478846+0.00211458\n",
            ", best_score: 0.478522, best_iteration: 812\n",
            "\n",
            "    gamma: 1\n",
            "[0]\ttrain-rmse:6.97188+0.000379107\ttest-rmse:6.9719+0.00169514\n",
            "[500]\ttrain-rmse:0.442142+0.000315649\ttest-rmse:0.478846+0.00211458\n",
            ", best_score: 0.478522, best_iteration: 812\n",
            "\n",
            "    gamma: 1.5\n",
            "[0]\ttrain-rmse:6.97188+0.000379107\ttest-rmse:6.9719+0.00169514\n",
            "[500]\ttrain-rmse:0.442142+0.000315649\ttest-rmse:0.478846+0.00211458\n",
            ", best_score: 0.478522, best_iteration: 812\n",
            "\n",
            "    gamma: 2\n",
            "[0]\ttrain-rmse:6.97188+0.000379107\ttest-rmse:6.9719+0.00169514\n",
            "[500]\ttrain-rmse:0.442142+0.000315649\ttest-rmse:0.478846+0.00211458\n",
            ", best_score: 0.478522, best_iteration: 812\n",
            "\n",
            "    gamma: 3\n",
            "[0]\ttrain-rmse:6.97188+0.000379107\ttest-rmse:6.9719+0.00169514\n",
            "[500]\ttrain-rmse:0.442142+0.000315649\ttest-rmse:0.478846+0.00211458\n",
            ", best_score: 0.478522, best_iteration: 812\n",
            "Best gamma is 0 with a score of 0.478522\n",
            "\n",
            " Tuning parameter colsample_bytree in [0.1, 0.3, 0.5, 0.7, 0.9]\n",
            "\n",
            "    colsample_bytree: 0.1\n",
            "[0]\ttrain-rmse:6.97258+0.000377971\ttest-rmse:6.97258+0.0016006\n",
            "[500]\ttrain-rmse:0.453066+0.000559523\ttest-rmse:0.480078+0.00185013\n",
            "[1000]\ttrain-rmse:0.436576+0.000588203\ttest-rmse:0.477741+0.00196131\n",
            "[1500]\ttrain-rmse:0.426331+0.000499641\ttest-rmse:0.477358+0.00199359\n",
            ", best_score: 0.477352, best_iteration: 1489\n",
            "\n",
            "    colsample_bytree: 0.3\n",
            "[0]\ttrain-rmse:6.97238+0.000389819\ttest-rmse:6.97238+0.00161476\n",
            "[500]\ttrain-rmse:0.446231+0.000382136\ttest-rmse:0.478402+0.00232185\n",
            "[1000]\ttrain-rmse:0.432273+0.000466787\ttest-rmse:0.477428+0.00230477\n",
            ", best_score: 0.477359, best_iteration: 1087\n",
            "\n",
            "    colsample_bytree: 0.5\n",
            "[0]\ttrain-rmse:6.97194+0.000380683\ttest-rmse:6.97194+0.0016888\n",
            "[500]\ttrain-rmse:0.444198+0.000336723\ttest-rmse:0.478693+0.00232235\n",
            "[1000]\ttrain-rmse:0.429421+0.000442443\ttest-rmse:0.477891+0.00229444\n",
            ", best_score: 0.477812, best_iteration: 1262\n",
            "\n",
            "    colsample_bytree: 0.7\n",
            "[0]\ttrain-rmse:6.9719+0.000380632\ttest-rmse:6.97191+0.00167566\n",
            "[500]\ttrain-rmse:0.442913+0.000290484\ttest-rmse:0.478567+0.00228382\n",
            "[1000]\ttrain-rmse:0.427674+0.000760251\ttest-rmse:0.478029+0.00226218\n",
            ", best_score: 0.478017, best_iteration: 993\n",
            "\n",
            "    colsample_bytree: 0.9\n",
            "[0]\ttrain-rmse:6.9719+0.000380743\ttest-rmse:6.97191+0.00169062\n",
            "[500]\ttrain-rmse:0.442508+0.000621832\ttest-rmse:0.478723+0.00215084\n",
            ", best_score: 0.478360, best_iteration: 734\n",
            "Best colsample_bytree is 0.1 with a score of 0.477352\n",
            "\n",
            " Tuning parameter subsample in [0.2, 0.4, 0.6, 0.8, 1]\n",
            "\n",
            "    subsample: 0.2\n",
            "[0]\ttrain-rmse:6.97295+0.000451466\ttest-rmse:6.97296+0.00157403\n",
            "[500]\ttrain-rmse:0.464501+0.000585983\ttest-rmse:0.483575+0.00180963\n",
            ", best_score: 0.481976, best_iteration: 894\n",
            "\n",
            "    subsample: 0.4\n",
            "[0]\ttrain-rmse:6.9728+0.000369618\ttest-rmse:6.97281+0.00163547\n",
            "[500]\ttrain-rmse:0.459148+0.000496447\ttest-rmse:0.481655+0.00190501\n",
            "[1000]\ttrain-rmse:0.441877+0.000486455\ttest-rmse:0.479407+0.00198862\n",
            ", best_score: 0.479371, best_iteration: 1029\n",
            "\n",
            "    subsample: 0.6\n",
            "[0]\ttrain-rmse:6.97267+0.00038055\ttest-rmse:6.97268+0.00162998\n",
            "[500]\ttrain-rmse:0.456431+0.000443521\ttest-rmse:0.480786+0.00208907\n",
            "[1000]\ttrain-rmse:0.438043+0.000413296\ttest-rmse:0.478319+0.00213241\n",
            ", best_score: 0.477927, best_iteration: 1425\n",
            "\n",
            "    subsample: 0.8\n",
            "[0]\ttrain-rmse:6.97264+0.000370065\ttest-rmse:6.97265+0.00160645\n",
            "[500]\ttrain-rmse:0.454488+0.000534156\ttest-rmse:0.480426+0.00200102\n",
            "[1000]\ttrain-rmse:0.43552+0.000431676\ttest-rmse:0.477904+0.00210697\n",
            "[1500]\ttrain-rmse:0.421178+0.000717234\ttest-rmse:0.477334+0.00204425\n",
            ", best_score: 0.477307, best_iteration: 1478\n",
            "\n",
            "    subsample: 1\n",
            "[0]\ttrain-rmse:6.97258+0.000377971\ttest-rmse:6.97258+0.0016006\n",
            "[500]\ttrain-rmse:0.453066+0.000559523\ttest-rmse:0.480078+0.00185013\n",
            "[1000]\ttrain-rmse:0.436576+0.000588203\ttest-rmse:0.477741+0.00196131\n",
            "[1500]\ttrain-rmse:0.426331+0.000499641\ttest-rmse:0.477358+0.00199359\n",
            ", best_score: 0.477352, best_iteration: 1489\n",
            "Best subsample is 0.8 with a score of 0.477307\n",
            "\n",
            " Tuning parameter min_child_weight in [1, 5, 10, 30, 100]\n",
            "\n",
            "    min_child_weight: 1\n",
            "[0]\ttrain-rmse:6.97264+0.000370065\ttest-rmse:6.97265+0.00160645\n",
            "[500]\ttrain-rmse:0.454488+0.000534156\ttest-rmse:0.480426+0.00200102\n",
            "[1000]\ttrain-rmse:0.43552+0.000431676\ttest-rmse:0.477904+0.00210697\n",
            "[1500]\ttrain-rmse:0.421178+0.000717234\ttest-rmse:0.477334+0.00204425\n",
            ", best_score: 0.477307, best_iteration: 1478\n",
            "\n",
            "    min_child_weight: 5\n",
            "[0]\ttrain-rmse:6.97264+0.000370065\ttest-rmse:6.97265+0.00160645\n",
            "[500]\ttrain-rmse:0.457126+0.000589505\ttest-rmse:0.480252+0.00200317\n",
            "[1000]\ttrain-rmse:0.439874+0.000558867\ttest-rmse:0.477816+0.00206886\n",
            "[1500]\ttrain-rmse:0.426884+0.000814094\ttest-rmse:0.477329+0.00207154\n",
            ", best_score: 0.477315, best_iteration: 1478\n",
            "\n",
            "    min_child_weight: 10\n",
            "[0]\ttrain-rmse:6.97264+0.000370065\ttest-rmse:6.97265+0.00160645\n",
            "[500]\ttrain-rmse:0.458869+0.000536754\ttest-rmse:0.480181+0.00199128\n",
            "[1000]\ttrain-rmse:0.442779+0.000576274\ttest-rmse:0.477602+0.00205711\n",
            "[1500]\ttrain-rmse:0.430783+0.00072705\ttest-rmse:0.477082+0.00205108\n",
            ", best_score: 0.477068, best_iteration: 1478\n",
            "\n",
            "    min_child_weight: 30\n",
            "[0]\ttrain-rmse:6.97264+0.000370065\ttest-rmse:6.97265+0.00160645\n",
            "[500]\ttrain-rmse:0.462488+0.000435376\ttest-rmse:0.479985+0.00194095\n",
            "[1000]\ttrain-rmse:0.448403+0.000418515\ttest-rmse:0.477319+0.00195044\n",
            "[1500]\ttrain-rmse:0.438246+0.000540742\ttest-rmse:0.476641+0.00198636\n",
            ", best_score: 0.476565, best_iteration: 1665\n",
            "\n",
            "    min_child_weight: 100\n",
            "[0]\ttrain-rmse:6.97264+0.000370065\ttest-rmse:6.97265+0.00160645\n",
            "[500]\ttrain-rmse:0.467457+0.000334064\ttest-rmse:0.480328+0.00195001\n",
            "[1000]\ttrain-rmse:0.455972+0.00046395\ttest-rmse:0.477346+0.0019258\n",
            "[1500]\ttrain-rmse:0.44808+0.000456495\ttest-rmse:0.4765+0.00189844\n",
            ", best_score: 0.476264, best_iteration: 1894\n",
            "Best min_child_weight is 100 with a score of 0.476264\n",
            "\n",
            " Tuning parameter lambda in [0, 0.01, 0.1, 1, 10, 100]\n",
            "\n",
            "    lambda: 0\n",
            "[0]\ttrain-rmse:6.97252+0.000390467\ttest-rmse:6.97254+0.00157614\n",
            "[500]\ttrain-rmse:0.466939+0.000404683\ttest-rmse:0.480325+0.00197422\n",
            "[1000]\ttrain-rmse:0.455633+0.000475301\ttest-rmse:0.477396+0.00201789\n",
            "[1500]\ttrain-rmse:0.447725+0.000502284\ttest-rmse:0.476439+0.00199078\n",
            ", best_score: 0.476209, best_iteration: 1887\n",
            "\n",
            "    lambda: 0.01\n",
            "[0]\ttrain-rmse:6.97252+0.000390739\ttest-rmse:6.97254+0.00157566\n",
            "[500]\ttrain-rmse:0.46697+0.00036039\ttest-rmse:0.480317+0.00192231\n",
            "[1000]\ttrain-rmse:0.455509+0.000461686\ttest-rmse:0.477344+0.00194086\n",
            "[1500]\ttrain-rmse:0.447595+0.000469087\ttest-rmse:0.476448+0.00195451\n",
            ", best_score: 0.476217, best_iteration: 1889\n",
            "\n",
            "    lambda: 0.1\n",
            "[0]\ttrain-rmse:6.97254+0.000390842\ttest-rmse:6.97255+0.0015689\n",
            "[500]\ttrain-rmse:0.466996+0.000435683\ttest-rmse:0.480301+0.0019467\n",
            "[1000]\ttrain-rmse:0.455597+0.000494606\ttest-rmse:0.477345+0.00201615\n",
            "[1500]\ttrain-rmse:0.447702+0.000481911\ttest-rmse:0.476387+0.00202443\n",
            ", best_score: 0.476189, best_iteration: 1871\n",
            "\n",
            "    lambda: 1\n",
            "[0]\ttrain-rmse:6.97264+0.000370065\ttest-rmse:6.97265+0.00160645\n",
            "[500]\ttrain-rmse:0.467457+0.000334064\ttest-rmse:0.480328+0.00195001\n",
            "[1000]\ttrain-rmse:0.455972+0.00046395\ttest-rmse:0.477346+0.0019258\n",
            "[1500]\ttrain-rmse:0.44808+0.000456495\ttest-rmse:0.4765+0.00189844\n",
            ", best_score: 0.476264, best_iteration: 1894\n",
            "\n",
            "    lambda: 10\n",
            "[0]\ttrain-rmse:6.97326+0.000363861\ttest-rmse:6.97327+0.00163165\n",
            "[500]\ttrain-rmse:0.469198+0.000506828\ttest-rmse:0.480791+0.00179881\n",
            "[1000]\ttrain-rmse:0.457653+0.000568286\ttest-rmse:0.477444+0.00184116\n",
            "[1500]\ttrain-rmse:0.44972+0.000488658\ttest-rmse:0.476387+0.00187886\n",
            "[2000]\ttrain-rmse:0.443152+0.000486909\ttest-rmse:0.476086+0.00188171\n",
            ", best_score: 0.476043, best_iteration: 2144\n",
            "\n",
            "    lambda: 100\n",
            "[0]\ttrain-rmse:6.97356+0.000378655\ttest-rmse:6.97356+0.00161519\n",
            "[500]\ttrain-rmse:0.473716+0.00047212\ttest-rmse:0.482311+0.00171503\n",
            "[1000]\ttrain-rmse:0.462679+0.000513092\ttest-rmse:0.478117+0.00178409\n",
            "[1500]\ttrain-rmse:0.455251+0.000495491\ttest-rmse:0.476677+0.00176866\n",
            "[2000]\ttrain-rmse:0.449021+0.000486991\ttest-rmse:0.476108+0.0017508\n",
            "[2500]\ttrain-rmse:0.443464+0.000535904\ttest-rmse:0.475888+0.00169009\n",
            ", best_score: 0.475881, best_iteration: 2487\n",
            "Best lambda is 100 with a score of 0.475881\n",
            "\n",
            " Tuning parameter alpha in [0, 0.01, 0.1, 1, 10, 100]\n",
            "\n",
            "    alpha: 0\n",
            "[0]\ttrain-rmse:6.97356+0.000378655\ttest-rmse:6.97356+0.00161519\n",
            "[500]\ttrain-rmse:0.473716+0.00047212\ttest-rmse:0.482311+0.00171503\n",
            "[1000]\ttrain-rmse:0.462679+0.000513092\ttest-rmse:0.478117+0.00178409\n",
            "[1500]\ttrain-rmse:0.455251+0.000495491\ttest-rmse:0.476677+0.00176866\n",
            "[2000]\ttrain-rmse:0.449021+0.000486991\ttest-rmse:0.476108+0.0017508\n",
            "[2500]\ttrain-rmse:0.443464+0.000535904\ttest-rmse:0.475888+0.00169009\n",
            ", best_score: 0.475881, best_iteration: 2487\n",
            "\n",
            "    alpha: 0.01\n",
            "[0]\ttrain-rmse:6.97356+0.000378655\ttest-rmse:6.97356+0.00161519\n",
            "[500]\ttrain-rmse:0.473693+0.000448869\ttest-rmse:0.482282+0.00174694\n",
            "[1000]\ttrain-rmse:0.462741+0.000533747\ttest-rmse:0.478113+0.00178002\n",
            "[1500]\ttrain-rmse:0.455284+0.000506521\ttest-rmse:0.47667+0.00177859\n",
            "[2000]\ttrain-rmse:0.449042+0.000475291\ttest-rmse:0.476095+0.0017613\n",
            ", best_score: 0.475955, best_iteration: 2245\n",
            "\n",
            "    alpha: 0.1\n",
            "[0]\ttrain-rmse:6.97356+0.000378655\ttest-rmse:6.97356+0.00161519\n",
            "[500]\ttrain-rmse:0.473746+0.000479652\ttest-rmse:0.48232+0.00176262\n",
            "[1000]\ttrain-rmse:0.462722+0.000572959\ttest-rmse:0.478063+0.00180569\n",
            "[1500]\ttrain-rmse:0.455315+0.000514103\ttest-rmse:0.476606+0.00176293\n",
            "[2000]\ttrain-rmse:0.449101+0.000511883\ttest-rmse:0.476017+0.00168943\n",
            "[2500]\ttrain-rmse:0.443612+0.000572316\ttest-rmse:0.475805+0.00162176\n",
            ", best_score: 0.475801, best_iteration: 2488\n",
            "\n",
            "    alpha: 1\n",
            "[0]\ttrain-rmse:6.97356+0.000378752\ttest-rmse:6.97356+0.0016154\n",
            "[500]\ttrain-rmse:0.474043+0.000494363\ttest-rmse:0.482455+0.00170657\n",
            "[1000]\ttrain-rmse:0.463382+0.000556904\ttest-rmse:0.478273+0.00179094\n",
            "[1500]\ttrain-rmse:0.456191+0.000567056\ttest-rmse:0.476789+0.00178684\n",
            "[2000]\ttrain-rmse:0.450213+0.00055369\ttest-rmse:0.47617+0.00175649\n",
            "[2500]\ttrain-rmse:0.444933+0.000612831\ttest-rmse:0.475877+0.00173785\n",
            ", best_score: 0.475875, best_iteration: 2485\n",
            "\n",
            "    alpha: 10\n",
            "[0]\ttrain-rmse:6.97357+0.000378653\ttest-rmse:6.97357+0.00161562\n",
            "[500]\ttrain-rmse:0.476645+0.00047533\ttest-rmse:0.483383+0.00168519\n",
            "[1000]\ttrain-rmse:0.467694+0.000528654\ttest-rmse:0.47897+0.00170856\n",
            "[1500]\ttrain-rmse:0.461849+0.000511025\ttest-rmse:0.477255+0.00167786\n",
            "[2000]\ttrain-rmse:0.456968+0.000499195\ttest-rmse:0.4764+0.00165009\n",
            "[2500]\ttrain-rmse:0.452714+0.000485091\ttest-rmse:0.475939+0.00159624\n",
            "[3000]\ttrain-rmse:0.44885+0.000412891\ttest-rmse:0.475722+0.00159018\n",
            ", best_score: 0.475638, best_iteration: 3357\n",
            "\n",
            "    alpha: 100\n",
            "[0]\ttrain-rmse:6.9736+0.000378641\ttest-rmse:6.9736+0.0016154\n",
            "[500]\ttrain-rmse:0.485638+0.000396447\ttest-rmse:0.488875+0.00170532\n",
            "[1000]\ttrain-rmse:0.480307+0.000434051\ttest-rmse:0.484839+0.00169708\n",
            "[1500]\ttrain-rmse:0.478118+0.00040784\ttest-rmse:0.483387+0.00170058\n",
            "[2000]\ttrain-rmse:0.476823+0.000399641\ttest-rmse:0.482627+0.00171691\n",
            "[2500]\ttrain-rmse:0.475864+0.000395444\ttest-rmse:0.482088+0.00170066\n",
            "[3000]\ttrain-rmse:0.475153+0.000387085\ttest-rmse:0.481723+0.00171012\n",
            "[3500]\ttrain-rmse:0.474549+0.000390672\ttest-rmse:0.481428+0.00171211\n",
            "[4000]\ttrain-rmse:0.474015+0.000397857\ttest-rmse:0.481165+0.00170348\n",
            "[4500]\ttrain-rmse:0.473536+0.000388949\ttest-rmse:0.48095+0.00171371\n",
            "[5000]\ttrain-rmse:0.473122+0.000379809\ttest-rmse:0.480764+0.00170889\n",
            "[5500]\ttrain-rmse:0.472731+0.000391944\ttest-rmse:0.480598+0.00169479\n",
            "[6000]\ttrain-rmse:0.472375+0.000393548\ttest-rmse:0.48045+0.0016931\n",
            "[6500]\ttrain-rmse:0.472052+0.000381222\ttest-rmse:0.480327+0.00170585\n",
            "[7000]\ttrain-rmse:0.471744+0.000377134\ttest-rmse:0.480204+0.00170651\n",
            "[7500]\ttrain-rmse:0.471455+0.000383135\ttest-rmse:0.480097+0.00170403\n",
            "[8000]\ttrain-rmse:0.47117+0.000376284\ttest-rmse:0.479984+0.00171437\n",
            "[8500]\ttrain-rmse:0.470897+0.000384613\ttest-rmse:0.479888+0.00171462\n",
            "[9000]\ttrain-rmse:0.470607+0.0003815\ttest-rmse:0.479781+0.00170219\n",
            "[9500]\ttrain-rmse:0.470353+0.000386891\ttest-rmse:0.479698+0.00169402\n",
            "[10000]\ttrain-rmse:0.470114+0.000384145\ttest-rmse:0.479618+0.00169183\n",
            "[10500]\ttrain-rmse:0.469867+0.000389215\ttest-rmse:0.479522+0.00168718\n",
            "[11000]\ttrain-rmse:0.469633+0.000393614\ttest-rmse:0.479434+0.00167904\n",
            "[11500]\ttrain-rmse:0.469408+0.000387559\ttest-rmse:0.479361+0.00168368\n",
            "[12000]\ttrain-rmse:0.469187+0.000392137\ttest-rmse:0.479286+0.00166976\n",
            "[12500]\ttrain-rmse:0.468987+0.000400852\ttest-rmse:0.479225+0.00166282\n",
            "[13000]\ttrain-rmse:0.468779+0.000403596\ttest-rmse:0.479159+0.00166231\n",
            "[13500]\ttrain-rmse:0.468579+0.000402621\ttest-rmse:0.479103+0.0016574\n",
            "[14000]\ttrain-rmse:0.468379+0.000402399\ttest-rmse:0.479042+0.00165087\n",
            "[14500]\ttrain-rmse:0.468187+0.000398236\ttest-rmse:0.478987+0.00164656\n",
            "[15000]\ttrain-rmse:0.468002+0.000396084\ttest-rmse:0.478934+0.00164863\n",
            "[15500]\ttrain-rmse:0.467825+0.000396373\ttest-rmse:0.478883+0.00163834\n",
            "[16000]\ttrain-rmse:0.467651+0.000397693\ttest-rmse:0.478836+0.00164085\n",
            "[16500]\ttrain-rmse:0.467473+0.000393297\ttest-rmse:0.478783+0.00163132\n",
            ", best_score: 0.478771, best_iteration: 16590\n",
            "Best alpha is 10 with a score of 0.475638\n",
            "\n",
            " Best manually tuned parameters: {'learning_rate': 0.05, 'metrics': 'rmse', 'subsample': 0.8, 'seed': 1234, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'max_leaves': 127, 'max_depth': 7, 'gamma': 0, 'colsample_bytree': 0.1, 'min_child_weight': 100, 'lambda': 100, 'alpha': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqujTw_IDKPx",
        "colab_type": "code",
        "outputId": "da488cd8-cb76-412a-d628-aa3b056c698c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print ('\\n Best manually tuned parameters:', best_xgb_params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Best manually tuned parameters: {'learning_rate': 0.05, 'metrics': 'rmse', 'subsample': 0.8, 'seed': 1234, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'max_leaves': 127, 'max_depth': 7, 'gamma': 0, 'colsample_bytree': 0.1, 'min_child_weight': 100, 'lambda': 100, 'alpha': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q19dDKb5LY86",
        "colab_type": "text"
      },
      "source": [
        "## Automated tuning\n",
        "\n",
        "We will be using a package BayesianOptimization for automated tuning. Results from manual tuning can be used to further narrow the space that needs to be searched from for better performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9GkGvVXLXxO",
        "colab_type": "code",
        "outputId": "28289dd6-3959-4917-ad1f-26655f1e6b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install bayesian-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.18.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwyMelrVLX2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bayes_opt import BayesianOptimization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcNwHGufbICf",
        "colab_type": "text"
      },
      "source": [
        "lgb vs xgb:\n",
        "bagging_freq = Not have this feature \\\\\n",
        "num_leaves = max_leaves \\\\\n",
        "max_depth = max_depth \\\\\n",
        "min_gain_to_split = gamma \\\\\n",
        "feature_fraction = colsample_bytree \\\\\n",
        "bagging_fraction = subsample \\\\\n",
        "min_sum_hessian_in_leaf = min_child_weight \\\\\n",
        "lambda_l2 = lambda \\\\\n",
        "lambda_l1 = alpha \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQjDuRQJLX7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xgb_evaluate(\n",
        "    max_leaves,\n",
        "    max_depth,\n",
        "    min_child_weight,\n",
        "    gamma,\n",
        "    colsample_bytree,\n",
        "    subsample,\n",
        "    lambda_1,\n",
        "    alpha\n",
        "):\n",
        "    params = dict()\n",
        "    params['objective'] = 'reg:squarederror'\n",
        "    params['learning_rate'] = 0.05\n",
        "    params['seed'] = 1234\n",
        "    params['max_leaves'] = int(max_leaves)\n",
        "    params['max_depth'] = int(max_depth)\n",
        "    params['min_child_weight'] = int(min_child_weight)\n",
        "    params['gamma'] = gamma\n",
        "    params['colsample_bytree'] = colsample_bytree\n",
        "    params['subsample'] = subsample\n",
        "    params['lambda'] = lambda_1\n",
        "    params['alpha'] = alpha\n",
        "    params[\"metric\"] = 'rmse'\n",
        "    params[\"tree_method\"] = 'gpu_hist'\n",
        "    params[\"gpu_id\"] = 0\n",
        "\n",
        "    xgb_cv = xgb.cv(params,\n",
        "                xgb.DMatrix(train_x,\n",
        "                            label=train_y\n",
        "                            ),\n",
        "                num_boost_round=100000,\n",
        "                nfold=kfolds,\n",
        "                folds=list(skf.split(train_y)),\n",
        "                stratified=False,\n",
        "                early_stopping_rounds=50,\n",
        "                verbose_eval=500)\n",
        "\n",
        "    best_xgb_score = min(xgb_cv['test-rmse-mean'])\n",
        "    best_xgb_iteration = len(xgb_cv['test-rmse-mean'])\n",
        "    print(', best_score: %f, best_iteration: %d' %\n",
        "          (best_xgb_score, best_xgb_iteration))\n",
        "\n",
        "    return -best_xgb_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If0Hq4l922Qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bayes_opt.logger import JSONLogger\n",
        "from bayes_opt.event import Events\n",
        "\n",
        "xgb_BO = BayesianOptimization(xgb_evaluate,\n",
        "                              {\n",
        "                                  'max_leaves': (7, 31),\n",
        "                                  'max_depth': (7, 31),\n",
        "                                  'min_child_weight': (0, 2),\n",
        "                                  'gamma': (0, 5),\n",
        "                                  'colsample_bytree': (0.3, 0.6),\n",
        "                                  'subsample': (0.9, 1),\n",
        "                                  'lambda_1': (0, 1),\n",
        "                                  'alpha': (0, 1)\n",
        "                              }\n",
        "                              )\n",
        "\n",
        "logger = JSONLogger(path=\"/content/drive/My Drive/Kaggle_Allstate/data/logs_gpu.json\")\n",
        "xgb_BO.subscribe(Events.OPTIMIZATION_STEP, logger)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m51prtU7LYGa",
        "colab_type": "code",
        "outputId": "17d0d2ff-08d4-42a8-c6d6-ccd91dc11539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "xgb_BO.maximize(init_points=5, n_iter=40) \n",
        "# in the competition, we should use more than 25, below we use 6 points as example. \n",
        "# xgb_BO.maximize(init_points=3, n_iter=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-rmse:6.97233+0.000378053\ttest-rmse:6.97232+0.00161649\n",
            "[500]\ttrain-rmse:0.470635+0.000545791\ttest-rmse:0.481696+0.00202751\n",
            "[1000]\ttrain-rmse:0.459975+0.000459702\ttest-rmse:0.478801+0.00213447\n",
            "[1500]\ttrain-rmse:0.452288+0.000447315\ttest-rmse:0.477866+0.00220792\n",
            "[2000]\ttrain-rmse:0.445671+0.000469774\ttest-rmse:0.477504+0.00221853\n",
            ", best_score: 0.477470, best_iteration: 2102\n",
            "[0]\ttrain-rmse:6.97241+0.000368375\ttest-rmse:6.97242+0.00164847\n",
            "[500]\ttrain-rmse:0.469116+0.000575465\ttest-rmse:0.481555+0.00206038\n",
            "[1000]\ttrain-rmse:0.457738+0.000469362\ttest-rmse:0.478779+0.00218331\n",
            "[1500]\ttrain-rmse:0.449586+0.000467544\ttest-rmse:0.47799+0.00221229\n",
            "[2000]\ttrain-rmse:0.442585+0.000466965\ttest-rmse:0.477685+0.00225143\n",
            ", best_score: 0.477573, best_iteration: 2346\n",
            "[0]\ttrain-rmse:6.97199+0.000377748\ttest-rmse:6.97198+0.00170074\n",
            "[500]\ttrain-rmse:0.469353+0.000625262\ttest-rmse:0.48161+0.00185947\n",
            "[1000]\ttrain-rmse:0.458241+0.000481047\ttest-rmse:0.478956+0.00207415\n",
            "[1500]\ttrain-rmse:0.450201+0.000459876\ttest-rmse:0.478233+0.00213522\n",
            ", best_score: 0.478017, best_iteration: 1803\n",
            "[0]\ttrain-rmse:6.97198+0.000386328\ttest-rmse:6.97198+0.00165623\n",
            "[500]\ttrain-rmse:0.469874+0.000404455\ttest-rmse:0.481541+0.00219357\n",
            "[1000]\ttrain-rmse:0.458898+0.000388102\ttest-rmse:0.478743+0.00225875\n",
            "[1500]\ttrain-rmse:0.451034+0.000436364\ttest-rmse:0.477968+0.00228352\n",
            "[2000]\ttrain-rmse:0.444309+0.000391688\ttest-rmse:0.477665+0.00233227\n",
            ", best_score: 0.477625, best_iteration: 2116\n",
            "[0]\ttrain-rmse:6.97193+0.000377258\ttest-rmse:6.97193+0.00168631\n",
            "[500]\ttrain-rmse:0.469221+0.000591853\ttest-rmse:0.481406+0.0019819\n",
            "[1000]\ttrain-rmse:0.458074+0.000488262\ttest-rmse:0.478759+0.00207682\n",
            "[1500]\ttrain-rmse:0.450074+0.00042826\ttest-rmse:0.478046+0.00216011\n",
            ", best_score: 0.477818, best_iteration: 1891\n",
            "[0]\ttrain-rmse:6.97213+0.000380642\ttest-rmse:6.97212+0.0016891\n",
            "[500]\ttrain-rmse:0.488741+0.000382012\ttest-rmse:0.491369+0.0019172\n",
            "[1000]\ttrain-rmse:0.480788+0.000417937\ttest-rmse:0.485553+0.00209047\n",
            "[1500]\ttrain-rmse:0.476766+0.000479826\ttest-rmse:0.483404+0.00208318\n",
            "[2000]\ttrain-rmse:0.473871+0.000522057\ttest-rmse:0.482277+0.00208882\n",
            "[2500]\ttrain-rmse:0.471537+0.000523796\ttest-rmse:0.481591+0.00211295\n",
            "[3000]\ttrain-rmse:0.469479+0.000546369\ttest-rmse:0.481089+0.00211595\n",
            "[3500]\ttrain-rmse:0.467631+0.000514185\ttest-rmse:0.480717+0.00213409\n",
            "[4000]\ttrain-rmse:0.465912+0.000502796\ttest-rmse:0.48039+0.00215227\n",
            "[4500]\ttrain-rmse:0.464303+0.000482888\ttest-rmse:0.480171+0.00214845\n",
            "[5000]\ttrain-rmse:0.462778+0.000469561\ttest-rmse:0.479992+0.00212826\n",
            ", best_score: 0.479900, best_iteration: 5303\n",
            "[0]\ttrain-rmse:6.97263+0.000376362\ttest-rmse:6.97263+0.00162069\n",
            "[500]\ttrain-rmse:0.489301+0.000359831\ttest-rmse:0.491785+0.00192522\n",
            "[1000]\ttrain-rmse:0.481217+0.000385516\ttest-rmse:0.485791+0.0020301\n",
            "[1500]\ttrain-rmse:0.477162+0.000434768\ttest-rmse:0.483638+0.00206188\n",
            "[2000]\ttrain-rmse:0.474257+0.000459862\ttest-rmse:0.48244+0.00205646\n",
            "[2500]\ttrain-rmse:0.471944+0.000454004\ttest-rmse:0.481742+0.00206531\n",
            "[3000]\ttrain-rmse:0.469899+0.000440569\ttest-rmse:0.481248+0.00205669\n",
            "[3500]\ttrain-rmse:0.46809+0.000431066\ttest-rmse:0.480878+0.00204763\n",
            "[4000]\ttrain-rmse:0.466406+0.000432257\ttest-rmse:0.480588+0.00205491\n",
            "[4500]\ttrain-rmse:0.46483+0.000425509\ttest-rmse:0.480357+0.00206347\n",
            "[5000]\ttrain-rmse:0.463311+0.000424556\ttest-rmse:0.480182+0.00205939\n",
            "[5500]\ttrain-rmse:0.461845+0.000433399\ttest-rmse:0.480009+0.00206256\n",
            "[6000]\ttrain-rmse:0.460449+0.000430218\ttest-rmse:0.479886+0.00203113\n",
            ", best_score: 0.479820, best_iteration: 6248\n",
            "[0]\ttrain-rmse:6.97195+0.000384017\ttest-rmse:6.97196+0.00166781\n",
            "[500]\ttrain-rmse:0.468009+0.000575025\ttest-rmse:0.48101+0.00200528\n",
            "[1000]\ttrain-rmse:0.456359+0.000545758\ttest-rmse:0.478384+0.0020488\n",
            "[1500]\ttrain-rmse:0.447908+0.000484008\ttest-rmse:0.47774+0.00217774\n",
            ", best_score: 0.477584, best_iteration: 1839\n",
            "[0]\ttrain-rmse:6.97238+0.000390724\ttest-rmse:6.97239+0.0016163\n",
            "[500]\ttrain-rmse:0.468651+0.000479648\ttest-rmse:0.48154+0.00204684\n",
            "[1000]\ttrain-rmse:0.4569+0.000465431\ttest-rmse:0.478902+0.00209793\n",
            "[1500]\ttrain-rmse:0.448343+0.000469734\ttest-rmse:0.478137+0.00210539\n",
            "[2000]\ttrain-rmse:0.441098+0.000423755\ttest-rmse:0.477838+0.00212222\n",
            ", best_score: 0.477705, best_iteration: 2263\n",
            "[0]\ttrain-rmse:6.97194+0.000384229\ttest-rmse:6.97194+0.00168552\n",
            "[500]\ttrain-rmse:0.467786+0.000529973\ttest-rmse:0.481125+0.0020054\n",
            "[1000]\ttrain-rmse:0.455765+0.000482449\ttest-rmse:0.478561+0.00221107\n",
            "[1500]\ttrain-rmse:0.44696+0.000472817\ttest-rmse:0.477926+0.00226468\n",
            ", best_score: 0.477790, best_iteration: 1858\n",
            "[0]\ttrain-rmse:6.97191+0.000379842\ttest-rmse:6.9719+0.00168489\n",
            "[500]\ttrain-rmse:0.467069+0.000514179\ttest-rmse:0.480966+0.00204639\n",
            "[1000]\ttrain-rmse:0.45469+0.00047282\ttest-rmse:0.478551+0.00214725\n",
            "[1500]\ttrain-rmse:0.445602+0.00045331\ttest-rmse:0.47799+0.00221883\n",
            ", best_score: 0.477859, best_iteration: 1790\n",
            "[0]\ttrain-rmse:6.97234+0.000391162\ttest-rmse:6.97234+0.00160948\n",
            "[500]\ttrain-rmse:0.468099+0.000539287\ttest-rmse:0.48135+0.00200617\n",
            "[1000]\ttrain-rmse:0.456318+0.000472817\ttest-rmse:0.478631+0.00211739\n",
            "[1500]\ttrain-rmse:0.44757+0.000514645\ttest-rmse:0.477877+0.00217854\n",
            "[2000]\ttrain-rmse:0.439992+0.000475603\ttest-rmse:0.477607+0.00220642\n",
            ", best_score: 0.477514, best_iteration: 2260\n",
            "[0]\ttrain-rmse:6.97243+0.00036922\ttest-rmse:6.97244+0.00164243\n",
            "[500]\ttrain-rmse:0.468182+0.000525793\ttest-rmse:0.481511+0.00202412\n",
            "[1000]\ttrain-rmse:0.456666+0.00040598\ttest-rmse:0.479155+0.00218726\n",
            "[1500]\ttrain-rmse:0.448692+0.000416229\ttest-rmse:0.478585+0.00227311\n",
            ", best_score: 0.478354, best_iteration: 1935\n",
            "[0]\ttrain-rmse:6.97243+0.00037502\ttest-rmse:6.97244+0.001633\n",
            "[500]\ttrain-rmse:0.468438+0.000460043\ttest-rmse:0.48147+0.00202416\n",
            "[1000]\ttrain-rmse:0.456667+0.000378726\ttest-rmse:0.479001+0.00221853\n",
            "[1500]\ttrain-rmse:0.448316+0.000341097\ttest-rmse:0.478357+0.00222523\n",
            ", best_score: 0.478186, best_iteration: 1810\n",
            "[0]\ttrain-rmse:6.97193+0.000380738\ttest-rmse:6.97193+0.00169788\n",
            "[500]\ttrain-rmse:0.467023+0.000426426\ttest-rmse:0.481009+0.00213749\n",
            "[1000]\ttrain-rmse:0.455034+0.000400838\ttest-rmse:0.47868+0.00227732\n",
            "[1500]\ttrain-rmse:0.446258+0.000421652\ttest-rmse:0.47805+0.00226524\n",
            ", best_score: 0.477935, best_iteration: 1708\n",
            "[0]\ttrain-rmse:6.97192+0.000374067\ttest-rmse:6.97192+0.00167462\n",
            "[500]\ttrain-rmse:0.466922+0.000578838\ttest-rmse:0.48105+0.00214925\n",
            "[1000]\ttrain-rmse:0.454323+0.000552118\ttest-rmse:0.478522+0.0022174\n",
            "[1500]\ttrain-rmse:0.445033+0.000504286\ttest-rmse:0.477971+0.00222366\n",
            ", best_score: 0.477881, best_iteration: 1618\n",
            "[0]\ttrain-rmse:6.97196+0.000387505\ttest-rmse:6.97197+0.00167079\n",
            "[500]\ttrain-rmse:0.468375+0.000699055\ttest-rmse:0.481248+0.00190382\n",
            "[1000]\ttrain-rmse:0.457058+0.000581768\ttest-rmse:0.478719+0.00208912\n",
            "[1500]\ttrain-rmse:0.448816+0.000568245\ttest-rmse:0.478004+0.00218663\n",
            ", best_score: 0.477761, best_iteration: 1891\n",
            "[0]\ttrain-rmse:6.97196+0.000381115\ttest-rmse:6.97198+0.00167306\n",
            "[500]\ttrain-rmse:0.467539+0.0005158\ttest-rmse:0.481124+0.00205952\n",
            "[1000]\ttrain-rmse:0.455829+0.000435106\ttest-rmse:0.478733+0.00218519\n",
            "[1500]\ttrain-rmse:0.447463+0.000433909\ttest-rmse:0.478136+0.00215931\n",
            "[2000]\ttrain-rmse:0.440307+0.000346103\ttest-rmse:0.477937+0.00218839\n",
            ", best_score: 0.477935, best_iteration: 2002\n",
            "[0]\ttrain-rmse:6.97234+0.000391482\ttest-rmse:6.97234+0.0016105\n",
            "[500]\ttrain-rmse:0.468374+0.000600417\ttest-rmse:0.481401+0.00210305\n",
            "[1000]\ttrain-rmse:0.456574+0.000522151\ttest-rmse:0.478684+0.00226706\n",
            "[1500]\ttrain-rmse:0.447879+0.000495609\ttest-rmse:0.477915+0.00235051\n",
            ", best_score: 0.477682, best_iteration: 1828\n",
            "[0]\ttrain-rmse:6.97246+0.00038517\ttest-rmse:6.97247+0.00162159\n",
            "[500]\ttrain-rmse:0.468579+0.00044432\ttest-rmse:0.481609+0.00205383\n",
            "[1000]\ttrain-rmse:0.456765+0.000493005\ttest-rmse:0.478959+0.00213023\n",
            "[1500]\ttrain-rmse:0.448287+0.000531022\ttest-rmse:0.478217+0.0020789\n",
            "[2000]\ttrain-rmse:0.441017+0.000465249\ttest-rmse:0.477944+0.0021198\n",
            ", best_score: 0.477874, best_iteration: 2207\n",
            "[0]\ttrain-rmse:6.97197+0.000383498\ttest-rmse:6.97198+0.00165953\n",
            "[500]\ttrain-rmse:0.468486+0.000427427\ttest-rmse:0.48127+0.00216958\n",
            "[1000]\ttrain-rmse:0.457691+0.000393385\ttest-rmse:0.478841+0.00228374\n",
            "[1500]\ttrain-rmse:0.450052+0.00038791\ttest-rmse:0.478182+0.00234995\n",
            ", best_score: 0.478054, best_iteration: 1686\n",
            "[0]\ttrain-rmse:6.97231+0.000398411\ttest-rmse:6.97231+0.00159758\n",
            "[500]\ttrain-rmse:0.468945+0.000437605\ttest-rmse:0.48123+0.00214945\n",
            "[1000]\ttrain-rmse:0.45795+0.000487795\ttest-rmse:0.478656+0.00218918\n",
            "[1500]\ttrain-rmse:0.450141+0.000444809\ttest-rmse:0.477903+0.00226388\n",
            "[2000]\ttrain-rmse:0.443384+0.000373889\ttest-rmse:0.47758+0.00231813\n",
            ", best_score: 0.477490, best_iteration: 2268\n",
            "[0]\ttrain-rmse:6.97194+0.000381087\ttest-rmse:6.97194+0.00170144\n",
            "[500]\ttrain-rmse:0.468386+0.000496132\ttest-rmse:0.481156+0.00205308\n",
            "[1000]\ttrain-rmse:0.457105+0.000389259\ttest-rmse:0.478591+0.00217059\n",
            "[1500]\ttrain-rmse:0.448933+0.000407966\ttest-rmse:0.477837+0.00220726\n",
            "[2000]\ttrain-rmse:0.441942+0.000382163\ttest-rmse:0.477588+0.00219282\n",
            ", best_score: 0.477532, best_iteration: 2143\n",
            "[0]\ttrain-rmse:6.97192+0.000384822\ttest-rmse:6.97192+0.00168014\n",
            "[500]\ttrain-rmse:0.468001+0.00061853\ttest-rmse:0.481284+0.00200971\n",
            "[1000]\ttrain-rmse:0.456292+0.000417562\ttest-rmse:0.478624+0.00213176\n",
            "[1500]\ttrain-rmse:0.447731+0.000417836\ttest-rmse:0.477937+0.00222104\n",
            ", best_score: 0.477787, best_iteration: 1801\n",
            "[0]\ttrain-rmse:6.97195+0.000382417\ttest-rmse:6.97195+0.00167426\n",
            "[500]\ttrain-rmse:0.46743+0.000443985\ttest-rmse:0.481055+0.00214908\n",
            "[1000]\ttrain-rmse:0.455706+0.00041797\ttest-rmse:0.478702+0.00223817\n",
            "[1500]\ttrain-rmse:0.447137+0.0003477\ttest-rmse:0.478073+0.00232952\n",
            ", best_score: 0.477923, best_iteration: 1700\n",
            "[0]\ttrain-rmse:6.97242+0.000361196\ttest-rmse:6.97242+0.00166145\n",
            "[500]\ttrain-rmse:0.468348+0.000602829\ttest-rmse:0.48151+0.00189492\n",
            "[1000]\ttrain-rmse:0.456182+0.000507066\ttest-rmse:0.478718+0.00202839\n",
            "[1500]\ttrain-rmse:0.447266+0.000508703\ttest-rmse:0.477971+0.00205488\n",
            "[2000]\ttrain-rmse:0.439503+0.000482347\ttest-rmse:0.477731+0.00205858\n",
            ", best_score: 0.477728, best_iteration: 1998\n",
            "[0]\ttrain-rmse:6.97192+0.000379576\ttest-rmse:6.97192+0.00168579\n",
            "[500]\ttrain-rmse:0.467696+0.000414436\ttest-rmse:0.481005+0.00204821\n",
            "[1000]\ttrain-rmse:0.455995+0.000316337\ttest-rmse:0.478445+0.00213622\n",
            "[1500]\ttrain-rmse:0.447553+0.000303321\ttest-rmse:0.477869+0.00217135\n",
            "[2000]\ttrain-rmse:0.440202+0.00035178\ttest-rmse:0.477652+0.00217604\n",
            ", best_score: 0.477640, best_iteration: 1975\n",
            "[0]\ttrain-rmse:6.97243+0.000389002\ttest-rmse:6.97244+0.00161434\n",
            "[500]\ttrain-rmse:0.467442+0.000559961\ttest-rmse:0.481385+0.0019998\n",
            "[1000]\ttrain-rmse:0.454944+0.000500983\ttest-rmse:0.478968+0.00210285\n",
            "[1500]\ttrain-rmse:0.445785+0.000529998\ttest-rmse:0.478325+0.00218316\n",
            ", best_score: 0.478231, best_iteration: 1695\n",
            "[0]\ttrain-rmse:6.97243+0.000384107\ttest-rmse:6.97245+0.00161615\n",
            "[500]\ttrain-rmse:0.468361+0.000455583\ttest-rmse:0.481308+0.00192735\n",
            "[1000]\ttrain-rmse:0.456856+0.000442756\ttest-rmse:0.478711+0.00197678\n",
            "[1500]\ttrain-rmse:0.44843+0.000397763\ttest-rmse:0.477977+0.00208749\n",
            "[2000]\ttrain-rmse:0.441203+0.000379077\ttest-rmse:0.477701+0.00205128\n",
            ", best_score: 0.477621, best_iteration: 2205\n",
            "[0]\ttrain-rmse:6.97194+0.000385504\ttest-rmse:6.97194+0.00169006\n",
            "[500]\ttrain-rmse:0.466802+0.000547869\ttest-rmse:0.481105+0.00201576\n",
            "[1000]\ttrain-rmse:0.454433+0.000461379\ttest-rmse:0.478822+0.00216468\n",
            "[1500]\ttrain-rmse:0.445444+0.000396991\ttest-rmse:0.478278+0.00221199\n",
            ", best_score: 0.478146, best_iteration: 1815\n",
            "[0]\ttrain-rmse:6.97196+0.000377081\ttest-rmse:6.97197+0.00168759\n",
            "[500]\ttrain-rmse:0.467156+0.000536622\ttest-rmse:0.48105+0.00209512\n",
            "[1000]\ttrain-rmse:0.455463+0.000422615\ttest-rmse:0.478809+0.00230988\n",
            "[1500]\ttrain-rmse:0.447095+0.000384371\ttest-rmse:0.478282+0.00232689\n",
            ", best_score: 0.478268, best_iteration: 1519\n",
            "[0]\ttrain-rmse:6.97234+0.000388573\ttest-rmse:6.97235+0.00162069\n",
            "[500]\ttrain-rmse:0.468251+0.000593809\ttest-rmse:0.481438+0.00196936\n",
            "[1000]\ttrain-rmse:0.456301+0.000559983\ttest-rmse:0.478843+0.00206738\n",
            "[1500]\ttrain-rmse:0.44775+0.000569553\ttest-rmse:0.478209+0.00213255\n",
            ", best_score: 0.478008, best_iteration: 1799\n",
            "[0]\ttrain-rmse:6.97244+0.000385095\ttest-rmse:6.97245+0.00161508\n",
            "[500]\ttrain-rmse:0.468534+0.00057775\ttest-rmse:0.481275+0.00204105\n",
            "[1000]\ttrain-rmse:0.457053+0.000519085\ttest-rmse:0.478572+0.00210826\n",
            "[1500]\ttrain-rmse:0.448771+0.000462917\ttest-rmse:0.477842+0.00218274\n",
            ", best_score: 0.477608, best_iteration: 1793\n",
            "[0]\ttrain-rmse:6.97198+0.000385176\ttest-rmse:6.97199+0.00168137\n",
            "[500]\ttrain-rmse:0.468154+0.00049209\ttest-rmse:0.481302+0.00204181\n",
            "[1000]\ttrain-rmse:0.456644+0.000437977\ttest-rmse:0.478813+0.00212218\n",
            "[1500]\ttrain-rmse:0.448507+0.000458959\ttest-rmse:0.478147+0.00210474\n",
            ", best_score: 0.477933, best_iteration: 1877\n",
            "[0]\ttrain-rmse:6.97196+0.000376459\ttest-rmse:6.97197+0.00169011\n",
            "[500]\ttrain-rmse:0.467958+0.000443812\ttest-rmse:0.480923+0.00209749\n",
            "[1000]\ttrain-rmse:0.456915+0.000305815\ttest-rmse:0.478575+0.00232007\n",
            "[1500]\ttrain-rmse:0.448948+0.00033018\ttest-rmse:0.477922+0.00233701\n",
            "[2000]\ttrain-rmse:0.442144+0.000315411\ttest-rmse:0.477709+0.00238422\n",
            ", best_score: 0.477701, best_iteration: 2013\n",
            "[0]\ttrain-rmse:6.97235+0.000384974\ttest-rmse:6.97235+0.00161932\n",
            "[500]\ttrain-rmse:0.469633+0.000591174\ttest-rmse:0.481571+0.00204191\n",
            "[1000]\ttrain-rmse:0.45838+0.000456659\ttest-rmse:0.478693+0.0022417\n",
            "[1500]\ttrain-rmse:0.450224+0.000480295\ttest-rmse:0.477785+0.00226468\n",
            "[2000]\ttrain-rmse:0.443175+0.000505677\ttest-rmse:0.477436+0.00230725\n",
            ", best_score: 0.477387, best_iteration: 2166\n",
            "[0]\ttrain-rmse:6.97195+0.00037674\ttest-rmse:6.97194+0.00167938\n",
            "[500]\ttrain-rmse:0.467363+0.00046024\ttest-rmse:0.481152+0.00211262\n",
            "[1000]\ttrain-rmse:0.456594+0.000437486\ttest-rmse:0.479176+0.00222585\n",
            "[1500]\ttrain-rmse:0.44916+0.000564735\ttest-rmse:0.478717+0.00226537\n",
            ", best_score: 0.478577, best_iteration: 1919\n",
            "[0]\ttrain-rmse:6.97228+0.000398512\ttest-rmse:6.97229+0.00159691\n",
            "[500]\ttrain-rmse:0.468056+0.000463245\ttest-rmse:0.481113+0.00220448\n",
            "[1000]\ttrain-rmse:0.45633+0.000484049\ttest-rmse:0.478692+0.00220628\n",
            "[1500]\ttrain-rmse:0.448011+0.000501699\ttest-rmse:0.478041+0.00220888\n",
            "[2000]\ttrain-rmse:0.440767+0.000563675\ttest-rmse:0.477791+0.00217671\n",
            ", best_score: 0.477716, best_iteration: 2208\n",
            "[0]\ttrain-rmse:6.97228+0.000384648\ttest-rmse:6.97227+0.00160113\n",
            "[500]\ttrain-rmse:0.467885+0.000525124\ttest-rmse:0.481237+0.00210449\n",
            "[1000]\ttrain-rmse:0.455768+0.000525202\ttest-rmse:0.478669+0.00216569\n",
            "[1500]\ttrain-rmse:0.446716+0.000467649\ttest-rmse:0.477925+0.00224943\n",
            ", best_score: 0.477791, best_iteration: 1693\n",
            "[0]\ttrain-rmse:6.97195+0.000397084\ttest-rmse:6.97195+0.00165283\n",
            "[500]\ttrain-rmse:0.46811+0.000502227\ttest-rmse:0.481066+0.00212668\n",
            "[1000]\ttrain-rmse:0.456222+0.000429385\ttest-rmse:0.478387+0.00216289\n",
            "[1500]\ttrain-rmse:0.447431+0.000434548\ttest-rmse:0.477671+0.00217968\n",
            ", best_score: 0.477511, best_iteration: 1836\n",
            "[0]\ttrain-rmse:6.97235+0.000375542\ttest-rmse:6.97236+0.00162647\n",
            "[500]\ttrain-rmse:0.469329+0.000512149\ttest-rmse:0.481449+0.00199723\n",
            "[1000]\ttrain-rmse:0.458146+0.000493059\ttest-rmse:0.478675+0.00201529\n",
            "[1500]\ttrain-rmse:0.450066+0.000442179\ttest-rmse:0.477848+0.00209163\n",
            "[2000]\ttrain-rmse:0.443101+0.000489067\ttest-rmse:0.477441+0.00211287\n",
            ", best_score: 0.477335, best_iteration: 2220\n",
            "[0]\ttrain-rmse:6.97249+0.00039073\ttest-rmse:6.97251+0.00161458\n",
            "[500]\ttrain-rmse:0.469128+0.000553729\ttest-rmse:0.481456+0.00196224\n",
            "[1000]\ttrain-rmse:0.458287+0.000415174\ttest-rmse:0.478886+0.00215581\n",
            "[1500]\ttrain-rmse:0.450638+0.000417315\ttest-rmse:0.478161+0.0021838\n",
            "[2000]\ttrain-rmse:0.444097+0.000462965\ttest-rmse:0.477796+0.00213395\n",
            ", best_score: 0.477707, best_iteration: 2221\n",
            "[0]\ttrain-rmse:6.97196+0.00038257\ttest-rmse:6.97197+0.00166385\n",
            "[500]\ttrain-rmse:0.467691+0.000501862\ttest-rmse:0.481157+0.00207869\n",
            "[1000]\ttrain-rmse:0.455815+0.000476538\ttest-rmse:0.478661+0.00206133\n",
            "[1500]\ttrain-rmse:0.447056+0.000439656\ttest-rmse:0.478012+0.00215255\n",
            ", best_score: 0.477897, best_iteration: 1624\n",
            "[0]\ttrain-rmse:6.97246+0.000372334\ttest-rmse:6.97246+0.00164146\n",
            "[500]\ttrain-rmse:0.469172+0.000627443\ttest-rmse:0.481597+0.00189046\n",
            "[1000]\ttrain-rmse:0.457779+0.000555324\ttest-rmse:0.478856+0.00203795\n",
            "[1500]\ttrain-rmse:0.449611+0.000491252\ttest-rmse:0.478055+0.00201516\n",
            ", best_score: 0.477845, best_iteration: 1821\n",
            "[0]\ttrain-rmse:6.97233+0.000392262\ttest-rmse:6.97233+0.0016071\n",
            "[500]\ttrain-rmse:0.467579+0.000510139\ttest-rmse:0.48127+0.00216942\n",
            "[1000]\ttrain-rmse:0.455485+0.000471897\ttest-rmse:0.478838+0.00227557\n",
            "[1500]\ttrain-rmse:0.446699+0.000456752\ttest-rmse:0.478249+0.00229941\n",
            ", best_score: 0.478091, best_iteration: 1703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phwEtk5U4Kvy",
        "colab_type": "code",
        "outputId": "c9d726fe-6501-4a46-cc25-dc3a38980fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "xgb_BO.res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'params': {'alpha': 0.9443408033988769,\n",
              "   'colsample_bytree': 0.3558431609964596,\n",
              "   'gamma': 4.696188321837841,\n",
              "   'lambda_1': 0.6518433972257632,\n",
              "   'max_depth': 17.312640079846492,\n",
              "   'max_leaves': 27.541824872093443,\n",
              "   'min_child_weight': 0.7528371837200969,\n",
              "   'subsample': 0.9077252918981312},\n",
              "  'target': -0.4774698},\n",
              " {'params': {'alpha': 0.6137969281145833,\n",
              "   'colsample_bytree': 0.31365474212352024,\n",
              "   'gamma': 4.151448278236447,\n",
              "   'lambda_1': 0.35034786633150883,\n",
              "   'max_depth': 18.68806955200861,\n",
              "   'max_leaves': 30.809364365330705,\n",
              "   'min_child_weight': 1.544144283558467,\n",
              "   'subsample': 0.948846066577156},\n",
              "  'target': -0.47757259999999996},\n",
              " {'params': {'alpha': 0.7680841123394783,\n",
              "   'colsample_bytree': 0.5262847718961265,\n",
              "   'gamma': 1.32858144342245,\n",
              "   'lambda_1': 0.13037813495797712,\n",
              "   'max_depth': 24.604570279921695,\n",
              "   'max_leaves': 27.63622000589335,\n",
              "   'min_child_weight': 0.5126015836326645,\n",
              "   'subsample': 0.9429597651457532},\n",
              "  'target': -0.4780168000000001},\n",
              " {'params': {'alpha': 0.8603228174475762,\n",
              "   'colsample_bytree': 0.4447821485094775,\n",
              "   'gamma': 0.021858932008684828,\n",
              "   'lambda_1': 0.3617239869110247,\n",
              "   'max_depth': 14.151384200562644,\n",
              "   'max_leaves': 27.12060412368484,\n",
              "   'min_child_weight': 1.3856392255645675,\n",
              "   'subsample': 0.929323530126744},\n",
              "  'target': -0.47762479999999996},\n",
              " {'params': {'alpha': 0.8328333842980086,\n",
              "   'colsample_bytree': 0.5019543032543798,\n",
              "   'gamma': 3.9997385801824126,\n",
              "   'lambda_1': 0.5822636386079263,\n",
              "   'max_depth': 27.911007710335486,\n",
              "   'max_leaves': 28.413341812601036,\n",
              "   'min_child_weight': 1.546213894443609,\n",
              "   'subsample': 0.9462241759213665},\n",
              "  'target': -0.47781799999999996},\n",
              " {'params': {'alpha': 0.5393318074967491,\n",
              "   'colsample_bytree': 0.5929765134785345,\n",
              "   'gamma': 0.19606760094918074,\n",
              "   'lambda_1': 0.6551929733823981,\n",
              "   'max_depth': 7.996902690932093,\n",
              "   'max_leaves': 7.053918451814851,\n",
              "   'min_child_weight': 0.5387162978655573,\n",
              "   'subsample': 0.9086480971095331},\n",
              "  'target': -0.47989979999999993},\n",
              " {'params': {'alpha': 0.11032380733736569,\n",
              "   'colsample_bytree': 0.3407982369976975,\n",
              "   'gamma': 4.937913142857701,\n",
              "   'lambda_1': 0.8565636481158597,\n",
              "   'max_depth': 30.941440018115145,\n",
              "   'max_leaves': 7.470958770492904,\n",
              "   'min_child_weight': 0.7809971632001689,\n",
              "   'subsample': 0.9042478530945173},\n",
              "  'target': -0.4798198},\n",
              " {'params': {'alpha': 0.774068606330182,\n",
              "   'colsample_bytree': 0.44780590434739453,\n",
              "   'gamma': 4.682184449704621,\n",
              "   'lambda_1': 0.16539826655440348,\n",
              "   'max_depth': 7.4848050138444275,\n",
              "   'max_leaves': 30.986256986619114,\n",
              "   'min_child_weight': 0.45563119823379505,\n",
              "   'subsample': 0.9350904763876389},\n",
              "  'target': -0.4775836},\n",
              " {'params': {'alpha': 0.40233559064305013,\n",
              "   'colsample_bytree': 0.32262965718971615,\n",
              "   'gamma': 0.2281337137774364,\n",
              "   'lambda_1': 0.43342994403767265,\n",
              "   'max_depth': 30.30556576087394,\n",
              "   'max_leaves': 30.4539527868697,\n",
              "   'min_child_weight': 0.005969361832837583,\n",
              "   'subsample': 0.9275108710048952},\n",
              "  'target': -0.47770460000000003},\n",
              " {'params': {'alpha': 0.4412227099988739,\n",
              "   'colsample_bytree': 0.5114083165330146,\n",
              "   'gamma': 0.9596067860354324,\n",
              "   'lambda_1': 0.9493668856691709,\n",
              "   'max_depth': 7.139716856526451,\n",
              "   'max_leaves': 30.82601733507171,\n",
              "   'min_child_weight': 1.774899058460324,\n",
              "   'subsample': 0.9145642319212075},\n",
              "  'target': -0.47779},\n",
              " {'params': {'alpha': 0.2860115915081878,\n",
              "   'colsample_bytree': 0.5993101690646665,\n",
              "   'gamma': 0.12064710944447055,\n",
              "   'lambda_1': 0.7163855964577441,\n",
              "   'max_depth': 30.35564198154754,\n",
              "   'max_leaves': 30.89010885802859,\n",
              "   'min_child_weight': 0.06950749690889224,\n",
              "   'subsample': 0.9308000682546406},\n",
              "  'target': -0.4778594},\n",
              " {'params': {'alpha': 0.5915087166041215,\n",
              "   'colsample_bytree': 0.39267741329202327,\n",
              "   'gamma': 0.07717443454162587,\n",
              "   'lambda_1': 0.2148868109904175,\n",
              "   'max_depth': 7.35851490482942,\n",
              "   'max_leaves': 30.8550339036575,\n",
              "   'min_child_weight': 1.6387159947756116,\n",
              "   'subsample': 0.9019153106114799},\n",
              "  'target': -0.47751420000000006},\n",
              " {'params': {'alpha': 0.05715697520399754,\n",
              "   'colsample_bytree': 0.32580213582831996,\n",
              "   'gamma': 4.829667378079841,\n",
              "   'lambda_1': 0.34785509062369013,\n",
              "   'max_depth': 7.936818428371933,\n",
              "   'max_leaves': 30.992958986928848,\n",
              "   'min_child_weight': 1.430398133764986,\n",
              "   'subsample': 0.9886234969292791},\n",
              "  'target': -0.4783544},\n",
              " {'params': {'alpha': 0.14927540830094,\n",
              "   'colsample_bytree': 0.30344699592786417,\n",
              "   'gamma': 4.91608293438946,\n",
              "   'lambda_1': 0.32805747283279985,\n",
              "   'max_depth': 30.744424613565684,\n",
              "   'max_leaves': 30.886896664217417,\n",
              "   'min_child_weight': 1.6954866338636012,\n",
              "   'subsample': 0.9694828263208384},\n",
              "  'target': -0.47818640000000007},\n",
              " {'params': {'alpha': 0.3588126267702263,\n",
              "   'colsample_bytree': 0.5948160326548302,\n",
              "   'gamma': 1.820157376851958,\n",
              "   'lambda_1': 0.43412866013494555,\n",
              "   'max_depth': 7.209459318181966,\n",
              "   'max_leaves': 30.970796903823725,\n",
              "   'min_child_weight': 1.5236647716147693,\n",
              "   'subsample': 0.9534682968594231},\n",
              "  'target': -0.4779352},\n",
              " {'params': {'alpha': 0.03775530741465094,\n",
              "   'colsample_bytree': 0.555710461500103,\n",
              "   'gamma': 0.26122565562498146,\n",
              "   'lambda_1': 0.7136234847064615,\n",
              "   'max_depth': 30.955892848348938,\n",
              "   'max_leaves': 30.956819017197684,\n",
              "   'min_child_weight': 0.61943848124225,\n",
              "   'subsample': 0.9356965765753464},\n",
              "  'target': -0.477881},\n",
              " {'params': {'alpha': 0.8723332911535051,\n",
              "   'colsample_bytree': 0.42921593711701056,\n",
              "   'gamma': 0.4890853359232167,\n",
              "   'lambda_1': 0.21606804830032345,\n",
              "   'max_depth': 7.419560912669763,\n",
              "   'max_leaves': 30.91803160444163,\n",
              "   'min_child_weight': 0.4351221322150902,\n",
              "   'subsample': 0.9500366965973678},\n",
              "  'target': -0.477761},\n",
              " {'params': {'alpha': 0.2324326692019021,\n",
              "   'colsample_bytree': 0.4583807273579257,\n",
              "   'gamma': 0.13748789404047612,\n",
              "   'lambda_1': 0.45801304264233367,\n",
              "   'max_depth': 7.096412126333444,\n",
              "   'max_leaves': 30.93894471525664,\n",
              "   'min_child_weight': 1.082798654336554,\n",
              "   'subsample': 0.9757667044885082},\n",
              "  'target': -0.47793460000000004},\n",
              " {'params': {'alpha': 0.6066385177711031,\n",
              "   'colsample_bytree': 0.37162913781471246,\n",
              "   'gamma': 0.9339345979472408,\n",
              "   'lambda_1': 0.20669351262473834,\n",
              "   'max_depth': 30.914977056812166,\n",
              "   'max_leaves': 30.633124868091986,\n",
              "   'min_child_weight': 1.1312760988970825,\n",
              "   'subsample': 0.9027112530946451},\n",
              "  'target': -0.47768239999999995},\n",
              " {'params': {'alpha': 0.4706750813527566,\n",
              "   'colsample_bytree': 0.30608091347822336,\n",
              "   'gamma': 3.4763137187905464,\n",
              "   'lambda_1': 0.01685946056684373,\n",
              "   'max_depth': 30.986990314858062,\n",
              "   'max_leaves': 30.93559414307086,\n",
              "   'min_child_weight': 0.38047833041303947,\n",
              "   'subsample': 0.9350564854377228},\n",
              "  'target': -0.4778736},\n",
              " {'params': {'alpha': 0.7859446816504939,\n",
              "   'colsample_bytree': 0.44331717286228056,\n",
              "   'gamma': 4.791542436611314,\n",
              "   'lambda_1': 0.6379922446428183,\n",
              "   'max_depth': 30.88634923410997,\n",
              "   'max_leaves': 30.979913770207673,\n",
              "   'min_child_weight': 1.5143419566364833,\n",
              "   'subsample': 0.987720388424556},\n",
              "  'target': -0.4780542},\n",
              " {'params': {'alpha': 0.7789091865702664,\n",
              "   'colsample_bytree': 0.37553195744921425,\n",
              "   'gamma': 4.461669867296688,\n",
              "   'lambda_1': 0.9973000727835762,\n",
              "   'max_depth': 7.515387446189982,\n",
              "   'max_leaves': 30.95294778702042,\n",
              "   'min_child_weight': 1.6659917658554397,\n",
              "   'subsample': 0.9600795862751299},\n",
              "  'target': -0.47748999999999997},\n",
              " {'params': {'alpha': 0.9836602177934417,\n",
              "   'colsample_bytree': 0.4931484089238944,\n",
              "   'gamma': 0.815941762992165,\n",
              "   'lambda_1': 0.40516553364745167,\n",
              "   'max_depth': 7.20607628268003,\n",
              "   'max_leaves': 30.859366581113598,\n",
              "   'min_child_weight': 1.8806498291765255,\n",
              "   'subsample': 0.9577397395370639},\n",
              "  'target': -0.4775324},\n",
              " {'params': {'alpha': 0.9518586583195692,\n",
              "   'colsample_bytree': 0.5381671637831394,\n",
              "   'gamma': 3.3197638110033676,\n",
              "   'lambda_1': 0.15918430155915964,\n",
              "   'max_depth': 7.5456903936452395,\n",
              "   'max_leaves': 30.993428171669105,\n",
              "   'min_child_weight': 0.021260254009545676,\n",
              "   'subsample': 0.9335208069469152},\n",
              "  'target': -0.47778660000000006},\n",
              " {'params': {'alpha': 0.3671809820143289,\n",
              "   'colsample_bytree': 0.5456027002869009,\n",
              "   'gamma': 4.81709206613555,\n",
              "   'lambda_1': 0.7648128971695414,\n",
              "   'max_depth': 7.175275084858589,\n",
              "   'max_leaves': 30.611832594467597,\n",
              "   'min_child_weight': 0.5626605154472002,\n",
              "   'subsample': 0.9574112916154142},\n",
              "  'target': -0.4779226},\n",
              " {'params': {'alpha': 0.23164528858420486,\n",
              "   'colsample_bytree': 0.32313293704887003,\n",
              "   'gamma': 0.34790104692309587,\n",
              "   'lambda_1': 0.290236221750942,\n",
              "   'max_depth': 30.929393432856614,\n",
              "   'max_leaves': 30.607452418112015,\n",
              "   'min_child_weight': 0.5475925095540268,\n",
              "   'subsample': 0.9037106657128688},\n",
              "  'target': -0.4777276},\n",
              " {'params': {'alpha': 0.5324391044092541,\n",
              "   'colsample_bytree': 0.46905125592639163,\n",
              "   'gamma': 4.870218435269706,\n",
              "   'lambda_1': 0.43540626230726065,\n",
              "   'max_depth': 7.063147218622463,\n",
              "   'max_leaves': 30.8507717855773,\n",
              "   'min_child_weight': 1.8294442770439128,\n",
              "   'subsample': 0.9473718501509957},\n",
              "  'target': -0.47764000000000006},\n",
              " {'params': {'alpha': 0.05191894402286146,\n",
              "   'colsample_bytree': 0.35224576993204054,\n",
              "   'gamma': 0.9857746908255871,\n",
              "   'lambda_1': 0.03207996416357073,\n",
              "   'max_depth': 7.422897106312337,\n",
              "   'max_leaves': 30.797644158351808,\n",
              "   'min_child_weight': 0.5942897085645156,\n",
              "   'subsample': 0.9264681991687957},\n",
              "  'target': -0.4782312},\n",
              " {'params': {'alpha': 0.745392494850252,\n",
              "   'colsample_bytree': 0.38761749933665046,\n",
              "   'gamma': 4.917397019410395,\n",
              "   'lambda_1': 0.0576459436912633,\n",
              "   'max_depth': 30.210225477777843,\n",
              "   'max_leaves': 30.939122512384017,\n",
              "   'min_child_weight': 1.9918411312973814,\n",
              "   'subsample': 0.9441818730290886},\n",
              "  'target': -0.4776214},\n",
              " {'params': {'alpha': 0.07510555646276984,\n",
              "   'colsample_bytree': 0.5566045459123692,\n",
              "   'gamma': 0.05800912372355027,\n",
              "   'lambda_1': 0.3750006353597579,\n",
              "   'max_depth': 30.466861928426084,\n",
              "   'max_leaves': 30.924649100887123,\n",
              "   'min_child_weight': 0.7887672083604644,\n",
              "   'subsample': 0.9650268137005732},\n",
              "  'target': -0.47814579999999995},\n",
              " {'params': {'alpha': 0.15571998590503067,\n",
              "   'colsample_bytree': 0.5506950902849056,\n",
              "   'gamma': 4.784269006665473,\n",
              "   'lambda_1': 0.6974315543147172,\n",
              "   'max_depth': 7.058234235953067,\n",
              "   'max_leaves': 30.833292405912196,\n",
              "   'min_child_weight': 0.8003647475969362,\n",
              "   'subsample': 0.9802969051427313},\n",
              "  'target': -0.4782676},\n",
              " {'params': {'alpha': 0.22095371761657745,\n",
              "   'colsample_bytree': 0.33521203440287695,\n",
              "   'gamma': 0.38348690404344077,\n",
              "   'lambda_1': 0.36073977430316617,\n",
              "   'max_depth': 29.857959447486934,\n",
              "   'max_leaves': 30.95089197558036,\n",
              "   'min_child_weight': 1.921203646570414,\n",
              "   'subsample': 0.9495105539722276},\n",
              "  'target': -0.4780082},\n",
              " {'params': {'alpha': 0.8430028628689652,\n",
              "   'colsample_bytree': 0.3972287727301807,\n",
              "   'gamma': 4.3330011316580554,\n",
              "   'lambda_1': 0.07787503184260458,\n",
              "   'max_depth': 7.312516456696857,\n",
              "   'max_leaves': 30.98285697465456,\n",
              "   'min_child_weight': 0.15283098846557785,\n",
              "   'subsample': 0.9399905423362176},\n",
              "  'target': -0.47760759999999997},\n",
              " {'params': {'alpha': 0.5419429126791687,\n",
              "   'colsample_bytree': 0.41775238760628963,\n",
              "   'gamma': 4.925818797004842,\n",
              "   'lambda_1': 0.3516742219803276,\n",
              "   'max_depth': 30.63924618539355,\n",
              "   'max_leaves': 30.898215049304163,\n",
              "   'min_child_weight': 1.494735998471878,\n",
              "   'subsample': 0.9690951482792418},\n",
              "  'target': -0.47793340000000006},\n",
              " {'params': {'alpha': 0.8417066909712568,\n",
              "   'colsample_bytree': 0.5471950360059656,\n",
              "   'gamma': 0.17971253355309913,\n",
              "   'lambda_1': 0.6014871399730652,\n",
              "   'max_depth': 7.01103025996438,\n",
              "   'max_leaves': 30.397565416740207,\n",
              "   'min_child_weight': 0.10548552197985561,\n",
              "   'subsample': 0.975788317858568},\n",
              "  'target': -0.47770140000000005},\n",
              " {'params': {'alpha': 0.7926608801038096,\n",
              "   'colsample_bytree': 0.3118106515219469,\n",
              "   'gamma': 0.05372003305748607,\n",
              "   'lambda_1': 0.8361462617603301,\n",
              "   'max_depth': 7.130655789370537,\n",
              "   'max_leaves': 30.71536789407153,\n",
              "   'min_child_weight': 0.40711733836756414,\n",
              "   'subsample': 0.9045235335367476},\n",
              "  'target': -0.4773866},\n",
              " {'params': {'alpha': 0.12053782401417557,\n",
              "   'colsample_bytree': 0.5948764552374435,\n",
              "   'gamma': 4.997163082858233,\n",
              "   'lambda_1': 0.7934916126074963,\n",
              "   'max_depth': 7.3964238502109385,\n",
              "   'max_leaves': 30.770307456306874,\n",
              "   'min_child_weight': 0.2945924013899015,\n",
              "   'subsample': 0.998704800749909},\n",
              "  'target': -0.47857659999999996},\n",
              " {'params': {'alpha': 0.16802080211005066,\n",
              "   'colsample_bytree': 0.3755839508182214,\n",
              "   'gamma': 0.18410023738585912,\n",
              "   'lambda_1': 0.6824782113386583,\n",
              "   'max_depth': 30.399775237373852,\n",
              "   'max_leaves': 30.925437099549143,\n",
              "   'min_child_weight': 0.5249110575225031,\n",
              "   'subsample': 0.9660503796246943},\n",
              "  'target': -0.4777158},\n",
              " {'params': {'alpha': 0.02641892795843981,\n",
              "   'colsample_bytree': 0.3835885738569553,\n",
              "   'gamma': 0.13502239596549503,\n",
              "   'lambda_1': 0.8312817251640933,\n",
              "   'max_depth': 7.004938296880049,\n",
              "   'max_leaves': 30.88034416109307,\n",
              "   'min_child_weight': 1.224934639987496,\n",
              "   'subsample': 0.9184785254835455},\n",
              "  'target': -0.4777912000000001},\n",
              " {'params': {'alpha': 0.4205963476515996,\n",
              "   'colsample_bytree': 0.411620724433636,\n",
              "   'gamma': 0.13933983775473613,\n",
              "   'lambda_1': 0.8335658283323252,\n",
              "   'max_depth': 30.22329301014227,\n",
              "   'max_leaves': 30.873791041693796,\n",
              "   'min_child_weight': 0.43847866299953786,\n",
              "   'subsample': 0.9011566575840817},\n",
              "  'target': -0.47751140000000003},\n",
              " {'params': {'alpha': 0.6893647794118353,\n",
              "   'colsample_bytree': 0.3082872289501678,\n",
              "   'gamma': 0.16560750234327126,\n",
              "   'lambda_1': 0.7363621472291056,\n",
              "   'max_depth': 30.937493512487297,\n",
              "   'max_leaves': 30.88617445826226,\n",
              "   'min_child_weight': 1.5702922618497404,\n",
              "   'subsample': 0.9333823338214866},\n",
              "  'target': -0.47733480000000006},\n",
              " {'params': {'alpha': 0.839936306076137,\n",
              "   'colsample_bytree': 0.3280502180190522,\n",
              "   'gamma': 0.007738833611045659,\n",
              "   'lambda_1': 0.12506315546392444,\n",
              "   'max_depth': 29.866395150704783,\n",
              "   'max_leaves': 30.9894171217007,\n",
              "   'min_child_weight': 0.4445181918220982,\n",
              "   'subsample': 0.982465376867488},\n",
              "  'target': -0.4777074},\n",
              " {'params': {'alpha': 0.5340070244233975,\n",
              "   'colsample_bytree': 0.4431084056272281,\n",
              "   'gamma': 4.788785915310297,\n",
              "   'lambda_1': 0.02196570794932562,\n",
              "   'max_depth': 30.708805507013235,\n",
              "   'max_leaves': 30.99277082107767,\n",
              "   'min_child_weight': 0.8436368736686337,\n",
              "   'subsample': 0.9461291266628706},\n",
              "  'target': -0.477897},\n",
              " {'params': {'alpha': 0.7129844520182512,\n",
              "   'colsample_bytree': 0.31214604090786174,\n",
              "   'gamma': 0.13765971147288336,\n",
              "   'lambda_1': 0.22890790096151326,\n",
              "   'max_depth': 7.0335894073240866,\n",
              "   'max_leaves': 30.95180624792588,\n",
              "   'min_child_weight': 0.9344954513860391,\n",
              "   'subsample': 0.9367517674707991},\n",
              "  'target': -0.4778448},\n",
              " {'params': {'alpha': 0.06949276761514689,\n",
              "   'colsample_bytree': 0.3833768870129593,\n",
              "   'gamma': 0.031064673842379764,\n",
              "   'lambda_1': 0.2922470853307376,\n",
              "   'max_depth': 7.633704345747812,\n",
              "   'max_leaves': 30.874067499203967,\n",
              "   'min_child_weight': 1.7083098652137796,\n",
              "   'subsample': 0.9563137021166247},\n",
              "  'target': -0.4780912}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guiQN9tgkvw2",
        "colab_type": "code",
        "outputId": "63d34a90-4e5f-45fc-cba5-0644905afb62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "xgb_BO_scores = pd.DataFrame()\n",
        "for dic in xgb_BO.res:\n",
        "  param = dic['params']\n",
        "  param = pd.DataFrame([param])\n",
        "  xgb_BO_scores = pd.concat([xgb_BO_scores, param], ignore_index=True)\n",
        "\n",
        "xgb_BO_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>colsample_bytree</th>\n",
              "      <th>gamma</th>\n",
              "      <th>lambda_1</th>\n",
              "      <th>max_depth</th>\n",
              "      <th>max_leaves</th>\n",
              "      <th>min_child_weight</th>\n",
              "      <th>subsample</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.944341</td>\n",
              "      <td>0.355843</td>\n",
              "      <td>4.696188</td>\n",
              "      <td>0.651843</td>\n",
              "      <td>17.312640</td>\n",
              "      <td>27.541825</td>\n",
              "      <td>0.752837</td>\n",
              "      <td>0.907725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.613797</td>\n",
              "      <td>0.313655</td>\n",
              "      <td>4.151448</td>\n",
              "      <td>0.350348</td>\n",
              "      <td>18.688070</td>\n",
              "      <td>30.809364</td>\n",
              "      <td>1.544144</td>\n",
              "      <td>0.948846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.768084</td>\n",
              "      <td>0.526285</td>\n",
              "      <td>1.328581</td>\n",
              "      <td>0.130378</td>\n",
              "      <td>24.604570</td>\n",
              "      <td>27.636220</td>\n",
              "      <td>0.512602</td>\n",
              "      <td>0.942960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.860323</td>\n",
              "      <td>0.444782</td>\n",
              "      <td>0.021859</td>\n",
              "      <td>0.361724</td>\n",
              "      <td>14.151384</td>\n",
              "      <td>27.120604</td>\n",
              "      <td>1.385639</td>\n",
              "      <td>0.929324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.832833</td>\n",
              "      <td>0.501954</td>\n",
              "      <td>3.999739</td>\n",
              "      <td>0.582264</td>\n",
              "      <td>27.911008</td>\n",
              "      <td>28.413342</td>\n",
              "      <td>1.546214</td>\n",
              "      <td>0.946224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.539332</td>\n",
              "      <td>0.592977</td>\n",
              "      <td>0.196068</td>\n",
              "      <td>0.655193</td>\n",
              "      <td>7.996903</td>\n",
              "      <td>7.053918</td>\n",
              "      <td>0.538716</td>\n",
              "      <td>0.908648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.110324</td>\n",
              "      <td>0.340798</td>\n",
              "      <td>4.937913</td>\n",
              "      <td>0.856564</td>\n",
              "      <td>30.941440</td>\n",
              "      <td>7.470959</td>\n",
              "      <td>0.780997</td>\n",
              "      <td>0.904248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.774069</td>\n",
              "      <td>0.447806</td>\n",
              "      <td>4.682184</td>\n",
              "      <td>0.165398</td>\n",
              "      <td>7.484805</td>\n",
              "      <td>30.986257</td>\n",
              "      <td>0.455631</td>\n",
              "      <td>0.935090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.402336</td>\n",
              "      <td>0.322630</td>\n",
              "      <td>0.228134</td>\n",
              "      <td>0.433430</td>\n",
              "      <td>30.305566</td>\n",
              "      <td>30.453953</td>\n",
              "      <td>0.005969</td>\n",
              "      <td>0.927511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.441223</td>\n",
              "      <td>0.511408</td>\n",
              "      <td>0.959607</td>\n",
              "      <td>0.949367</td>\n",
              "      <td>7.139717</td>\n",
              "      <td>30.826017</td>\n",
              "      <td>1.774899</td>\n",
              "      <td>0.914564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.286012</td>\n",
              "      <td>0.599310</td>\n",
              "      <td>0.120647</td>\n",
              "      <td>0.716386</td>\n",
              "      <td>30.355642</td>\n",
              "      <td>30.890109</td>\n",
              "      <td>0.069507</td>\n",
              "      <td>0.930800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.591509</td>\n",
              "      <td>0.392677</td>\n",
              "      <td>0.077174</td>\n",
              "      <td>0.214887</td>\n",
              "      <td>7.358515</td>\n",
              "      <td>30.855034</td>\n",
              "      <td>1.638716</td>\n",
              "      <td>0.901915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.057157</td>\n",
              "      <td>0.325802</td>\n",
              "      <td>4.829667</td>\n",
              "      <td>0.347855</td>\n",
              "      <td>7.936818</td>\n",
              "      <td>30.992959</td>\n",
              "      <td>1.430398</td>\n",
              "      <td>0.988623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.149275</td>\n",
              "      <td>0.303447</td>\n",
              "      <td>4.916083</td>\n",
              "      <td>0.328057</td>\n",
              "      <td>30.744425</td>\n",
              "      <td>30.886897</td>\n",
              "      <td>1.695487</td>\n",
              "      <td>0.969483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.358813</td>\n",
              "      <td>0.594816</td>\n",
              "      <td>1.820157</td>\n",
              "      <td>0.434129</td>\n",
              "      <td>7.209459</td>\n",
              "      <td>30.970797</td>\n",
              "      <td>1.523665</td>\n",
              "      <td>0.953468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.037755</td>\n",
              "      <td>0.555710</td>\n",
              "      <td>0.261226</td>\n",
              "      <td>0.713623</td>\n",
              "      <td>30.955893</td>\n",
              "      <td>30.956819</td>\n",
              "      <td>0.619438</td>\n",
              "      <td>0.935697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.872333</td>\n",
              "      <td>0.429216</td>\n",
              "      <td>0.489085</td>\n",
              "      <td>0.216068</td>\n",
              "      <td>7.419561</td>\n",
              "      <td>30.918032</td>\n",
              "      <td>0.435122</td>\n",
              "      <td>0.950037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.232433</td>\n",
              "      <td>0.458381</td>\n",
              "      <td>0.137488</td>\n",
              "      <td>0.458013</td>\n",
              "      <td>7.096412</td>\n",
              "      <td>30.938945</td>\n",
              "      <td>1.082799</td>\n",
              "      <td>0.975767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.606639</td>\n",
              "      <td>0.371629</td>\n",
              "      <td>0.933935</td>\n",
              "      <td>0.206694</td>\n",
              "      <td>30.914977</td>\n",
              "      <td>30.633125</td>\n",
              "      <td>1.131276</td>\n",
              "      <td>0.902711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.470675</td>\n",
              "      <td>0.306081</td>\n",
              "      <td>3.476314</td>\n",
              "      <td>0.016859</td>\n",
              "      <td>30.986990</td>\n",
              "      <td>30.935594</td>\n",
              "      <td>0.380478</td>\n",
              "      <td>0.935056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.785945</td>\n",
              "      <td>0.443317</td>\n",
              "      <td>4.791542</td>\n",
              "      <td>0.637992</td>\n",
              "      <td>30.886349</td>\n",
              "      <td>30.979914</td>\n",
              "      <td>1.514342</td>\n",
              "      <td>0.987720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.778909</td>\n",
              "      <td>0.375532</td>\n",
              "      <td>4.461670</td>\n",
              "      <td>0.997300</td>\n",
              "      <td>7.515387</td>\n",
              "      <td>30.952948</td>\n",
              "      <td>1.665992</td>\n",
              "      <td>0.960080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.983660</td>\n",
              "      <td>0.493148</td>\n",
              "      <td>0.815942</td>\n",
              "      <td>0.405166</td>\n",
              "      <td>7.206076</td>\n",
              "      <td>30.859367</td>\n",
              "      <td>1.880650</td>\n",
              "      <td>0.957740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.951859</td>\n",
              "      <td>0.538167</td>\n",
              "      <td>3.319764</td>\n",
              "      <td>0.159184</td>\n",
              "      <td>7.545690</td>\n",
              "      <td>30.993428</td>\n",
              "      <td>0.021260</td>\n",
              "      <td>0.933521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.367181</td>\n",
              "      <td>0.545603</td>\n",
              "      <td>4.817092</td>\n",
              "      <td>0.764813</td>\n",
              "      <td>7.175275</td>\n",
              "      <td>30.611833</td>\n",
              "      <td>0.562661</td>\n",
              "      <td>0.957411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.231645</td>\n",
              "      <td>0.323133</td>\n",
              "      <td>0.347901</td>\n",
              "      <td>0.290236</td>\n",
              "      <td>30.929393</td>\n",
              "      <td>30.607452</td>\n",
              "      <td>0.547593</td>\n",
              "      <td>0.903711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.532439</td>\n",
              "      <td>0.469051</td>\n",
              "      <td>4.870218</td>\n",
              "      <td>0.435406</td>\n",
              "      <td>7.063147</td>\n",
              "      <td>30.850772</td>\n",
              "      <td>1.829444</td>\n",
              "      <td>0.947372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.051919</td>\n",
              "      <td>0.352246</td>\n",
              "      <td>0.985775</td>\n",
              "      <td>0.032080</td>\n",
              "      <td>7.422897</td>\n",
              "      <td>30.797644</td>\n",
              "      <td>0.594290</td>\n",
              "      <td>0.926468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.745392</td>\n",
              "      <td>0.387617</td>\n",
              "      <td>4.917397</td>\n",
              "      <td>0.057646</td>\n",
              "      <td>30.210225</td>\n",
              "      <td>30.939123</td>\n",
              "      <td>1.991841</td>\n",
              "      <td>0.944182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.075106</td>\n",
              "      <td>0.556605</td>\n",
              "      <td>0.058009</td>\n",
              "      <td>0.375001</td>\n",
              "      <td>30.466862</td>\n",
              "      <td>30.924649</td>\n",
              "      <td>0.788767</td>\n",
              "      <td>0.965027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.155720</td>\n",
              "      <td>0.550695</td>\n",
              "      <td>4.784269</td>\n",
              "      <td>0.697432</td>\n",
              "      <td>7.058234</td>\n",
              "      <td>30.833292</td>\n",
              "      <td>0.800365</td>\n",
              "      <td>0.980297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.220954</td>\n",
              "      <td>0.335212</td>\n",
              "      <td>0.383487</td>\n",
              "      <td>0.360740</td>\n",
              "      <td>29.857959</td>\n",
              "      <td>30.950892</td>\n",
              "      <td>1.921204</td>\n",
              "      <td>0.949511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.843003</td>\n",
              "      <td>0.397229</td>\n",
              "      <td>4.333001</td>\n",
              "      <td>0.077875</td>\n",
              "      <td>7.312516</td>\n",
              "      <td>30.982857</td>\n",
              "      <td>0.152831</td>\n",
              "      <td>0.939991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.541943</td>\n",
              "      <td>0.417752</td>\n",
              "      <td>4.925819</td>\n",
              "      <td>0.351674</td>\n",
              "      <td>30.639246</td>\n",
              "      <td>30.898215</td>\n",
              "      <td>1.494736</td>\n",
              "      <td>0.969095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.841707</td>\n",
              "      <td>0.547195</td>\n",
              "      <td>0.179713</td>\n",
              "      <td>0.601487</td>\n",
              "      <td>7.011030</td>\n",
              "      <td>30.397565</td>\n",
              "      <td>0.105486</td>\n",
              "      <td>0.975788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.792661</td>\n",
              "      <td>0.311811</td>\n",
              "      <td>0.053720</td>\n",
              "      <td>0.836146</td>\n",
              "      <td>7.130656</td>\n",
              "      <td>30.715368</td>\n",
              "      <td>0.407117</td>\n",
              "      <td>0.904524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.120538</td>\n",
              "      <td>0.594876</td>\n",
              "      <td>4.997163</td>\n",
              "      <td>0.793492</td>\n",
              "      <td>7.396424</td>\n",
              "      <td>30.770307</td>\n",
              "      <td>0.294592</td>\n",
              "      <td>0.998705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.168021</td>\n",
              "      <td>0.375584</td>\n",
              "      <td>0.184100</td>\n",
              "      <td>0.682478</td>\n",
              "      <td>30.399775</td>\n",
              "      <td>30.925437</td>\n",
              "      <td>0.524911</td>\n",
              "      <td>0.966050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.026419</td>\n",
              "      <td>0.383589</td>\n",
              "      <td>0.135022</td>\n",
              "      <td>0.831282</td>\n",
              "      <td>7.004938</td>\n",
              "      <td>30.880344</td>\n",
              "      <td>1.224935</td>\n",
              "      <td>0.918479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.420596</td>\n",
              "      <td>0.411621</td>\n",
              "      <td>0.139340</td>\n",
              "      <td>0.833566</td>\n",
              "      <td>30.223293</td>\n",
              "      <td>30.873791</td>\n",
              "      <td>0.438479</td>\n",
              "      <td>0.901157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.689365</td>\n",
              "      <td>0.308287</td>\n",
              "      <td>0.165608</td>\n",
              "      <td>0.736362</td>\n",
              "      <td>30.937494</td>\n",
              "      <td>30.886174</td>\n",
              "      <td>1.570292</td>\n",
              "      <td>0.933382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.839936</td>\n",
              "      <td>0.328050</td>\n",
              "      <td>0.007739</td>\n",
              "      <td>0.125063</td>\n",
              "      <td>29.866395</td>\n",
              "      <td>30.989417</td>\n",
              "      <td>0.444518</td>\n",
              "      <td>0.982465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.534007</td>\n",
              "      <td>0.443108</td>\n",
              "      <td>4.788786</td>\n",
              "      <td>0.021966</td>\n",
              "      <td>30.708806</td>\n",
              "      <td>30.992771</td>\n",
              "      <td>0.843637</td>\n",
              "      <td>0.946129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.712984</td>\n",
              "      <td>0.312146</td>\n",
              "      <td>0.137660</td>\n",
              "      <td>0.228908</td>\n",
              "      <td>7.033589</td>\n",
              "      <td>30.951806</td>\n",
              "      <td>0.934495</td>\n",
              "      <td>0.936752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.069493</td>\n",
              "      <td>0.383377</td>\n",
              "      <td>0.031065</td>\n",
              "      <td>0.292247</td>\n",
              "      <td>7.633704</td>\n",
              "      <td>30.874067</td>\n",
              "      <td>1.708310</td>\n",
              "      <td>0.956314</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       alpha  colsample_bytree  ...  min_child_weight  subsample\n",
              "0   0.944341          0.355843  ...          0.752837   0.907725\n",
              "1   0.613797          0.313655  ...          1.544144   0.948846\n",
              "2   0.768084          0.526285  ...          0.512602   0.942960\n",
              "3   0.860323          0.444782  ...          1.385639   0.929324\n",
              "4   0.832833          0.501954  ...          1.546214   0.946224\n",
              "5   0.539332          0.592977  ...          0.538716   0.908648\n",
              "6   0.110324          0.340798  ...          0.780997   0.904248\n",
              "7   0.774069          0.447806  ...          0.455631   0.935090\n",
              "8   0.402336          0.322630  ...          0.005969   0.927511\n",
              "9   0.441223          0.511408  ...          1.774899   0.914564\n",
              "10  0.286012          0.599310  ...          0.069507   0.930800\n",
              "11  0.591509          0.392677  ...          1.638716   0.901915\n",
              "12  0.057157          0.325802  ...          1.430398   0.988623\n",
              "13  0.149275          0.303447  ...          1.695487   0.969483\n",
              "14  0.358813          0.594816  ...          1.523665   0.953468\n",
              "15  0.037755          0.555710  ...          0.619438   0.935697\n",
              "16  0.872333          0.429216  ...          0.435122   0.950037\n",
              "17  0.232433          0.458381  ...          1.082799   0.975767\n",
              "18  0.606639          0.371629  ...          1.131276   0.902711\n",
              "19  0.470675          0.306081  ...          0.380478   0.935056\n",
              "20  0.785945          0.443317  ...          1.514342   0.987720\n",
              "21  0.778909          0.375532  ...          1.665992   0.960080\n",
              "22  0.983660          0.493148  ...          1.880650   0.957740\n",
              "23  0.951859          0.538167  ...          0.021260   0.933521\n",
              "24  0.367181          0.545603  ...          0.562661   0.957411\n",
              "25  0.231645          0.323133  ...          0.547593   0.903711\n",
              "26  0.532439          0.469051  ...          1.829444   0.947372\n",
              "27  0.051919          0.352246  ...          0.594290   0.926468\n",
              "28  0.745392          0.387617  ...          1.991841   0.944182\n",
              "29  0.075106          0.556605  ...          0.788767   0.965027\n",
              "30  0.155720          0.550695  ...          0.800365   0.980297\n",
              "31  0.220954          0.335212  ...          1.921204   0.949511\n",
              "32  0.843003          0.397229  ...          0.152831   0.939991\n",
              "33  0.541943          0.417752  ...          1.494736   0.969095\n",
              "34  0.841707          0.547195  ...          0.105486   0.975788\n",
              "35  0.792661          0.311811  ...          0.407117   0.904524\n",
              "36  0.120538          0.594876  ...          0.294592   0.998705\n",
              "37  0.168021          0.375584  ...          0.524911   0.966050\n",
              "38  0.026419          0.383589  ...          1.224935   0.918479\n",
              "39  0.420596          0.411621  ...          0.438479   0.901157\n",
              "40  0.689365          0.308287  ...          1.570292   0.933382\n",
              "41  0.839936          0.328050  ...          0.444518   0.982465\n",
              "42  0.534007          0.443108  ...          0.843637   0.946129\n",
              "43  0.712984          0.312146  ...          0.934495   0.936752\n",
              "44  0.069493          0.383377  ...          1.708310   0.956314\n",
              "\n",
              "[45 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBHFRn1E8T8R",
        "colab_type": "code",
        "outputId": "1053ee89-7c3d-497a-d74d-9433412a682e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "xgb_BO_scores['score'] = pd.DataFrame(xgb_BO.res)['target']\n",
        "xgb_BO_scores.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>colsample_bytree</th>\n",
              "      <th>gamma</th>\n",
              "      <th>lambda_1</th>\n",
              "      <th>max_depth</th>\n",
              "      <th>max_leaves</th>\n",
              "      <th>min_child_weight</th>\n",
              "      <th>subsample</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.944341</td>\n",
              "      <td>0.355843</td>\n",
              "      <td>4.696188</td>\n",
              "      <td>0.651843</td>\n",
              "      <td>17.312640</td>\n",
              "      <td>27.541825</td>\n",
              "      <td>0.752837</td>\n",
              "      <td>0.907725</td>\n",
              "      <td>-0.477470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.613797</td>\n",
              "      <td>0.313655</td>\n",
              "      <td>4.151448</td>\n",
              "      <td>0.350348</td>\n",
              "      <td>18.688070</td>\n",
              "      <td>30.809364</td>\n",
              "      <td>1.544144</td>\n",
              "      <td>0.948846</td>\n",
              "      <td>-0.477573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.768084</td>\n",
              "      <td>0.526285</td>\n",
              "      <td>1.328581</td>\n",
              "      <td>0.130378</td>\n",
              "      <td>24.604570</td>\n",
              "      <td>27.636220</td>\n",
              "      <td>0.512602</td>\n",
              "      <td>0.942960</td>\n",
              "      <td>-0.478017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.860323</td>\n",
              "      <td>0.444782</td>\n",
              "      <td>0.021859</td>\n",
              "      <td>0.361724</td>\n",
              "      <td>14.151384</td>\n",
              "      <td>27.120604</td>\n",
              "      <td>1.385639</td>\n",
              "      <td>0.929324</td>\n",
              "      <td>-0.477625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.832833</td>\n",
              "      <td>0.501954</td>\n",
              "      <td>3.999739</td>\n",
              "      <td>0.582264</td>\n",
              "      <td>27.911008</td>\n",
              "      <td>28.413342</td>\n",
              "      <td>1.546214</td>\n",
              "      <td>0.946224</td>\n",
              "      <td>-0.477818</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      alpha  colsample_bytree     gamma  ...  min_child_weight  subsample     score\n",
              "0  0.944341          0.355843  4.696188  ...          0.752837   0.907725 -0.477470\n",
              "1  0.613797          0.313655  4.151448  ...          1.544144   0.948846 -0.477573\n",
              "2  0.768084          0.526285  1.328581  ...          0.512602   0.942960 -0.478017\n",
              "3  0.860323          0.444782  0.021859  ...          1.385639   0.929324 -0.477625\n",
              "4  0.832833          0.501954  3.999739  ...          1.546214   0.946224 -0.477818\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zzqeEd-7GbO",
        "colab_type": "code",
        "outputId": "b4c7872a-f341-4fcd-f0c9-41b90bdaf6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "xgb_BO_scores = xgb_BO_scores.sort_values(by='score',ascending=False)\n",
        "xgb_BO_scores.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>colsample_bytree</th>\n",
              "      <th>gamma</th>\n",
              "      <th>lambda_1</th>\n",
              "      <th>max_depth</th>\n",
              "      <th>max_leaves</th>\n",
              "      <th>min_child_weight</th>\n",
              "      <th>subsample</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.689365</td>\n",
              "      <td>0.308287</td>\n",
              "      <td>0.165608</td>\n",
              "      <td>0.736362</td>\n",
              "      <td>30.937494</td>\n",
              "      <td>30.886174</td>\n",
              "      <td>1.570292</td>\n",
              "      <td>0.933382</td>\n",
              "      <td>-0.477335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.792661</td>\n",
              "      <td>0.311811</td>\n",
              "      <td>0.053720</td>\n",
              "      <td>0.836146</td>\n",
              "      <td>7.130656</td>\n",
              "      <td>30.715368</td>\n",
              "      <td>0.407117</td>\n",
              "      <td>0.904524</td>\n",
              "      <td>-0.477387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.944341</td>\n",
              "      <td>0.355843</td>\n",
              "      <td>4.696188</td>\n",
              "      <td>0.651843</td>\n",
              "      <td>17.312640</td>\n",
              "      <td>27.541825</td>\n",
              "      <td>0.752837</td>\n",
              "      <td>0.907725</td>\n",
              "      <td>-0.477470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.778909</td>\n",
              "      <td>0.375532</td>\n",
              "      <td>4.461670</td>\n",
              "      <td>0.997300</td>\n",
              "      <td>7.515387</td>\n",
              "      <td>30.952948</td>\n",
              "      <td>1.665992</td>\n",
              "      <td>0.960080</td>\n",
              "      <td>-0.477490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.420596</td>\n",
              "      <td>0.411621</td>\n",
              "      <td>0.139340</td>\n",
              "      <td>0.833566</td>\n",
              "      <td>30.223293</td>\n",
              "      <td>30.873791</td>\n",
              "      <td>0.438479</td>\n",
              "      <td>0.901157</td>\n",
              "      <td>-0.477511</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       alpha  colsample_bytree     gamma  ...  min_child_weight  subsample     score\n",
              "40  0.689365          0.308287  0.165608  ...          1.570292   0.933382 -0.477335\n",
              "35  0.792661          0.311811  0.053720  ...          0.407117   0.904524 -0.477387\n",
              "0   0.944341          0.355843  4.696188  ...          0.752837   0.907725 -0.477470\n",
              "21  0.778909          0.375532  4.461670  ...          1.665992   0.960080 -0.477490\n",
              "39  0.420596          0.411621  0.139340  ...          0.438479   0.901157 -0.477511\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULCpUi5m7Kqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_BO_scores.to_csv(\"/content/drive/My Drive/Kaggle_Allstate/data/tuned_xgb_parameters_gpu.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZobiYtQ88gQ",
        "colab_type": "text"
      },
      "source": [
        "### Retrain the model with smaller learning rate \n",
        "\n",
        "lgb vs xgb:\n",
        "bagging_freq = Not have this feature \\\\\n",
        "num_leaves = max_leaves \\\\\n",
        "max_depth = max_depth \\\\\n",
        "min_gain_to_split = gamma \\\\\n",
        "feature_fraction = colsample_bytree \\\\\n",
        "bagging_fraction = subsample \\\\\n",
        "min_sum_hessian_in_leaf = min_child_weight \\\\\n",
        "lambda_l2 = lambda \\\\\n",
        "lambda_l1 = alpha \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tvNZsVr9NFQ",
        "colab_type": "code",
        "outputId": "8860ddd6-1333-4057-b113-7113f67dcffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "params = xgb_BO_scores.iloc[0].to_dict()\n",
        "best_auto_xgb_params = dict()\n",
        "best_auto_xgb_params['objective'] = 'reg:squarederror'\n",
        "best_auto_xgb_params[\"metric\"] = 'rmse'\n",
        "best_auto_xgb_params[\"tree_method\"] = 'gpu_hist'\n",
        "best_auto_xgb_params[\"gpu_id\"] = 0\n",
        "best_auto_xgb_params['learning_rate'] = 0.01 # Smaller learning rate\n",
        "best_auto_xgb_params['max_leaves'] = int(params['max_leaves'])    \n",
        "best_auto_xgb_params['max_depth'] = int(params['max_depth'])    \n",
        "best_auto_xgb_params['min_child_weight'] = int(params['min_child_weight'])\n",
        "best_auto_xgb_params['gamma'] = params['gamma']     \n",
        "best_auto_xgb_params['colsample_bytree'] = params['colsample_bytree']\n",
        "best_auto_xgb_params['subsample'] = params['subsample']\n",
        "best_auto_xgb_params['lambda'] = params['lambda_1']\n",
        "best_auto_xgb_params['alpha'] = params['alpha']\n",
        "best_auto_xgb_params['seed'] = 1234\n",
        "\n",
        "\n",
        "print (best_auto_xgb_params)\n",
        "\n",
        "xgb_cv = xgb.cv(best_auto_xgb_params,\n",
        "            xgb.DMatrix(train_x,\n",
        "                        label=train_y\n",
        "                        ),\n",
        "            num_boost_round=100000,\n",
        "            nfold=kfolds,\n",
        "            folds=list(skf.split(train_y)),\n",
        "            stratified=False,\n",
        "            early_stopping_rounds=50,\n",
        "            verbose_eval=500)\n",
        "\n",
        "best_xgb_score = min(xgb_cv['test-rmse-mean'])\n",
        "best_xgb_iteration = len(xgb_cv['test-rmse-mean'])\n",
        "print (', best_score: %f, best_iteration: %d' % (best_xgb_score, best_xgb_iteration))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'objective': 'reg:squarederror', 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'learning_rate': 0.01, 'max_leaves': 30, 'max_depth': 30, 'min_child_weight': 1, 'gamma': 0.16560750234327126, 'colsample_bytree': 0.3082872289501678, 'subsample': 0.9333823338214866, 'lambda': 0.7363621472291056, 'alpha': 0.6893647794118353, 'seed': 1234}\n",
            "[0]\ttrain-rmse:7.26358+0.000394456\ttest-rmse:7.26358+0.00160251\n",
            "[500]\ttrain-rmse:0.502699+0.000418735\ttest-rmse:0.505879+0.00184392\n",
            "[1000]\ttrain-rmse:0.483845+0.00045759\ttest-rmse:0.489541+0.00183452\n",
            "[1500]\ttrain-rmse:0.476802+0.000540656\ttest-rmse:0.484806+0.00187917\n",
            "[2000]\ttrain-rmse:0.472252+0.000573244\ttest-rmse:0.482347+0.0019262\n",
            "[2500]\ttrain-rmse:0.46882+0.00053982\ttest-rmse:0.480866+0.0019953\n",
            "[3000]\ttrain-rmse:0.466021+0.000535945\ttest-rmse:0.479908+0.00203895\n",
            "[3500]\ttrain-rmse:0.463573+0.000505705\ttest-rmse:0.479205+0.00207654\n",
            "[4000]\ttrain-rmse:0.461374+0.000486815\ttest-rmse:0.478675+0.0020993\n",
            "[4500]\ttrain-rmse:0.45937+0.000467675\ttest-rmse:0.478271+0.00213205\n",
            "[5000]\ttrain-rmse:0.457486+0.000456041\ttest-rmse:0.477959+0.00215023\n",
            "[5500]\ttrain-rmse:0.455717+0.000436282\ttest-rmse:0.477695+0.0021596\n",
            "[6000]\ttrain-rmse:0.454053+0.000463626\ttest-rmse:0.47749+0.00214505\n",
            "[6500]\ttrain-rmse:0.452459+0.00045215\ttest-rmse:0.477331+0.00215112\n",
            "[7000]\ttrain-rmse:0.450896+0.000454579\ttest-rmse:0.477177+0.00215205\n",
            "[7500]\ttrain-rmse:0.449399+0.000454411\ttest-rmse:0.477053+0.00215571\n",
            "[8000]\ttrain-rmse:0.44795+0.000454629\ttest-rmse:0.476939+0.00216766\n",
            "[8500]\ttrain-rmse:0.446537+0.000456353\ttest-rmse:0.476846+0.00217799\n",
            "[9000]\ttrain-rmse:0.44514+0.00046638\ttest-rmse:0.476763+0.00217914\n",
            "[9500]\ttrain-rmse:0.443776+0.000466211\ttest-rmse:0.476701+0.00218408\n",
            "[10000]\ttrain-rmse:0.442448+0.000464046\ttest-rmse:0.476655+0.00219129\n",
            "[10500]\ttrain-rmse:0.441131+0.000456224\ttest-rmse:0.476607+0.00218094\n",
            "[11000]\ttrain-rmse:0.439849+0.00044672\ttest-rmse:0.476563+0.00217994\n",
            "[11500]\ttrain-rmse:0.438588+0.000450176\ttest-rmse:0.476521+0.00218386\n",
            ", best_score: 0.476511, best_iteration: 11611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ_S61dUUAds",
        "colab_type": "text"
      },
      "source": [
        "## XGBosst blending function \n",
        "\n",
        "1. KFold.split()返回的是(train_idx, test_idx)，在旧版本中KFold()返回的是(train_idx, test_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adXe7YglTMGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold \n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvQTiKCqUbKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xgb_blend(xgb_params, train_x, train_y, test_x, kfolds, early_stopping_rounds=0, train_y_dummy=None):\n",
        "    \"\"\"\n",
        "    estimator: parameters \n",
        "    \"\"\"\n",
        "    print(f\"Blend {len(xgb_params)} estimators for {kfolds} folds\")\n",
        "    skf = KFold(n_splits=kfolds, random_state=1234)\n",
        "    skf_ids = list(skf.split(train_y))\n",
        "\n",
        "    train_blend_x = np.zeros((train_x.shape[0], len(xgb_params)))\n",
        "    test_blend_x = np.zeros((test_x.shape[0], len(xgb_params)))\n",
        "    blend_scores = np.zeros ((kfolds,len(xgb_params)))\n",
        "    best_rounds = np.zeros ((kfolds,len(xgb_params)))\n",
        "\n",
        "    print(\"Start stacking\")\n",
        "\n",
        "    for j, est in enumerate(xgb_params):\n",
        "        print(\"Stacking model\", j+1, est)\n",
        "        test_blend_x_j = np.zeros((test_x.shape[0]))\n",
        "        for i, (train_ids, val_ids) in enumerate(skf_ids):\n",
        "            start = time.time()\n",
        "            print (f\"Model {j+1} fold {i+1}\")\n",
        "            train_x_fold = train_x[train_ids]\n",
        "            train_y_fold = train_y[train_ids]\n",
        "            val_x_fold = train_x[val_ids]\n",
        "            val_y_fold = train_y[val_ids]\n",
        "            print(i, est)\n",
        "\n",
        "            if early_stopping_rounds == 0: \n",
        "                #without early stoping\n",
        "                num_boost_round = copy.deepcopy(params['num_boost_round'])\n",
        "                model = xgb.train(params, \n",
        "                                  xgb.DMatrix(train_x_fold, \n",
        "                                              train_y_fold\n",
        "                                              ),\n",
        "                                  num_boost_round=num_boost_round,\n",
        "                                  verbose_eval=500\n",
        "                                  )\n",
        "                val_y_predict_fold = model.predict(xgb.DMatrix(val_x_fold))\n",
        "                socre = log_mae(val_y_fold, val_y_predict_fold, 200)\n",
        "                print(f\"Score for Model {j+1} fold {i+1}: {score}\")\n",
        "                blend_scores[i, j] = score\n",
        "                train_blend_x[val_ids, j] = val_y_predict_fold\n",
        "                test_blend_x_j = test_blend_x_j + model.predict(xgb.DMatrix(test_x))\n",
        "                print(f\"Model {j+1} fold {i+1} finished in {time.time()-start} seconds.\")\n",
        "            else:\n",
        "                valid_data = xgb.DMatrix(val_x_fold, val_y_fold)\n",
        "                model = xgb.train(params,\n",
        "                                  xgb.DMatrix(train_x_fold,\n",
        "                                              train_y_fold\n",
        "                                              ),\n",
        "                                  evals=[(valid_data, 'valid')], \n",
        "                                  num_boost_round=10000000,\n",
        "                                  early_stopping_rounds=early_stopping_rounds, \n",
        "                                  verbose_eval=500)\n",
        "                best_iteration = model.best_iteration\n",
        "                best_ntree_limit = model.best_ntree_limit\n",
        "                best_rounds[i, j] = best_iteration\n",
        "                print(f\"best iteration: {best_iteration}\")\n",
        "                print(f\"best score: {model.best_score}\")\n",
        "                val_y_predict_fold = model.predict(xgb.DMatrix(val_x_fold), ntree_limit=best_ntree_limit)\n",
        "                score = log_mae(val_y_fold, val_y_predict_fold, 200)\n",
        "                print(f\"Score for Model {j+1} fold {i+1}: {score}\")\n",
        "                blend_scores[i, j] = score \n",
        "                train_blend_x[val_ids, j] = val_y_predict_fold\n",
        "                test_blend_x_j = test_blend_x_j + model.predict(xgb.DMatrix(test_x), ntree_limit=best_ntree_limit)\n",
        "                print(f\"Model {j+1} fold {i+1} finished in {time.time()-start}\")\n",
        "\n",
        "        test_blend_x[:, j]= test_blend_x_j / kfolds\n",
        "        print(f\"Score for model {j+1} is {np.mean(blend_scores[:, j])}\")\n",
        "    print(f\"Socre for blended models is {np.mean(blend_scores)}\")\n",
        "    return train_blend_x, test_blend_x, blend_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG42_DpSVh0y",
        "colab_type": "text"
      },
      "source": [
        "### Select top 4 XGBoost models \n",
        "\n",
        "lgb vs xgb:\n",
        "bagging_freq = Not have this feature \\\\\n",
        "num_leaves = max_leaves \\\\\n",
        "max_depth = max_depth \\\\\n",
        "min_gain_to_split = gamma \\\\\n",
        "feature_fraction = colsample_bytree \\\\\n",
        "bagging_fraction = subsample \\\\\n",
        "min_sum_hessian_in_leaf = min_child_weight \\\\\n",
        "lambda_l2 = lambda \\\\\n",
        "lambda_l1 = alpha \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysCLWftaaub0",
        "colab_type": "code",
        "outputId": "4ef23493-998d-4826-b930-9fb444a41203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "xgb_params = []\n",
        "## Top 5 auto-tuned parameters\n",
        "for i in range(4):\n",
        "    params=dict()\n",
        "    params['max_leaves'] = int(xgb_BO_scores['max_leaves'][i])\n",
        "    params['max_depth'] = int(xgb_BO_scores['max_depth'][i])\n",
        "    params['min_child_weight'] = int(xgb_BO_scores['min_child_weight'][i])\n",
        "    params['gamma'] = xgb_BO_scores['gamma'][i]\n",
        "    params['colsample_bytree'] = xgb_BO_scores['colsample_bytree'][i]\n",
        "    params['subsample'] = xgb_BO_scores['subsample'][i]\n",
        "    params['lambda_1'] = xgb_BO_scores['lambda_1'][i]\n",
        "    params['alpha'] = int(xgb_BO_scores['alpha'][i])\n",
        "    params['objective'] = 'reg:squarederror'\n",
        "    params['learning_rate'] = 0.05\n",
        "    params['num_boost_round']= best_xgb_iteration # use the result best_xgb_iteration from the cell Retrain the model with smaller learning rate, here use a example value \n",
        "    params['seed'] = 1234\n",
        "    params[\"metric\"] = 'rmse'\n",
        "    params[\"tree_method\"] = 'gpu_hist'\n",
        "    params[\"gpu_id\"] = 0\n",
        "    xgb_params.append(params)\n",
        "\n",
        "## Best manual-tuned parameters\n",
        "xgb_params.append(best_xgb_params)    \n",
        "print(xgb_params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'max_leaves': 27, 'max_depth': 17, 'min_child_weight': 0, 'gamma': 4.696188321837841, 'colsample_bytree': 0.3558431609964596, 'subsample': 0.9077252918981312, 'lambda_1': 0.6518433972257632, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}, {'max_leaves': 30, 'max_depth': 18, 'min_child_weight': 1, 'gamma': 4.151448278236447, 'colsample_bytree': 0.31365474212352024, 'subsample': 0.948846066577156, 'lambda_1': 0.35034786633150883, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}, {'max_leaves': 27, 'max_depth': 24, 'min_child_weight': 0, 'gamma': 1.32858144342245, 'colsample_bytree': 0.5262847718961265, 'subsample': 0.9429597651457532, 'lambda_1': 0.13037813495797712, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}, {'max_leaves': 27, 'max_depth': 14, 'min_child_weight': 1, 'gamma': 0.021858932008684828, 'colsample_bytree': 0.4447821485094775, 'subsample': 0.929323530126744, 'lambda_1': 0.3617239869110247, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}, {'learning_rate': 0.05, 'metrics': 'rmse', 'subsample': 0.8, 'seed': 1234, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'max_leaves': 127, 'max_depth': 7, 'gamma': 0, 'colsample_bytree': 0.1, 'min_child_weight': 100, 'lambda': 100, 'alpha': 10}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bsJbYpBbG9d",
        "colab_type": "code",
        "outputId": "b983ca29-a96c-43e5-fef0-65db9fc34d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_blend_x_xgb_ohe, test_blend_x_xgb_ohe, blend_scores_xgb_ohe = xgb_blend(xgb_params, train_x, train_y, test_x, 4, 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Blend 5 estimators for 4 folds\n",
            "Start stacking\n",
            "Stacking model 1 {'max_leaves': 27, 'max_depth': 17, 'min_child_weight': 0, 'gamma': 4.696188321837841, 'colsample_bytree': 0.3558431609964596, 'subsample': 0.9077252918981312, 'lambda_1': 0.6518433972257632, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "Model 1 fold 1\n",
            "0 {'max_leaves': 27, 'max_depth': 17, 'min_child_weight': 0, 'gamma': 4.696188321837841, 'colsample_bytree': 0.3558431609964596, 'subsample': 0.9077252918981312, 'lambda_1': 0.6518433972257632, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\tvalid-rmse:6.97003\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481303\n",
            "[1000]\tvalid-rmse:0.478493\n",
            "[1500]\tvalid-rmse:0.477661\n",
            "[2000]\tvalid-rmse:0.477338\n",
            "[2500]\tvalid-rmse:0.477442\n",
            "Stopping. Best iteration:\n",
            "[1923]\tvalid-rmse:0.477295\n",
            "\n",
            "best iteration: 1923\n",
            "best score: 0.477295\n",
            "Score for Model 1 fold 1: 1142.9010680131807\n",
            "Model 1 fold 1 finished in 62.28277015686035\n",
            "Model 1 fold 2\n",
            "1 {'max_leaves': 27, 'max_depth': 17, 'min_child_weight': 0, 'gamma': 4.696188321837841, 'colsample_bytree': 0.3558431609964596, 'subsample': 0.9077252918981312, 'lambda_1': 0.6518433972257632, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97463\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481001\n",
            "[1000]\tvalid-rmse:0.478194\n",
            "[1500]\tvalid-rmse:0.477612\n",
            "[2000]\tvalid-rmse:0.477429\n",
            "[2500]\tvalid-rmse:0.477501\n",
            "Stopping. Best iteration:\n",
            "[1920]\tvalid-rmse:0.477398\n",
            "\n",
            "best iteration: 1920\n",
            "best score: 0.477398\n",
            "Score for Model 1 fold 2: 1148.743418680896\n",
            "Model 1 fold 2 finished in 62.3311493396759\n",
            "Model 1 fold 3\n",
            "2 {'max_leaves': 27, 'max_depth': 17, 'min_child_weight': 0, 'gamma': 4.696188321837841, 'colsample_bytree': 0.3558431609964596, 'subsample': 0.9077252918981312, 'lambda_1': 0.6518433972257632, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.9699\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.482914\n",
            "[1000]\tvalid-rmse:0.480452\n",
            "[1500]\tvalid-rmse:0.479749\n",
            "[2000]\tvalid-rmse:0.479595\n",
            "[2500]\tvalid-rmse:0.47964\n",
            "[3000]\tvalid-rmse:0.479821\n",
            "Stopping. Best iteration:\n",
            "[2217]\tvalid-rmse:0.479561\n",
            "\n",
            "best iteration: 2217\n",
            "best score: 0.479561\n",
            "Score for Model 1 fold 3: 1142.8595311860597\n",
            "Model 1 fold 3 finished in 67.26535868644714\n",
            "Model 1 fold 4\n",
            "3 {'max_leaves': 27, 'max_depth': 17, 'min_child_weight': 0, 'gamma': 4.696188321837841, 'colsample_bytree': 0.3558431609964596, 'subsample': 0.9077252918981312, 'lambda_1': 0.6518433972257632, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97335\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481202\n",
            "[1000]\tvalid-rmse:0.478304\n",
            "[1500]\tvalid-rmse:0.4776\n",
            "[2000]\tvalid-rmse:0.477301\n",
            "[2500]\tvalid-rmse:0.477342\n",
            "Stopping. Best iteration:\n",
            "[1977]\tvalid-rmse:0.477273\n",
            "\n",
            "best iteration: 1977\n",
            "best score: 0.477273\n",
            "Score for Model 1 fold 4: 1142.7794875860154\n",
            "Model 1 fold 4 finished in 63.00209307670593\n",
            "Score for model 1 is 1144.320876366538\n",
            "Stacking model 2 {'max_leaves': 30, 'max_depth': 18, 'min_child_weight': 1, 'gamma': 4.151448278236447, 'colsample_bytree': 0.31365474212352024, 'subsample': 0.948846066577156, 'lambda_1': 0.35034786633150883, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "Model 2 fold 1\n",
            "0 {'max_leaves': 30, 'max_depth': 18, 'min_child_weight': 1, 'gamma': 4.151448278236447, 'colsample_bytree': 0.31365474212352024, 'subsample': 0.948846066577156, 'lambda_1': 0.35034786633150883, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97003\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481303\n",
            "[1000]\tvalid-rmse:0.478493\n",
            "[1500]\tvalid-rmse:0.477661\n",
            "[2000]\tvalid-rmse:0.477338\n",
            "[2500]\tvalid-rmse:0.477442\n",
            "Stopping. Best iteration:\n",
            "[1923]\tvalid-rmse:0.477295\n",
            "\n",
            "best iteration: 1923\n",
            "best score: 0.477295\n",
            "Score for Model 2 fold 1: 1142.9010680131807\n",
            "Model 2 fold 1 finished in 62.403220891952515\n",
            "Model 2 fold 2\n",
            "1 {'max_leaves': 30, 'max_depth': 18, 'min_child_weight': 1, 'gamma': 4.151448278236447, 'colsample_bytree': 0.31365474212352024, 'subsample': 0.948846066577156, 'lambda_1': 0.35034786633150883, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97463\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481001\n",
            "[1000]\tvalid-rmse:0.478194\n",
            "[1500]\tvalid-rmse:0.477612\n",
            "[2000]\tvalid-rmse:0.477429\n",
            "[2500]\tvalid-rmse:0.477501\n",
            "Stopping. Best iteration:\n",
            "[1920]\tvalid-rmse:0.477398\n",
            "\n",
            "best iteration: 1920\n",
            "best score: 0.477398\n",
            "Score for Model 2 fold 2: 1148.743418680896\n",
            "Model 2 fold 2 finished in 62.35756874084473\n",
            "Model 2 fold 3\n",
            "2 {'max_leaves': 30, 'max_depth': 18, 'min_child_weight': 1, 'gamma': 4.151448278236447, 'colsample_bytree': 0.31365474212352024, 'subsample': 0.948846066577156, 'lambda_1': 0.35034786633150883, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.9699\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.482914\n",
            "[1000]\tvalid-rmse:0.480452\n",
            "[1500]\tvalid-rmse:0.479749\n",
            "[2000]\tvalid-rmse:0.479595\n",
            "[2500]\tvalid-rmse:0.47964\n",
            "[3000]\tvalid-rmse:0.479821\n",
            "Stopping. Best iteration:\n",
            "[2217]\tvalid-rmse:0.479561\n",
            "\n",
            "best iteration: 2217\n",
            "best score: 0.479561\n",
            "Score for Model 2 fold 3: 1142.8595311860597\n",
            "Model 2 fold 3 finished in 67.23400092124939\n",
            "Model 2 fold 4\n",
            "3 {'max_leaves': 30, 'max_depth': 18, 'min_child_weight': 1, 'gamma': 4.151448278236447, 'colsample_bytree': 0.31365474212352024, 'subsample': 0.948846066577156, 'lambda_1': 0.35034786633150883, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97335\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481202\n",
            "[1000]\tvalid-rmse:0.478304\n",
            "[1500]\tvalid-rmse:0.4776\n",
            "[2000]\tvalid-rmse:0.477301\n",
            "[2500]\tvalid-rmse:0.477342\n",
            "Stopping. Best iteration:\n",
            "[1977]\tvalid-rmse:0.477273\n",
            "\n",
            "best iteration: 1977\n",
            "best score: 0.477273\n",
            "Score for Model 2 fold 4: 1142.7794875860154\n",
            "Model 2 fold 4 finished in 63.08368968963623\n",
            "Score for model 2 is 1144.320876366538\n",
            "Stacking model 3 {'max_leaves': 27, 'max_depth': 24, 'min_child_weight': 0, 'gamma': 1.32858144342245, 'colsample_bytree': 0.5262847718961265, 'subsample': 0.9429597651457532, 'lambda_1': 0.13037813495797712, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "Model 3 fold 1\n",
            "0 {'max_leaves': 27, 'max_depth': 24, 'min_child_weight': 0, 'gamma': 1.32858144342245, 'colsample_bytree': 0.5262847718961265, 'subsample': 0.9429597651457532, 'lambda_1': 0.13037813495797712, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97003\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481303\n",
            "[1000]\tvalid-rmse:0.478493\n",
            "[1500]\tvalid-rmse:0.477661\n",
            "[2000]\tvalid-rmse:0.477338\n",
            "[2500]\tvalid-rmse:0.477442\n",
            "Stopping. Best iteration:\n",
            "[1923]\tvalid-rmse:0.477295\n",
            "\n",
            "best iteration: 1923\n",
            "best score: 0.477295\n",
            "Score for Model 3 fold 1: 1142.9010680131807\n",
            "Model 3 fold 1 finished in 62.36728096008301\n",
            "Model 3 fold 2\n",
            "1 {'max_leaves': 27, 'max_depth': 24, 'min_child_weight': 0, 'gamma': 1.32858144342245, 'colsample_bytree': 0.5262847718961265, 'subsample': 0.9429597651457532, 'lambda_1': 0.13037813495797712, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97463\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481001\n",
            "[1000]\tvalid-rmse:0.478194\n",
            "[1500]\tvalid-rmse:0.477612\n",
            "[2000]\tvalid-rmse:0.477429\n",
            "[2500]\tvalid-rmse:0.477501\n",
            "Stopping. Best iteration:\n",
            "[1920]\tvalid-rmse:0.477398\n",
            "\n",
            "best iteration: 1920\n",
            "best score: 0.477398\n",
            "Score for Model 3 fold 2: 1148.743418680896\n",
            "Model 3 fold 2 finished in 62.380226373672485\n",
            "Model 3 fold 3\n",
            "2 {'max_leaves': 27, 'max_depth': 24, 'min_child_weight': 0, 'gamma': 1.32858144342245, 'colsample_bytree': 0.5262847718961265, 'subsample': 0.9429597651457532, 'lambda_1': 0.13037813495797712, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.9699\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.482914\n",
            "[1000]\tvalid-rmse:0.480452\n",
            "[1500]\tvalid-rmse:0.479749\n",
            "[2000]\tvalid-rmse:0.479595\n",
            "[2500]\tvalid-rmse:0.47964\n",
            "[3000]\tvalid-rmse:0.479821\n",
            "Stopping. Best iteration:\n",
            "[2217]\tvalid-rmse:0.479561\n",
            "\n",
            "best iteration: 2217\n",
            "best score: 0.479561\n",
            "Score for Model 3 fold 3: 1142.8595311860597\n",
            "Model 3 fold 3 finished in 67.13196659088135\n",
            "Model 3 fold 4\n",
            "3 {'max_leaves': 27, 'max_depth': 24, 'min_child_weight': 0, 'gamma': 1.32858144342245, 'colsample_bytree': 0.5262847718961265, 'subsample': 0.9429597651457532, 'lambda_1': 0.13037813495797712, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97335\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481202\n",
            "[1000]\tvalid-rmse:0.478304\n",
            "[1500]\tvalid-rmse:0.4776\n",
            "[2000]\tvalid-rmse:0.477301\n",
            "[2500]\tvalid-rmse:0.477342\n",
            "Stopping. Best iteration:\n",
            "[1977]\tvalid-rmse:0.477273\n",
            "\n",
            "best iteration: 1977\n",
            "best score: 0.477273\n",
            "Score for Model 3 fold 4: 1142.7794875860154\n",
            "Model 3 fold 4 finished in 62.95444297790527\n",
            "Score for model 3 is 1144.320876366538\n",
            "Stacking model 4 {'max_leaves': 27, 'max_depth': 14, 'min_child_weight': 1, 'gamma': 0.021858932008684828, 'colsample_bytree': 0.4447821485094775, 'subsample': 0.929323530126744, 'lambda_1': 0.3617239869110247, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "Model 4 fold 1\n",
            "0 {'max_leaves': 27, 'max_depth': 14, 'min_child_weight': 1, 'gamma': 0.021858932008684828, 'colsample_bytree': 0.4447821485094775, 'subsample': 0.929323530126744, 'lambda_1': 0.3617239869110247, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97003\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481303\n",
            "[1000]\tvalid-rmse:0.478493\n",
            "[1500]\tvalid-rmse:0.477661\n",
            "[2000]\tvalid-rmse:0.477338\n",
            "[2500]\tvalid-rmse:0.477442\n",
            "Stopping. Best iteration:\n",
            "[1923]\tvalid-rmse:0.477295\n",
            "\n",
            "best iteration: 1923\n",
            "best score: 0.477295\n",
            "Score for Model 4 fold 1: 1142.9010680131807\n",
            "Model 4 fold 1 finished in 62.182945251464844\n",
            "Model 4 fold 2\n",
            "1 {'max_leaves': 27, 'max_depth': 14, 'min_child_weight': 1, 'gamma': 0.021858932008684828, 'colsample_bytree': 0.4447821485094775, 'subsample': 0.929323530126744, 'lambda_1': 0.3617239869110247, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97463\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481001\n",
            "[1000]\tvalid-rmse:0.478194\n",
            "[1500]\tvalid-rmse:0.477612\n",
            "[2000]\tvalid-rmse:0.477429\n",
            "[2500]\tvalid-rmse:0.477501\n",
            "Stopping. Best iteration:\n",
            "[1920]\tvalid-rmse:0.477398\n",
            "\n",
            "best iteration: 1920\n",
            "best score: 0.477398\n",
            "Score for Model 4 fold 2: 1148.743418680896\n",
            "Model 4 fold 2 finished in 62.221980810165405\n",
            "Model 4 fold 3\n",
            "2 {'max_leaves': 27, 'max_depth': 14, 'min_child_weight': 1, 'gamma': 0.021858932008684828, 'colsample_bytree': 0.4447821485094775, 'subsample': 0.929323530126744, 'lambda_1': 0.3617239869110247, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.9699\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.482914\n",
            "[1000]\tvalid-rmse:0.480452\n",
            "[1500]\tvalid-rmse:0.479749\n",
            "[2000]\tvalid-rmse:0.479595\n",
            "[2500]\tvalid-rmse:0.47964\n",
            "[3000]\tvalid-rmse:0.479821\n",
            "Stopping. Best iteration:\n",
            "[2217]\tvalid-rmse:0.479561\n",
            "\n",
            "best iteration: 2217\n",
            "best score: 0.479561\n",
            "Score for Model 4 fold 3: 1142.8595311860597\n",
            "Model 4 fold 3 finished in 67.07457423210144\n",
            "Model 4 fold 4\n",
            "3 {'max_leaves': 27, 'max_depth': 14, 'min_child_weight': 1, 'gamma': 0.021858932008684828, 'colsample_bytree': 0.4447821485094775, 'subsample': 0.929323530126744, 'lambda_1': 0.3617239869110247, 'alpha': 0, 'objective': 'reg:squarederror', 'learning_rate': 0.05, 'num_boost_round': 11611, 'seed': 1234, 'metric': 'rmse', 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
            "[0]\tvalid-rmse:6.97335\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481202\n",
            "[1000]\tvalid-rmse:0.478304\n",
            "[1500]\tvalid-rmse:0.4776\n",
            "[2000]\tvalid-rmse:0.477301\n",
            "[2500]\tvalid-rmse:0.477342\n",
            "Stopping. Best iteration:\n",
            "[1977]\tvalid-rmse:0.477273\n",
            "\n",
            "best iteration: 1977\n",
            "best score: 0.477273\n",
            "Score for Model 4 fold 4: 1142.7794875860154\n",
            "Model 4 fold 4 finished in 62.90056848526001\n",
            "Score for model 4 is 1144.320876366538\n",
            "Stacking model 5 {'learning_rate': 0.05, 'metrics': 'rmse', 'subsample': 0.8, 'seed': 1234, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'max_leaves': 127, 'max_depth': 7, 'gamma': 0, 'colsample_bytree': 0.1, 'min_child_weight': 100, 'lambda': 100, 'alpha': 10}\n",
            "Model 5 fold 1\n",
            "0 {'learning_rate': 0.05, 'metrics': 'rmse', 'subsample': 0.8, 'seed': 1234, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'max_leaves': 127, 'max_depth': 7, 'gamma': 0, 'colsample_bytree': 0.1, 'min_child_weight': 100, 'lambda': 100, 'alpha': 10}\n",
            "[0]\tvalid-rmse:6.97003\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481303\n",
            "[1000]\tvalid-rmse:0.478493\n",
            "[1500]\tvalid-rmse:0.477661\n",
            "[2000]\tvalid-rmse:0.477338\n",
            "[2500]\tvalid-rmse:0.477442\n",
            "Stopping. Best iteration:\n",
            "[1923]\tvalid-rmse:0.477295\n",
            "\n",
            "best iteration: 1923\n",
            "best score: 0.477295\n",
            "Score for Model 5 fold 1: 1142.9010680131807\n",
            "Model 5 fold 1 finished in 62.143983125686646\n",
            "Model 5 fold 2\n",
            "1 {'learning_rate': 0.05, 'metrics': 'rmse', 'subsample': 0.8, 'seed': 1234, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'max_leaves': 127, 'max_depth': 7, 'gamma': 0, 'colsample_bytree': 0.1, 'min_child_weight': 100, 'lambda': 100, 'alpha': 10}\n",
            "[0]\tvalid-rmse:6.97463\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481001\n",
            "[1000]\tvalid-rmse:0.478194\n",
            "[1500]\tvalid-rmse:0.477612\n",
            "[2000]\tvalid-rmse:0.477429\n",
            "[2500]\tvalid-rmse:0.477501\n",
            "Stopping. Best iteration:\n",
            "[1920]\tvalid-rmse:0.477398\n",
            "\n",
            "best iteration: 1920\n",
            "best score: 0.477398\n",
            "Score for Model 5 fold 2: 1148.743418680896\n",
            "Model 5 fold 2 finished in 62.09753394126892\n",
            "Model 5 fold 3\n",
            "2 {'learning_rate': 0.05, 'metrics': 'rmse', 'subsample': 0.8, 'seed': 1234, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'max_leaves': 127, 'max_depth': 7, 'gamma': 0, 'colsample_bytree': 0.1, 'min_child_weight': 100, 'lambda': 100, 'alpha': 10}\n",
            "[0]\tvalid-rmse:6.9699\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.482914\n",
            "[1000]\tvalid-rmse:0.480452\n",
            "[1500]\tvalid-rmse:0.479749\n",
            "[2000]\tvalid-rmse:0.479595\n",
            "[2500]\tvalid-rmse:0.47964\n",
            "[3000]\tvalid-rmse:0.479821\n",
            "Stopping. Best iteration:\n",
            "[2217]\tvalid-rmse:0.479561\n",
            "\n",
            "best iteration: 2217\n",
            "best score: 0.479561\n",
            "Score for Model 5 fold 3: 1142.8595311860597\n",
            "Model 5 fold 3 finished in 66.94543075561523\n",
            "Model 5 fold 4\n",
            "3 {'learning_rate': 0.05, 'metrics': 'rmse', 'subsample': 0.8, 'seed': 1234, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'max_leaves': 127, 'max_depth': 7, 'gamma': 0, 'colsample_bytree': 0.1, 'min_child_weight': 100, 'lambda': 100, 'alpha': 10}\n",
            "[0]\tvalid-rmse:6.97335\n",
            "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
            "[500]\tvalid-rmse:0.481202\n",
            "[1000]\tvalid-rmse:0.478304\n",
            "[1500]\tvalid-rmse:0.4776\n",
            "[2000]\tvalid-rmse:0.477301\n",
            "[2500]\tvalid-rmse:0.477342\n",
            "Stopping. Best iteration:\n",
            "[1977]\tvalid-rmse:0.477273\n",
            "\n",
            "best iteration: 1977\n",
            "best score: 0.477273\n",
            "Score for Model 5 fold 4: 1142.7794875860154\n",
            "Model 5 fold 4 finished in 62.88046336174011\n",
            "Score for model 5 is 1144.320876366538\n",
            "Socre for blended models is 1144.3208763665382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G2P2QPtzqZ3",
        "colab_type": "code",
        "outputId": "d0f3a972-43ba-4fa3-e532-574ff96d68c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (np.mean(blend_scores_xgb_ohe,axis=0))\n",
        "np.savetxt(\"/content/drive/My Drive/Kaggle_Allstate/data/train_blend_x_xgb_ohe_gpu.csv\",train_blend_x_xgb_ohe, delimiter=\",\")\n",
        "np.savetxt(\"/content/drive/My Drive/Kaggle_Allstate/data/test_blend_x_xgb_ohe_gpu.csv\",test_blend_x_xgb_ohe, delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1144.32087637 1144.32087637 1144.32087637 1144.32087637 1144.32087637]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D2e0as20JMU",
        "colab_type": "code",
        "outputId": "b36c9b99-ab7d-4dc3-a1d8-1ce214d1d64d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "blend_scores_xgb_ohe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1142.90106801, 1142.90106801, 1142.90106801, 1142.90106801,\n",
              "        1142.90106801],\n",
              "       [1148.74341868, 1148.74341868, 1148.74341868, 1148.74341868,\n",
              "        1148.74341868],\n",
              "       [1142.85953119, 1142.85953119, 1142.85953119, 1142.85953119,\n",
              "        1142.85953119],\n",
              "       [1142.77948759, 1142.77948759, 1142.77948759, 1142.77948759,\n",
              "        1142.77948759]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLuv56qBBWpA",
        "colab_type": "text"
      },
      "source": [
        "## LightGBM tuning \n",
        "\n",
        "### Manual tuning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okB3tg9JzZdK",
        "colab_type": "code",
        "outputId": "af6e7b1d-5ed1-4dc6-ce27-81331b499ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip uninstall lightgbm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling lightgbm-2.3.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "    /usr/local/lib/python3.6/dist-packages/lightgbm-2.3.2-py3.6.egg-info\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled lightgbm-2.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcTPF2Jt3ngr",
        "colab_type": "code",
        "outputId": "06bc93a8-118e-4cf7-ab77-4b8b4258f48a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrOKxaBz3si0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r LightGBM/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Y5FX1Y2uQ_",
        "colab_type": "code",
        "outputId": "acd1df5b-c1f6-46fa-b098-06c1955697dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!git clone --recursive https://github.com/Microsoft/LightGBM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'LightGBM'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 17509 (delta 0), reused 0 (delta 0), pack-reused 17505\u001b[K\n",
            "Receiving objects: 100% (17509/17509), 11.91 MiB | 17.40 MiB/s, done.\n",
            "Resolving deltas: 100% (12771/12771), done.\n",
            "Submodule 'include/boost/compute' (https://github.com/boostorg/compute) registered for path 'compute'\n",
            "Cloning into '/content/LightGBM/compute'...\n",
            "remote: Enumerating objects: 21728, done.        \n",
            "remote: Total 21728 (delta 0), reused 0 (delta 0), pack-reused 21728        \n",
            "Receiving objects: 100% (21728/21728), 8.51 MiB | 15.53 MiB/s, done.\n",
            "Resolving deltas: 100% (17565/17565), done.\n",
            "Submodule path 'compute': checked out '36c89134d4013b2e5e45bc55656a18bd6141995a'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TiCTF_QCFGI",
        "colab_type": "code",
        "outputId": "de26f4dd-9608-42de-91a0-6c482778db29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- Looking for CL_VERSION_2_2\n",
            "-- Looking for CL_VERSION_2_2 - found\n",
            "-- Found OpenCL: /usr/lib/x86_64-linux-gnu/libOpenCL.so (found version \"2.2\") \n",
            "-- OpenCL include directory: /usr/include\n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   filesystem\n",
            "--   system\n",
            "-- Performing Test MM_PREFETCH\n",
            "-- Performing Test MM_PREFETCH - Success\n",
            "-- Using _mm_prefetch\n",
            "-- Performing Test MM_MALLOC\n",
            "-- Performing Test MM_MALLOC - Success\n",
            "-- Using _mm_malloc\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/LightGBM/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target _lightgbm\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lightgbm\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/main.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/application/application.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/application/application.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/boosting.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/boosting.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/bin.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/config.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/config_auto.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/bin.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/dataset.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/dataset_loader.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/file_io.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/json11.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/metadata.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/parser.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/tree.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/metric/metric.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linker_topo.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linkers_socket.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/config.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/network.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/objective/objective_function.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/config_auto.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/dataset.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/dataset_loader.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/file_io.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/json11.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/metadata.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/parser.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/tree.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/metric/metric.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linker_topo.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linkers_socket.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/network.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/objective/objective_function.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../lightgbm\u001b[0m\n",
            "[ 90%] Built target lightgbm\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/c_api.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX shared library ../lib_lightgbm.so\u001b[0m\n",
            "[100%] Built target _lightgbm\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "INFO:root:Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
            "INFO:root:Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/lightgbm\n",
            "copying lightgbm/__init__.py -> build/lib/lightgbm\n",
            "copying lightgbm/callback.py -> build/lib/lightgbm\n",
            "copying lightgbm/sklearn.py -> build/lib/lightgbm\n",
            "copying lightgbm/engine.py -> build/lib/lightgbm\n",
            "copying lightgbm/compat.py -> build/lib/lightgbm\n",
            "copying lightgbm/libpath.py -> build/lib/lightgbm\n",
            "copying lightgbm/plotting.py -> build/lib/lightgbm\n",
            "copying lightgbm/basic.py -> build/lib/lightgbm\n",
            "running egg_info\n",
            "creating lightgbm.egg-info\n",
            "writing lightgbm.egg-info/PKG-INFO\n",
            "writing dependency_links to lightgbm.egg-info/dependency_links.txt\n",
            "writing requirements to lightgbm.egg-info/requires.txt\n",
            "writing top-level names to lightgbm.egg-info/top_level.txt\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "no previously-included directories found matching 'build'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching '*.txt'\n",
            "warning: no files found matching '*.so' under directory 'lightgbm'\n",
            "warning: no files found matching '*.txt' under directory 'compile'\n",
            "warning: no files found matching '*.so' under directory 'compile'\n",
            "warning: no files found matching '*.dll' under directory 'compile/Release'\n",
            "warning: no files found matching '*' under directory 'compile/compute'\n",
            "warning: no files found matching '*' under directory 'compile/include'\n",
            "warning: no files found matching '*' under directory 'compile/src'\n",
            "warning: no files found matching 'LightGBM.sln' under directory 'compile/windows'\n",
            "warning: no files found matching 'LightGBM.vcxproj' under directory 'compile/windows'\n",
            "warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n",
            "warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "copying lightgbm/VERSION.txt -> build/lib/lightgbm\n",
            "running install_lib\n",
            "creating /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/__init__.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/callback.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/sklearn.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/engine.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/compat.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/libpath.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/VERSION.txt -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/plotting.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/basic.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "INFO:LightGBM:Installing lib_lightgbm from: ['../lib_lightgbm.so']\n",
            "copying ../lib_lightgbm.so -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/callback.py to callback.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/sklearn.py to sklearn.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/engine.py to engine.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/compat.py to compat.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/libpath.py to libpath.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/plotting.py to plotting.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/basic.py to basic.cpython-36.pyc\n",
            "running install_egg_info\n",
            "Copying lightgbm.egg-info to /usr/local/lib/python3.6/dist-packages/lightgbm-2.3.2-py3.6.egg-info\n",
            "running install_scripts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lTy-CmACn_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmA12VeqBf9D",
        "colab_type": "code",
        "outputId": "0e341ba8-b7a7-46de-9302-f80f8e1f34d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import copy\n",
        "default_lgb_params = {}\n",
        "default_lgb_params[\"learning_rate\"] = 0.05\n",
        "default_lgb_params[\"metric\"] = 'rmse'\n",
        "default_lgb_params[\"bagging_freq\"] = 1\n",
        "default_lgb_params[\"seed\"] = 1234\n",
        "default_lgb_params[\"objective\"] = \"regression\"\n",
        "default_lgb_params['device'] = 'gpu'\n",
        "# default_lgb_params['gpu_platform_id'] = 0\n",
        "# default_lgb_params['gpu_device_id'] = 0\n",
        "\n",
        "params_lgb_space = {}\n",
        "params_lgb_space['num_leaves'] = [3, 7, 15, 31, 63, 127, 255]\n",
        "params_lgb_space['max_depth'] = [3 ,4 ,5 ,6 ,7 ,8, -1]\n",
        "params_lgb_space['min_gain_to_split'] = [0, 0.1, 0.3, 1, 1.5, 2, 3]\n",
        "params_lgb_space['feature_fraction'] = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "params_lgb_space['bagging_fraction'] = [0.2, 0.4, 0.6, 0.8, 1]\n",
        "params_lgb_space['min_sum_hessian_in_leaf'] = [1, 5, 10, 30, 100]\n",
        "params_lgb_space['lambda_l2'] = [0, 0.01, 0.1, 1, 10, 100]\n",
        "params_lgb_space['lambda_l1'] = [0, 0.01, 0.1, 1, 10, 100]\n",
        "# params_lgb_space['objective'] = ['regression', 'regression_l1', 'poisson']\n",
        "\n",
        "\n",
        "\n",
        "greater_is_better = False\n",
        "\n",
        "best_lgb_params = copy.copy(default_lgb_params)\n",
        "\n",
        "for p in params_lgb_space:\n",
        "    print (\"\\n Tuning parameter %s in %s\" % (p, params_lgb_space[p]))\n",
        "\n",
        "    params = best_lgb_params\n",
        "    scores = []    \n",
        "    for v in params_lgb_space[p]:\n",
        "        print ('\\n    %s: %s' % (p, v), end=\"\\n\")\n",
        "        params[p] = v\n",
        "        lgb_cv = lgb.cv(params,\n",
        "                lgb.Dataset(train_x,\n",
        "                            label=train_y\n",
        "                            ),\n",
        "                num_boost_round=100000,\n",
        "                nfold=kfolds,\n",
        "                folds=skf.split(train_x,train_y),\n",
        "                stratified=False,\n",
        "                early_stopping_rounds=50,\n",
        "                verbose_eval=500)\n",
        "\n",
        "        best_lgb_score = min(lgb_cv['rmse-mean'])\n",
        "        best_lgb_iteration = len(lgb_cv['rmse-mean'])\n",
        "        print (', best_score: %f, best_iteration: %d' % (best_lgb_score, best_lgb_iteration))\n",
        "        scores.append([v, best_lgb_score])\n",
        "    # best param value in the space\n",
        "    best_param_value = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][0]\n",
        "    best_param_score = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][1]\n",
        "    best_lgb_params[p] = best_param_value\n",
        "    print (\"Best %s is %s with a score of %f\" %(p, best_param_value, best_param_score))\n",
        "\n",
        "print ('\\n Best manually tuned parameters:', best_lgb_params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Tuning parameter num_leaves in [3, 7, 15, 31, 63, 127, 255]\n",
            "\n",
            "    num_leaves: 3\n",
            "[500]\tcv_agg's rmse: 0.505839 + 0.00191511\n",
            "[1000]\tcv_agg's rmse: 0.496659 + 0.00204289\n",
            "[1500]\tcv_agg's rmse: 0.492759 + 0.00203624\n",
            "[2000]\tcv_agg's rmse: 0.49048 + 0.00207569\n",
            "[2500]\tcv_agg's rmse: 0.488903 + 0.00210408\n",
            "[3000]\tcv_agg's rmse: 0.487686 + 0.00204859\n",
            "[3500]\tcv_agg's rmse: 0.486779 + 0.0020033\n",
            "[4000]\tcv_agg's rmse: 0.486048 + 0.00202421\n",
            "[4500]\tcv_agg's rmse: 0.485457 + 0.00204797\n",
            "[5000]\tcv_agg's rmse: 0.484939 + 0.00208115\n",
            "[5500]\tcv_agg's rmse: 0.484511 + 0.00208058\n",
            "[6000]\tcv_agg's rmse: 0.484101 + 0.00211364\n",
            "[6500]\tcv_agg's rmse: 0.483767 + 0.00212561\n",
            "[7000]\tcv_agg's rmse: 0.48336 + 0.00205232\n",
            "[7500]\tcv_agg's rmse: 0.483085 + 0.00205272\n",
            "[8000]\tcv_agg's rmse: 0.482864 + 0.00206986\n",
            "[8500]\tcv_agg's rmse: 0.482666 + 0.00206528\n",
            "[9000]\tcv_agg's rmse: 0.482503 + 0.00206903\n",
            "[9500]\tcv_agg's rmse: 0.482375 + 0.00206197\n",
            "[10000]\tcv_agg's rmse: 0.482248 + 0.00203426\n",
            "[10500]\tcv_agg's rmse: 0.482114 + 0.0020366\n",
            "[11000]\tcv_agg's rmse: 0.482003 + 0.00202945\n",
            "[11500]\tcv_agg's rmse: 0.481882 + 0.00204145\n",
            "[12000]\tcv_agg's rmse: 0.481758 + 0.00202922\n",
            ", best_score: 0.481749, best_iteration: 12059\n",
            "\n",
            "    num_leaves: 7\n",
            "[500]\tcv_agg's rmse: 0.487416 + 0.00195201\n",
            "[1000]\tcv_agg's rmse: 0.483694 + 0.00206926\n",
            "[1500]\tcv_agg's rmse: 0.48202 + 0.00219419\n",
            "[2000]\tcv_agg's rmse: 0.480939 + 0.0022213\n",
            "[2500]\tcv_agg's rmse: 0.480263 + 0.00217464\n",
            "[3000]\tcv_agg's rmse: 0.479837 + 0.00222989\n",
            "[3500]\tcv_agg's rmse: 0.479539 + 0.00226824\n",
            "[4000]\tcv_agg's rmse: 0.479345 + 0.00229994\n",
            "[4500]\tcv_agg's rmse: 0.479178 + 0.00228673\n",
            "[5000]\tcv_agg's rmse: 0.479055 + 0.00228228\n",
            ", best_score: 0.478959, best_iteration: 5300\n",
            "\n",
            "    num_leaves: 15\n",
            "[500]\tcv_agg's rmse: 0.481275 + 0.00202354\n",
            "[1000]\tcv_agg's rmse: 0.479555 + 0.00217745\n",
            "[1500]\tcv_agg's rmse: 0.478774 + 0.00218379\n",
            "[2000]\tcv_agg's rmse: 0.47847 + 0.00221184\n",
            ", best_score: 0.478451, best_iteration: 2046\n",
            "\n",
            "    num_leaves: 31\n",
            "[500]\tcv_agg's rmse: 0.478615 + 0.00202681\n",
            "[1000]\tcv_agg's rmse: 0.477961 + 0.00206248\n",
            ", best_score: 0.477951, best_iteration: 1045\n",
            "\n",
            "    num_leaves: 63\n",
            "[500]\tcv_agg's rmse: 0.477414 + 0.00194189\n",
            ", best_score: 0.477411, best_iteration: 501\n",
            "\n",
            "    num_leaves: 127\n",
            ", best_score: 0.477340, best_iteration: 313\n",
            "\n",
            "    num_leaves: 255\n",
            ", best_score: 0.477703, best_iteration: 190\n",
            "Best num_leaves is 127 with a score of 0.477340\n",
            "\n",
            " Tuning parameter max_depth in [3, 4, 5, 6, 7, 8, -1]\n",
            "\n",
            "    max_depth: 3\n",
            "[500]\tcv_agg's rmse: 0.490114 + 0.00181874\n",
            "[1000]\tcv_agg's rmse: 0.484829 + 0.0019378\n",
            "[1500]\tcv_agg's rmse: 0.482852 + 0.00202065\n",
            "[2000]\tcv_agg's rmse: 0.48181 + 0.00212197\n",
            "[2500]\tcv_agg's rmse: 0.481177 + 0.00210356\n",
            "[3000]\tcv_agg's rmse: 0.480762 + 0.00204693\n",
            "[3500]\tcv_agg's rmse: 0.480522 + 0.00207407\n",
            "[4000]\tcv_agg's rmse: 0.480322 + 0.00210139\n",
            ", best_score: 0.480205, best_iteration: 4434\n",
            "\n",
            "    max_depth: 4\n",
            "[500]\tcv_agg's rmse: 0.484195 + 0.00203187\n",
            "[1000]\tcv_agg's rmse: 0.480843 + 0.00221635\n",
            "[1500]\tcv_agg's rmse: 0.479704 + 0.00225192\n",
            "[2000]\tcv_agg's rmse: 0.479164 + 0.00228406\n",
            ", best_score: 0.478912, best_iteration: 2423\n",
            "\n",
            "    max_depth: 5\n",
            "[500]\tcv_agg's rmse: 0.48102 + 0.00217754\n",
            "[1000]\tcv_agg's rmse: 0.478855 + 0.00235586\n",
            "[1500]\tcv_agg's rmse: 0.478243 + 0.00242909\n",
            ", best_score: 0.478152, best_iteration: 1824\n",
            "\n",
            "    max_depth: 6\n",
            "[500]\tcv_agg's rmse: 0.479439 + 0.0021331\n",
            "[1000]\tcv_agg's rmse: 0.478188 + 0.00230782\n",
            ", best_score: 0.478028, best_iteration: 1241\n",
            "\n",
            "    max_depth: 7\n",
            "[500]\tcv_agg's rmse: 0.478645 + 0.00219573\n",
            ", best_score: 0.478126, best_iteration: 854\n",
            "\n",
            "    max_depth: 8\n",
            "[500]\tcv_agg's rmse: 0.478276 + 0.00206533\n",
            ", best_score: 0.478181, best_iteration: 561\n",
            "\n",
            "    max_depth: -1\n",
            ", best_score: 0.477337, best_iteration: 313\n",
            "Best max_depth is -1 with a score of 0.477337\n",
            "\n",
            " Tuning parameter min_gain_to_split in [0, 0.1, 0.3, 1, 1.5, 2, 3]\n",
            "\n",
            "    min_gain_to_split: 0\n",
            ", best_score: 0.477338, best_iteration: 313\n",
            "\n",
            "    min_gain_to_split: 0.1\n",
            ", best_score: 0.477662, best_iteration: 259\n",
            "\n",
            "    min_gain_to_split: 0.3\n",
            ", best_score: 0.477867, best_iteration: 266\n",
            "\n",
            "    min_gain_to_split: 1\n",
            ", best_score: 0.479707, best_iteration: 183\n",
            "\n",
            "    min_gain_to_split: 1.5\n",
            ", best_score: 0.480695, best_iteration: 175\n",
            "\n",
            "    min_gain_to_split: 2\n",
            ", best_score: 0.481536, best_iteration: 173\n",
            "\n",
            "    min_gain_to_split: 3\n",
            ", best_score: 0.483239, best_iteration: 163\n",
            "Best min_gain_to_split is 0 with a score of 0.477338\n",
            "\n",
            " Tuning parameter feature_fraction in [0.1, 0.3, 0.5, 0.7, 0.9]\n",
            "\n",
            "    feature_fraction: 0.1\n",
            "[500]\tcv_agg's rmse: 0.475963 + 0.00192981\n",
            ", best_score: 0.475691, best_iteration: 775\n",
            "\n",
            "    feature_fraction: 0.3\n",
            ", best_score: 0.475560, best_iteration: 371\n",
            "\n",
            "    feature_fraction: 0.5\n",
            ", best_score: 0.476143, best_iteration: 420\n",
            "\n",
            "    feature_fraction: 0.7\n",
            ", best_score: 0.476828, best_iteration: 364\n",
            "\n",
            "    feature_fraction: 0.9\n",
            ", best_score: 0.477097, best_iteration: 393\n",
            "Best feature_fraction is 0.3 with a score of 0.475560\n",
            "\n",
            " Tuning parameter bagging_fraction in [0.2, 0.4, 0.6, 0.8, 1]\n",
            "\n",
            "    bagging_fraction: 0.2\n",
            ", best_score: 0.479016, best_iteration: 276\n",
            "\n",
            "    bagging_fraction: 0.4\n",
            ", best_score: 0.476901, best_iteration: 323\n",
            "\n",
            "    bagging_fraction: 0.6\n",
            ", best_score: 0.475898, best_iteration: 438\n",
            "\n",
            "    bagging_fraction: 0.8\n",
            "[500]\tcv_agg's rmse: 0.475471 + 0.00215\n",
            ", best_score: 0.475432, best_iteration: 456\n",
            "\n",
            "    bagging_fraction: 1\n",
            ", best_score: 0.475559, best_iteration: 371\n",
            "Best bagging_fraction is 0.8 with a score of 0.475432\n",
            "\n",
            " Tuning parameter min_sum_hessian_in_leaf in [1, 5, 10, 30, 100]\n",
            "\n",
            "    min_sum_hessian_in_leaf: 1\n",
            ", best_score: 0.475568, best_iteration: 418\n",
            "\n",
            "    min_sum_hessian_in_leaf: 5\n",
            ", best_score: 0.475563, best_iteration: 388\n",
            "\n",
            "    min_sum_hessian_in_leaf: 10\n",
            ", best_score: 0.475555, best_iteration: 388\n",
            "\n",
            "    min_sum_hessian_in_leaf: 30\n",
            ", best_score: 0.475437, best_iteration: 421\n",
            "\n",
            "    min_sum_hessian_in_leaf: 100\n",
            ", best_score: 0.475166, best_iteration: 421\n",
            "Best min_sum_hessian_in_leaf is 100 with a score of 0.475166\n",
            "\n",
            " Tuning parameter lambda_l2 in [0, 0.01, 0.1, 1, 10, 100]\n",
            "\n",
            "    lambda_l2: 0\n",
            ", best_score: 0.475091, best_iteration: 390\n",
            "\n",
            "    lambda_l2: 0.01\n",
            ", best_score: 0.475137, best_iteration: 389\n",
            "\n",
            "    lambda_l2: 0.1\n",
            ", best_score: 0.474993, best_iteration: 403\n",
            "\n",
            "    lambda_l2: 1\n",
            ", best_score: 0.475146, best_iteration: 399\n",
            "\n",
            "    lambda_l2: 10\n",
            "[500]\tcv_agg's rmse: 0.475181 + 0.00215175\n",
            ", best_score: 0.475131, best_iteration: 465\n",
            "\n",
            "    lambda_l2: 100\n",
            "[500]\tcv_agg's rmse: 0.475291 + 0.00200915\n",
            ", best_score: 0.475288, best_iteration: 498\n",
            "Best lambda_l2 is 0.1 with a score of 0.474993\n",
            "\n",
            " Tuning parameter lambda_l1 in [0, 0.01, 0.1, 1, 10, 100]\n",
            "\n",
            "    lambda_l1: 0\n",
            ", best_score: 0.475093, best_iteration: 417\n",
            "\n",
            "    lambda_l1: 0.01\n",
            ", best_score: 0.475085, best_iteration: 440\n",
            "\n",
            "    lambda_l1: 0.1\n",
            ", best_score: 0.475322, best_iteration: 399\n",
            "\n",
            "    lambda_l1: 1\n",
            ", best_score: 0.475395, best_iteration: 410\n",
            "\n",
            "    lambda_l1: 10\n",
            "[500]\tcv_agg's rmse: 0.476346 + 0.00187813\n",
            ", best_score: 0.476330, best_iteration: 529\n",
            "\n",
            "    lambda_l1: 100\n",
            "[500]\tcv_agg's rmse: 0.484401 + 0.0018007\n",
            "[1000]\tcv_agg's rmse: 0.483122 + 0.00179596\n",
            "[1500]\tcv_agg's rmse: 0.482583 + 0.00178096\n",
            "[2000]\tcv_agg's rmse: 0.482223 + 0.00179022\n",
            "[2500]\tcv_agg's rmse: 0.481978 + 0.00179162\n",
            "[3000]\tcv_agg's rmse: 0.481769 + 0.00178888\n",
            "[3500]\tcv_agg's rmse: 0.481596 + 0.00178745\n",
            "[4000]\tcv_agg's rmse: 0.481457 + 0.00178544\n",
            "[4500]\tcv_agg's rmse: 0.48132 + 0.00177355\n",
            "[5000]\tcv_agg's rmse: 0.481202 + 0.00177285\n",
            "[5500]\tcv_agg's rmse: 0.481097 + 0.00177555\n",
            "[6000]\tcv_agg's rmse: 0.480995 + 0.00176689\n",
            "[6500]\tcv_agg's rmse: 0.480911 + 0.00176358\n",
            "[7000]\tcv_agg's rmse: 0.480825 + 0.00174863\n",
            "[7500]\tcv_agg's rmse: 0.480753 + 0.00174323\n",
            "[8000]\tcv_agg's rmse: 0.480687 + 0.00173859\n",
            "[8500]\tcv_agg's rmse: 0.480616 + 0.00173127\n",
            "[9000]\tcv_agg's rmse: 0.480557 + 0.00172103\n",
            ", best_score: 0.480540, best_iteration: 9122\n",
            "Best lambda_l1 is 0.01 with a score of 0.475085\n",
            "\n",
            " Best manually tuned parameters: {'learning_rate': 0.05, 'metric': 'rmse', 'bagging_freq': 1, 'seed': 1234, 'objective': 'regression', 'device': 'gpu', 'num_leaves': 127, 'max_depth': -1, 'min_gain_to_split': 0, 'feature_fraction': 0.3, 'bagging_fraction': 0.8, 'min_sum_hessian_in_leaf': 100, 'lambda_l2': 0.1, 'lambda_l1': 0.01}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbFB-d06Ck_A",
        "colab_type": "code",
        "outputId": "42d9adc1-1059-43af-8f47-5f843a204b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "best_param_value = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][0]\n",
        "best_param_score = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][1]\n",
        "best_lgb_params[p] = best_param_value\n",
        "print (\"Best %s is %s with a score of %f\" %(p, best_param_value, best_param_score))\n",
        "\n",
        "print ('\\n Best manually tuned parameters:', best_lgb_params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best lambda_l1 is 0.01 with a score of 0.475085\n",
            "\n",
            " Best manually tuned parameters: {'learning_rate': 0.05, 'metric': 'rmse', 'bagging_freq': 1, 'seed': 1234, 'objective': 'regression', 'device': 'gpu', 'num_leaves': 127, 'max_depth': -1, 'min_gain_to_split': 0, 'feature_fraction': 0.3, 'bagging_fraction': 0.8, 'min_sum_hessian_in_leaf': 100, 'lambda_l2': 0.1, 'lambda_l1': 0.01}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFZezWcNcKD9",
        "colab_type": "text"
      },
      "source": [
        "### Automated tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcNKCiRDcEEK",
        "colab_type": "code",
        "outputId": "adf4338a-62c4-4f08-fc38-34949b984ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def lgb_evaluate(\n",
        "    num_leaves,\n",
        "    max_depth,\n",
        "    min_sum_hessian_in_leaf,\n",
        "    min_gain_to_split,\n",
        "    feature_fraction,\n",
        "    bagging_fraction,\n",
        "    lambda_l2,\n",
        "    lambda_l1\n",
        "):\n",
        "    params = dict()\n",
        "    params['objective'] = 'regression'\n",
        "    params['learning_rate'] = 0.05\n",
        "    params['seed'] = 1234\n",
        "    params['num_leaves'] = int(num_leaves)\n",
        "    params['max_depth'] = int(max_depth)\n",
        "    params['min_sum_hessian_in_leaf'] = int(min_sum_hessian_in_leaf)\n",
        "    params['min_gain_to_split'] = min_gain_to_split\n",
        "    params['feature_fraction'] = feature_fraction\n",
        "    params['bagging_fraction'] = bagging_fraction\n",
        "    params['bagging_freq'] = 1\n",
        "    params['lambda_l2'] = lambda_l2\n",
        "    params['lambda_l1'] = lambda_l1\n",
        "    params[\"metric\"] = 'rmse'\n",
        "\n",
        "    lgb_cv = lgb.cv(params,\n",
        "                    lgb.Dataset(train_x,\n",
        "                                label=train_y\n",
        "                                ),\n",
        "                    num_boost_round=100000,\n",
        "                    nfold=kfolds,\n",
        "                    folds=skf.split(train_x, train_y),\n",
        "                    early_stopping_rounds=50,\n",
        "                    verbose_eval=500)\n",
        "\n",
        "    best_lgb_score = min(lgb_cv['rmse-mean'])\n",
        "    best_lgb_iteration = len(lgb_cv['rmse-mean'])\n",
        "    print(', best_score: %f, best_iteration: %d' %\n",
        "          (best_lgb_score, best_lgb_iteration))\n",
        "\n",
        "    return -best_lgb_score\n",
        "\n",
        "\n",
        "lgb_BO = BayesianOptimization(lgb_evaluate,\n",
        "                              {\n",
        "                                  'num_leaves': (7, 31),\n",
        "                                  'max_depth': (7, 31),\n",
        "                                  'min_sum_hessian_in_leaf': (0, 2),\n",
        "                                  'min_gain_to_split': (0, 5),\n",
        "                                  'feature_fraction': (0.3, 0.6),\n",
        "                                  'bagging_fraction': (0.9, 1),\n",
        "                                  'lambda_l2': (0, 1),\n",
        "                                  'lambda_l1': (0, 1)\n",
        "                              }\n",
        "                              )\n",
        "\n",
        "lgb_BO.maximize(init_points=5, n_iter=40)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | max_depth | min_ga... | min_su... | num_le... |\n",
            "-------------------------------------------------------------------------------------------------------------------------\n",
            "[500]\tcv_agg's rmse: 0.484597 + 0.00193512\n",
            ", best_score: 0.484563, best_iteration: 825\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.4846  \u001b[0m | \u001b[0m 0.9955  \u001b[0m | \u001b[0m 0.4315  \u001b[0m | \u001b[0m 0.3609  \u001b[0m | \u001b[0m 0.9566  \u001b[0m | \u001b[0m 29.35   \u001b[0m | \u001b[0m 2.322   \u001b[0m | \u001b[0m 1.869   \u001b[0m | \u001b[0m 17.27   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.480458 + 0.00218518\n",
            "[1000]\tcv_agg's rmse: 0.480178 + 0.00217923\n",
            "[1500]\tcv_agg's rmse: 0.480105 + 0.00218394\n",
            ", best_score: 0.480080, best_iteration: 1913\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.4801  \u001b[0m | \u001b[95m 0.9203  \u001b[0m | \u001b[95m 0.3006  \u001b[0m | \u001b[95m 0.1219  \u001b[0m | \u001b[95m 0.1827  \u001b[0m | \u001b[95m 12.92   \u001b[0m | \u001b[95m 0.7886  \u001b[0m | \u001b[95m 0.1555  \u001b[0m | \u001b[95m 22.41   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.48386 + 0.00202926\n",
            "[1000]\tcv_agg's rmse: 0.483664 + 0.00202562\n",
            "[1500]\tcv_agg's rmse: 0.483589 + 0.0020208\n",
            "[2000]\tcv_agg's rmse: 0.483549 + 0.00202981\n",
            ", best_score: 0.483529, best_iteration: 2346\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.4835  \u001b[0m | \u001b[0m 0.9152  \u001b[0m | \u001b[0m 0.5135  \u001b[0m | \u001b[0m 0.4124  \u001b[0m | \u001b[0m 0.8489  \u001b[0m | \u001b[0m 26.34   \u001b[0m | \u001b[0m 1.966   \u001b[0m | \u001b[0m 0.603   \u001b[0m | \u001b[0m 15.08   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.48894 + 0.00193191\n",
            "[1000]\tcv_agg's rmse: 0.488804 + 0.00192863\n",
            "[1500]\tcv_agg's rmse: 0.488744 + 0.00192651\n",
            "[2000]\tcv_agg's rmse: 0.48871 + 0.00192405\n",
            ", best_score: 0.488707, best_iteration: 2109\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.4887  \u001b[0m | \u001b[0m 0.9594  \u001b[0m | \u001b[0m 0.502   \u001b[0m | \u001b[0m 0.6302  \u001b[0m | \u001b[0m 0.4859  \u001b[0m | \u001b[0m 20.23   \u001b[0m | \u001b[0m 4.535   \u001b[0m | \u001b[0m 0.06274 \u001b[0m | \u001b[0m 9.67    \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.484042 + 0.00194568\n",
            "[1000]\tcv_agg's rmse: 0.483887 + 0.00196297\n",
            "[1500]\tcv_agg's rmse: 0.483835 + 0.0019691\n",
            "[2000]\tcv_agg's rmse: 0.483806 + 0.00196557\n",
            "[2500]\tcv_agg's rmse: 0.483781 + 0.00195918\n",
            ", best_score: 0.483768, best_iteration: 2815\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.4838  \u001b[0m | \u001b[0m 0.9292  \u001b[0m | \u001b[0m 0.42    \u001b[0m | \u001b[0m 0.1931  \u001b[0m | \u001b[0m 0.8     \u001b[0m | \u001b[0m 13.66   \u001b[0m | \u001b[0m 2.585   \u001b[0m | \u001b[0m 0.989   \u001b[0m | \u001b[0m 21.46   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.483545 + 0.00203664\n",
            "[1000]\tcv_agg's rmse: 0.483485 + 0.00204337\n",
            "[1500]\tcv_agg's rmse: 0.483442 + 0.00203942\n",
            "[2000]\tcv_agg's rmse: 0.483422 + 0.00203948\n",
            "[2500]\tcv_agg's rmse: 0.483406 + 0.00203946\n",
            ", best_score: 0.483398, best_iteration: 2692\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.4834  \u001b[0m | \u001b[0m 0.9751  \u001b[0m | \u001b[0m 0.5641  \u001b[0m | \u001b[0m 0.783   \u001b[0m | \u001b[0m 0.4186  \u001b[0m | \u001b[0m 30.29   \u001b[0m | \u001b[0m 2.66    \u001b[0m | \u001b[0m 0.08255 \u001b[0m | \u001b[0m 30.83   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.479124 + 0.00217031\n",
            "[1000]\tcv_agg's rmse: 0.478032 + 0.00218234\n",
            ", best_score: 0.477901, best_iteration: 1250\n",
            "| \u001b[95m 7       \u001b[0m | \u001b[95m-0.4779  \u001b[0m | \u001b[95m 0.9014  \u001b[0m | \u001b[95m 0.423   \u001b[0m | \u001b[95m 0.978   \u001b[0m | \u001b[95m 0.4664  \u001b[0m | \u001b[95m 7.184   \u001b[0m | \u001b[95m 0.177   \u001b[0m | \u001b[95m 1.992   \u001b[0m | \u001b[95m 29.48   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478745 + 0.00211419\n",
            "[1000]\tcv_agg's rmse: 0.477895 + 0.00220026\n",
            ", best_score: 0.477834, best_iteration: 1128\n",
            "| \u001b[95m 8       \u001b[0m | \u001b[95m-0.4778  \u001b[0m | \u001b[95m 0.9378  \u001b[0m | \u001b[95m 0.3946  \u001b[0m | \u001b[95m 0.2685  \u001b[0m | \u001b[95m 0.9266  \u001b[0m | \u001b[95m 8.584   \u001b[0m | \u001b[95m 0.1235  \u001b[0m | \u001b[95m 0.1961  \u001b[0m | \u001b[95m 30.66   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.477949 + 0.00213823\n",
            "[1000]\tcv_agg's rmse: 0.476833 + 0.00216942\n",
            ", best_score: 0.476556, best_iteration: 1392\n",
            "| \u001b[95m 9       \u001b[0m | \u001b[95m-0.4766  \u001b[0m | \u001b[95m 0.9558  \u001b[0m | \u001b[95m 0.4423  \u001b[0m | \u001b[95m 0.984   \u001b[0m | \u001b[95m 0.5582  \u001b[0m | \u001b[95m 11.32   \u001b[0m | \u001b[95m 0.01056 \u001b[0m | \u001b[95m 1.163   \u001b[0m | \u001b[95m 30.95   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.479115 + 0.00190551\n",
            "[1000]\tcv_agg's rmse: 0.478172 + 0.00196456\n",
            ", best_score: 0.478125, best_iteration: 1109\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.4781  \u001b[0m | \u001b[0m 0.9158  \u001b[0m | \u001b[0m 0.5749  \u001b[0m | \u001b[0m 0.9966  \u001b[0m | \u001b[0m 0.0581  \u001b[0m | \u001b[0m 7.259   \u001b[0m | \u001b[0m 0.1581  \u001b[0m | \u001b[0m 1.748   \u001b[0m | \u001b[0m 29.78   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.479024 + 0.00204219\n",
            "[1000]\tcv_agg's rmse: 0.477993 + 0.00203538\n",
            ", best_score: 0.477913, best_iteration: 1259\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.4779  \u001b[0m | \u001b[0m 0.953   \u001b[0m | \u001b[0m 0.4751  \u001b[0m | \u001b[0m 0.2827  \u001b[0m | \u001b[0m 0.3702  \u001b[0m | \u001b[0m 7.294   \u001b[0m | \u001b[0m 0.09606 \u001b[0m | \u001b[0m 1.907   \u001b[0m | \u001b[0m 30.3    \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478992 + 0.00207214\n",
            "[1000]\tcv_agg's rmse: 0.478215 + 0.00211839\n",
            ", best_score: 0.478159, best_iteration: 1169\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.4782  \u001b[0m | \u001b[0m 0.981   \u001b[0m | \u001b[0m 0.4838  \u001b[0m | \u001b[0m 0.4361  \u001b[0m | \u001b[0m 0.003957\u001b[0m | \u001b[0m 8.523   \u001b[0m | \u001b[0m 0.1235  \u001b[0m | \u001b[0m 0.06986 \u001b[0m | \u001b[0m 29.59   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478882 + 0.00212227\n",
            "[1000]\tcv_agg's rmse: 0.477424 + 0.00206039\n",
            ", best_score: 0.477173, best_iteration: 1433\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.4772  \u001b[0m | \u001b[0m 0.9289  \u001b[0m | \u001b[0m 0.4188  \u001b[0m | \u001b[0m 0.9762  \u001b[0m | \u001b[0m 0.009043\u001b[0m | \u001b[0m 7.356   \u001b[0m | \u001b[0m 0.02279 \u001b[0m | \u001b[0m 0.8173  \u001b[0m | \u001b[0m 29.93   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.477777 + 0.00211517\n",
            "[1000]\tcv_agg's rmse: 0.476679 + 0.00212863\n",
            ", best_score: 0.476492, best_iteration: 1309\n",
            "| \u001b[95m 14      \u001b[0m | \u001b[95m-0.4765  \u001b[0m | \u001b[95m 0.9011  \u001b[0m | \u001b[95m 0.335   \u001b[0m | \u001b[95m 0.8367  \u001b[0m | \u001b[95m 0.06556 \u001b[0m | \u001b[95m 30.38   \u001b[0m | \u001b[95m 0.02201 \u001b[0m | \u001b[95m 1.564   \u001b[0m | \u001b[95m 30.62   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478164 + 0.00212342\n",
            "[1000]\tcv_agg's rmse: 0.477379 + 0.00215672\n",
            ", best_score: 0.477378, best_iteration: 1001\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.4774  \u001b[0m | \u001b[0m 0.9657  \u001b[0m | \u001b[0m 0.3483  \u001b[0m | \u001b[0m 0.1582  \u001b[0m | \u001b[0m 0.5248  \u001b[0m | \u001b[0m 28.24   \u001b[0m | \u001b[0m 0.07597 \u001b[0m | \u001b[0m 1.473   \u001b[0m | \u001b[0m 30.59   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478158 + 0.0020461\n",
            "[1000]\tcv_agg's rmse: 0.47725 + 0.00204356\n",
            ", best_score: 0.477144, best_iteration: 1333\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.4771  \u001b[0m | \u001b[0m 0.9099  \u001b[0m | \u001b[0m 0.3205  \u001b[0m | \u001b[0m 0.791   \u001b[0m | \u001b[0m 0.4706  \u001b[0m | \u001b[0m 22.76   \u001b[0m | \u001b[0m 0.07923 \u001b[0m | \u001b[0m 0.02382 \u001b[0m | \u001b[0m 30.93   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.477992 + 0.00209597\n",
            "[1000]\tcv_agg's rmse: 0.477034 + 0.00216293\n",
            ", best_score: 0.476839, best_iteration: 1338\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.4768  \u001b[0m | \u001b[0m 0.9997  \u001b[0m | \u001b[0m 0.3091  \u001b[0m | \u001b[0m 0.2653  \u001b[0m | \u001b[0m 0.2463  \u001b[0m | \u001b[0m 25.87   \u001b[0m | \u001b[0m 0.01292 \u001b[0m | \u001b[0m 1.754   \u001b[0m | \u001b[0m 30.47   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478135 + 0.00201088\n",
            "[1000]\tcv_agg's rmse: 0.47732 + 0.00198073\n",
            ", best_score: 0.477247, best_iteration: 1130\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.4772  \u001b[0m | \u001b[0m 0.9413  \u001b[0m | \u001b[0m 0.3655  \u001b[0m | \u001b[0m 0.9029  \u001b[0m | \u001b[0m 0.4127  \u001b[0m | \u001b[0m 26.46   \u001b[0m | \u001b[0m 0.07897 \u001b[0m | \u001b[0m 1.945   \u001b[0m | \u001b[0m 30.83   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478275 + 0.00207507\n",
            "[1000]\tcv_agg's rmse: 0.477481 + 0.00210518\n",
            ", best_score: 0.477383, best_iteration: 1248\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.4774  \u001b[0m | \u001b[0m 0.9474  \u001b[0m | \u001b[0m 0.3771  \u001b[0m | \u001b[0m 0.9725  \u001b[0m | \u001b[0m 0.9068  \u001b[0m | \u001b[0m 25.16   \u001b[0m | \u001b[0m 0.1382  \u001b[0m | \u001b[0m 0.02873 \u001b[0m | \u001b[0m 30.35   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.477782 + 0.00209996\n",
            "[1000]\tcv_agg's rmse: 0.476719 + 0.00212319\n",
            "[1500]\tcv_agg's rmse: 0.476433 + 0.00207236\n",
            ", best_score: 0.476424, best_iteration: 1455\n",
            "| \u001b[95m 20      \u001b[0m | \u001b[95m-0.4764  \u001b[0m | \u001b[95m 0.9049  \u001b[0m | \u001b[95m 0.4435  \u001b[0m | \u001b[95m 0.9726  \u001b[0m | \u001b[95m 0.4346  \u001b[0m | \u001b[95m 18.61   \u001b[0m | \u001b[95m 0.007592\u001b[0m | \u001b[95m 0.147   \u001b[0m | \u001b[95m 30.66   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.477718 + 0.00201107\n",
            "[1000]\tcv_agg's rmse: 0.47663 + 0.00207883\n",
            ", best_score: 0.476282, best_iteration: 1449\n",
            "| \u001b[95m 21      \u001b[0m | \u001b[95m-0.4763  \u001b[0m | \u001b[95m 0.9806  \u001b[0m | \u001b[95m 0.3033  \u001b[0m | \u001b[95m 0.5503  \u001b[0m | \u001b[95m 0.02993 \u001b[0m | \u001b[95m 27.09   \u001b[0m | \u001b[95m 0.002784\u001b[0m | \u001b[95m 0.1459  \u001b[0m | \u001b[95m 30.81   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.477804 + 0.0020627\n",
            "[1000]\tcv_agg's rmse: 0.476744 + 0.00200509\n",
            ", best_score: 0.476508, best_iteration: 1433\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.4765  \u001b[0m | \u001b[0m 0.9283  \u001b[0m | \u001b[0m 0.5329  \u001b[0m | \u001b[0m 0.9023  \u001b[0m | \u001b[0m 0.1745  \u001b[0m | \u001b[0m 28.51   \u001b[0m | \u001b[0m 0.00401 \u001b[0m | \u001b[0m 1.808   \u001b[0m | \u001b[0m 30.19   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.47803 + 0.00196335\n",
            "[1000]\tcv_agg's rmse: 0.477145 + 0.00200376\n",
            ", best_score: 0.477004, best_iteration: 1222\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.477   \u001b[0m | \u001b[0m 0.9142  \u001b[0m | \u001b[0m 0.5978  \u001b[0m | \u001b[0m 0.546   \u001b[0m | \u001b[0m 0.943   \u001b[0m | \u001b[0m 30.5    \u001b[0m | \u001b[0m 0.03498 \u001b[0m | \u001b[0m 1.47    \u001b[0m | \u001b[0m 30.7    \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478245 + 0.00197725\n",
            "[1000]\tcv_agg's rmse: 0.477461 + 0.0020393\n",
            ", best_score: 0.477374, best_iteration: 1326\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.4774  \u001b[0m | \u001b[0m 0.9703  \u001b[0m | \u001b[0m 0.5142  \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.9769  \u001b[0m | \u001b[0m 27.72   \u001b[0m | \u001b[0m 0.06896 \u001b[0m | \u001b[0m 1.945   \u001b[0m | \u001b[0m 30.41   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478035 + 0.00201662\n",
            "[1000]\tcv_agg's rmse: 0.476765 + 0.00198819\n",
            "[1500]\tcv_agg's rmse: 0.476342 + 0.00201367\n",
            ", best_score: 0.476326, best_iteration: 1476\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.4763  \u001b[0m | \u001b[0m 0.9007  \u001b[0m | \u001b[0m 0.3076  \u001b[0m | \u001b[0m 0.9438  \u001b[0m | \u001b[0m 0.5699  \u001b[0m | \u001b[0m 28.67   \u001b[0m | \u001b[0m 0.005864\u001b[0m | \u001b[0m 0.1587  \u001b[0m | \u001b[0m 27.97   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478076 + 0.00213262\n",
            "[1000]\tcv_agg's rmse: 0.477162 + 0.00207047\n",
            ", best_score: 0.477030, best_iteration: 1235\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.477   \u001b[0m | \u001b[0m 0.9242  \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 0.01602 \u001b[0m | \u001b[0m 0.4228  \u001b[0m | \u001b[0m 30.47   \u001b[0m | \u001b[0m 0.05135 \u001b[0m | \u001b[0m 0.4063  \u001b[0m | \u001b[0m 30.4    \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478069 + 0.00202701\n",
            "[1000]\tcv_agg's rmse: 0.477072 + 0.0020118\n",
            "[1500]\tcv_agg's rmse: 0.476863 + 0.0019661\n",
            ", best_score: 0.476856, best_iteration: 1542\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.4769  \u001b[0m | \u001b[0m 0.9462  \u001b[0m | \u001b[0m 0.3488  \u001b[0m | \u001b[0m 0.8038  \u001b[0m | \u001b[0m 0.9667  \u001b[0m | \u001b[0m 30.47   \u001b[0m | \u001b[0m 0.05485 \u001b[0m | \u001b[0m 0.1047  \u001b[0m | \u001b[0m 29.78   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.481954 + 0.00211449\n",
            "[1000]\tcv_agg's rmse: 0.480107 + 0.00224189\n",
            "[1500]\tcv_agg's rmse: 0.479402 + 0.00223242\n",
            "[2000]\tcv_agg's rmse: 0.479121 + 0.00223542\n",
            "[2500]\tcv_agg's rmse: 0.478955 + 0.0022173\n",
            ", best_score: 0.478910, best_iteration: 2788\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.4789  \u001b[0m | \u001b[0m 0.9825  \u001b[0m | \u001b[0m 0.5929  \u001b[0m | \u001b[0m 0.9046  \u001b[0m | \u001b[0m 0.03868 \u001b[0m | \u001b[0m 30.32   \u001b[0m | \u001b[0m 0.1294  \u001b[0m | \u001b[0m 0.1187  \u001b[0m | \u001b[0m 13.07   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478467 + 0.00200562\n",
            "[1000]\tcv_agg's rmse: 0.477793 + 0.0019682\n",
            ", best_score: 0.477780, best_iteration: 982\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.4778  \u001b[0m | \u001b[0m 0.9272  \u001b[0m | \u001b[0m 0.5821  \u001b[0m | \u001b[0m 0.9878  \u001b[0m | \u001b[0m 0.05213 \u001b[0m | \u001b[0m 26.82   \u001b[0m | \u001b[0m 0.1814  \u001b[0m | \u001b[0m 0.5977  \u001b[0m | \u001b[0m 30.59   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478244 + 0.00201234\n",
            "[1000]\tcv_agg's rmse: 0.477221 + 0.00196706\n",
            ", best_score: 0.477018, best_iteration: 1286\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.477   \u001b[0m | \u001b[0m 0.9005  \u001b[0m | \u001b[0m 0.3086  \u001b[0m | \u001b[0m 0.04871 \u001b[0m | \u001b[0m 0.04828 \u001b[0m | \u001b[0m 24.91   \u001b[0m | \u001b[0m 0.04883 \u001b[0m | \u001b[0m 0.1171  \u001b[0m | \u001b[0m 28.8    \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.477828 + 0.00201959\n",
            "[1000]\tcv_agg's rmse: 0.476804 + 0.00203308\n",
            ", best_score: 0.476513, best_iteration: 1343\n",
            "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.4765  \u001b[0m | \u001b[0m 0.9576  \u001b[0m | \u001b[0m 0.3117  \u001b[0m | \u001b[0m 0.5337  \u001b[0m | \u001b[0m 0.9602  \u001b[0m | \u001b[0m 19.46   \u001b[0m | \u001b[0m 0.01222 \u001b[0m | \u001b[0m 0.1958  \u001b[0m | \u001b[0m 30.61   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478105 + 0.00215418\n",
            "[1000]\tcv_agg's rmse: 0.477147 + 0.00219508\n",
            ", best_score: 0.476920, best_iteration: 1416\n",
            "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.4769  \u001b[0m | \u001b[0m 0.9694  \u001b[0m | \u001b[0m 0.3181  \u001b[0m | \u001b[0m 0.1194  \u001b[0m | \u001b[0m 0.2022  \u001b[0m | \u001b[0m 25.87   \u001b[0m | \u001b[0m 0.02182 \u001b[0m | \u001b[0m 0.8225  \u001b[0m | \u001b[0m 30.64   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478494 + 0.00194562\n",
            "[1000]\tcv_agg's rmse: 0.477714 + 0.00193646\n",
            ", best_score: 0.477641, best_iteration: 1096\n",
            "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.4776  \u001b[0m | \u001b[0m 0.996   \u001b[0m | \u001b[0m 0.3859  \u001b[0m | \u001b[0m 0.1644  \u001b[0m | \u001b[0m 0.1831  \u001b[0m | \u001b[0m 28.32   \u001b[0m | \u001b[0m 0.0685  \u001b[0m | \u001b[0m 0.0386  \u001b[0m | \u001b[0m 29.69   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.47789 + 0.00202927\n",
            "[1000]\tcv_agg's rmse: 0.476855 + 0.00207053\n",
            ", best_score: 0.476645, best_iteration: 1330\n",
            "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.4766  \u001b[0m | \u001b[0m 0.9001  \u001b[0m | \u001b[0m 0.4926  \u001b[0m | \u001b[0m 0.1599  \u001b[0m | \u001b[0m 0.1402  \u001b[0m | \u001b[0m 29.41   \u001b[0m | \u001b[0m 0.000305\u001b[0m | \u001b[0m 0.5245  \u001b[0m | \u001b[0m 30.04   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478454 + 0.00203336\n",
            "[1000]\tcv_agg's rmse: 0.47783 + 0.00205531\n",
            ", best_score: 0.477825, best_iteration: 1004\n",
            "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.4778  \u001b[0m | \u001b[0m 0.9633  \u001b[0m | \u001b[0m 0.5125  \u001b[0m | \u001b[0m 0.1275  \u001b[0m | \u001b[0m 0.8745  \u001b[0m | \u001b[0m 24.15   \u001b[0m | \u001b[0m 0.1108  \u001b[0m | \u001b[0m 0.08994 \u001b[0m | \u001b[0m 30.91   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478292 + 0.00203482\n",
            "[1000]\tcv_agg's rmse: 0.477628 + 0.00209235\n",
            ", best_score: 0.477572, best_iteration: 1113\n",
            "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.4776  \u001b[0m | \u001b[0m 0.9388  \u001b[0m | \u001b[0m 0.505   \u001b[0m | \u001b[0m 0.8674  \u001b[0m | \u001b[0m 0.06218 \u001b[0m | \u001b[0m 29.36   \u001b[0m | \u001b[0m 0.109   \u001b[0m | \u001b[0m 1.686   \u001b[0m | \u001b[0m 30.72   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478246 + 0.00185886\n",
            "[1000]\tcv_agg's rmse: 0.477409 + 0.00187113\n",
            ", best_score: 0.477343, best_iteration: 1104\n",
            "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.4773  \u001b[0m | \u001b[0m 0.9854  \u001b[0m | \u001b[0m 0.5278  \u001b[0m | \u001b[0m 0.971   \u001b[0m | \u001b[0m 0.9085  \u001b[0m | \u001b[0m 25.37   \u001b[0m | \u001b[0m 0.05587 \u001b[0m | \u001b[0m 0.8383  \u001b[0m | \u001b[0m 30.84   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.4784 + 0.0021218\n",
            "[1000]\tcv_agg's rmse: 0.477217 + 0.00208869\n",
            "[1500]\tcv_agg's rmse: 0.476759 + 0.00207061\n",
            ", best_score: 0.476710, best_iteration: 1609\n",
            "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.4767  \u001b[0m | \u001b[0m 0.9625  \u001b[0m | \u001b[0m 0.421   \u001b[0m | \u001b[0m 0.9351  \u001b[0m | \u001b[0m 0.5561  \u001b[0m | \u001b[0m 30.69   \u001b[0m | \u001b[0m 0.009952\u001b[0m | \u001b[0m 0.07169 \u001b[0m | \u001b[0m 27.95   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.477988 + 0.00196696\n",
            "[1000]\tcv_agg's rmse: 0.476811 + 0.00194678\n",
            "[1500]\tcv_agg's rmse: 0.476482 + 0.00196922\n",
            ", best_score: 0.476448, best_iteration: 1668\n",
            "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.4764  \u001b[0m | \u001b[0m 0.9555  \u001b[0m | \u001b[0m 0.3232  \u001b[0m | \u001b[0m 0.8586  \u001b[0m | \u001b[0m 0.891   \u001b[0m | \u001b[0m 12.78   \u001b[0m | \u001b[0m 0.01476 \u001b[0m | \u001b[0m 1.551   \u001b[0m | \u001b[0m 30.97   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478159 + 0.00204541\n",
            "[1000]\tcv_agg's rmse: 0.477209 + 0.00207094\n",
            ", best_score: 0.476999, best_iteration: 1341\n",
            "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.477   \u001b[0m | \u001b[0m 0.951   \u001b[0m | \u001b[0m 0.3916  \u001b[0m | \u001b[0m 0.3343  \u001b[0m | \u001b[0m 0.03914 \u001b[0m | \u001b[0m 25.48   \u001b[0m | \u001b[0m 0.03365 \u001b[0m | \u001b[0m 0.4867  \u001b[0m | \u001b[0m 30.99   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478502 + 0.00207986\n",
            "[1000]\tcv_agg's rmse: 0.477358 + 0.00205888\n",
            ", best_score: 0.477253, best_iteration: 1271\n",
            "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.4773  \u001b[0m | \u001b[0m 0.9358  \u001b[0m | \u001b[0m 0.3724  \u001b[0m | \u001b[0m 0.9861  \u001b[0m | \u001b[0m 0.9529  \u001b[0m | \u001b[0m 8.23    \u001b[0m | \u001b[0m 0.06242 \u001b[0m | \u001b[0m 0.04951 \u001b[0m | \u001b[0m 29.67   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.477975 + 0.00200413\n",
            "[1000]\tcv_agg's rmse: 0.477065 + 0.00197994\n",
            ", best_score: 0.476890, best_iteration: 1343\n",
            "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.4769  \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 0.5121  \u001b[0m | \u001b[0m 0.4212  \u001b[0m | \u001b[0m 0.8482  \u001b[0m | \u001b[0m 29.34   \u001b[0m | \u001b[0m 0.01642 \u001b[0m | \u001b[0m 1.622   \u001b[0m | \u001b[0m 30.72   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.47831 + 0.00214945\n",
            "[1000]\tcv_agg's rmse: 0.477509 + 0.00214592\n",
            ", best_score: 0.477497, best_iteration: 1074\n",
            "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.4775  \u001b[0m | \u001b[0m 0.9628  \u001b[0m | \u001b[0m 0.4353  \u001b[0m | \u001b[0m 0.6977  \u001b[0m | \u001b[0m 0.5611  \u001b[0m | \u001b[0m 28.34   \u001b[0m | \u001b[0m 0.07511 \u001b[0m | \u001b[0m 0.02417 \u001b[0m | \u001b[0m 30.96   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.478145 + 0.00211293\n",
            "[1000]\tcv_agg's rmse: 0.477149 + 0.00212169\n",
            ", best_score: 0.476961, best_iteration: 1319\n",
            "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.477   \u001b[0m | \u001b[0m 0.9485  \u001b[0m | \u001b[0m 0.3481  \u001b[0m | \u001b[0m 0.8756  \u001b[0m | \u001b[0m 0.9091  \u001b[0m | \u001b[0m 30.36   \u001b[0m | \u001b[0m 0.04529 \u001b[0m | \u001b[0m 0.1843  \u001b[0m | \u001b[0m 28.86   \u001b[0m |\n",
            "[500]\tcv_agg's rmse: 0.477994 + 0.00214787\n",
            "[1000]\tcv_agg's rmse: 0.477001 + 0.00215495\n",
            ", best_score: 0.476724, best_iteration: 1348\n",
            "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.4767  \u001b[0m | \u001b[0m 0.9411  \u001b[0m | \u001b[0m 0.377   \u001b[0m | \u001b[0m 0.4169  \u001b[0m | \u001b[0m 0.1497  \u001b[0m | \u001b[0m 28.7    \u001b[0m | \u001b[0m 0.01958 \u001b[0m | \u001b[0m 0.2997  \u001b[0m | \u001b[0m 30.97   \u001b[0m |\n",
            "=========================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vab0i8qvdU41",
        "colab_type": "code",
        "outputId": "ba7443c7-207f-4a7d-a564-d29f62528e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "lgb_BO_scores = pd.DataFrame()\n",
        "for dic in lgb_BO.res:\n",
        "  param = dic['params']\n",
        "  param = pd.DataFrame([param])\n",
        "  lgb_BO_scores = pd.concat([lgb_BO_scores, param], ignore_index=True)\n",
        "\n",
        "lgb_BO_scores['score'] = pd.DataFrame(lgb_BO.res)['target']\n",
        "lgb_BO_scores = lgb_BO_scores.sort_values(by='score',ascending=False)\n",
        "lgb_BO_scores.to_csv(\"/content/drive/My Drive/Kaggle_Allstate/data/tuned_lgb_parameters_gpu.csv\", index=False)\n",
        "lgb_BO_scores.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bagging_fraction</th>\n",
              "      <th>feature_fraction</th>\n",
              "      <th>lambda_l1</th>\n",
              "      <th>lambda_l2</th>\n",
              "      <th>max_depth</th>\n",
              "      <th>min_gain_to_split</th>\n",
              "      <th>min_sum_hessian_in_leaf</th>\n",
              "      <th>num_leaves</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.980568</td>\n",
              "      <td>0.303306</td>\n",
              "      <td>0.550268</td>\n",
              "      <td>0.029935</td>\n",
              "      <td>27.089429</td>\n",
              "      <td>0.002784</td>\n",
              "      <td>0.145851</td>\n",
              "      <td>30.808009</td>\n",
              "      <td>-0.476282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.900711</td>\n",
              "      <td>0.307562</td>\n",
              "      <td>0.943830</td>\n",
              "      <td>0.569945</td>\n",
              "      <td>28.668160</td>\n",
              "      <td>0.005864</td>\n",
              "      <td>0.158714</td>\n",
              "      <td>27.967231</td>\n",
              "      <td>-0.476326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.904914</td>\n",
              "      <td>0.443526</td>\n",
              "      <td>0.972553</td>\n",
              "      <td>0.434593</td>\n",
              "      <td>18.605972</td>\n",
              "      <td>0.007592</td>\n",
              "      <td>0.147021</td>\n",
              "      <td>30.662430</td>\n",
              "      <td>-0.476424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.955465</td>\n",
              "      <td>0.323185</td>\n",
              "      <td>0.858551</td>\n",
              "      <td>0.890984</td>\n",
              "      <td>12.777574</td>\n",
              "      <td>0.014763</td>\n",
              "      <td>1.550804</td>\n",
              "      <td>30.968166</td>\n",
              "      <td>-0.476448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.901150</td>\n",
              "      <td>0.335025</td>\n",
              "      <td>0.836701</td>\n",
              "      <td>0.065558</td>\n",
              "      <td>30.376536</td>\n",
              "      <td>0.022011</td>\n",
              "      <td>1.563845</td>\n",
              "      <td>30.618696</td>\n",
              "      <td>-0.476492</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    bagging_fraction  feature_fraction  ...  num_leaves     score\n",
              "20          0.980568          0.303306  ...   30.808009 -0.476282\n",
              "24          0.900711          0.307562  ...   27.967231 -0.476326\n",
              "19          0.904914          0.443526  ...   30.662430 -0.476424\n",
              "38          0.955465          0.323185  ...   30.968166 -0.476448\n",
              "13          0.901150          0.335025  ...   30.618696 -0.476492\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPicmzw8eYQQ",
        "colab_type": "code",
        "outputId": "93696168-dbbb-49e0-c9dc-78c2e3ecc4fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "params = lgb_BO_scores.iloc[0].to_dict()\n",
        "best_auto_lgb_params = dict()\n",
        "best_auto_lgb_params['objective'] = 'regression'\n",
        "best_auto_lgb_params[\"metric\"] = 'rmse'\n",
        "best_auto_lgb_params['learning_rate'] = 0.01 # Smaller learning rate\n",
        "best_auto_lgb_params['num_leaves'] = int(params['num_leaves'])    \n",
        "best_auto_lgb_params['max_depth'] = int(params['max_depth'])    \n",
        "best_auto_lgb_params['min_sum_hessian_in_leaf'] = int(params['min_sum_hessian_in_leaf'])\n",
        "best_auto_lgb_params['min_gain_to_split'] = params['min_gain_to_split']     \n",
        "best_auto_lgb_params['feature_fraction'] = params['feature_fraction']\n",
        "best_auto_lgb_params['bagging_fraction'] = params['bagging_fraction']\n",
        "best_auto_lgb_params['bagging_freq'] = 1\n",
        "best_auto_lgb_params['lambda_l2'] = params['lambda_l2']\n",
        "best_auto_lgb_params['lambda_l1'] = params['lambda_l1']\n",
        "best_auto_lgb_params['seed'] = 1234\n",
        "\n",
        "\n",
        "print (best_auto_lgb_params)\n",
        "\n",
        "\n",
        "lgb_cv = lgb.cv(best_auto_lgb_params,\n",
        "            lgb.Dataset(train_x,\n",
        "                        label=train_y\n",
        "                        ),\n",
        "            num_boost_round=100000,\n",
        "            nfold=kfolds,\n",
        "            folds=skf.split(train_x,train_y),\n",
        "            early_stopping_rounds=50,\n",
        "            verbose_eval=500)\n",
        "\n",
        "best_lgb_score = min(lgb_cv['rmse-mean'])\n",
        "best_lgb_iteration = len(lgb_cv['rmse-mean'])\n",
        "print (', best_score: %f, best_iteration: %d' % (best_lgb_score, best_lgb_iteration))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.01, 'num_leaves': 30, 'max_depth': 27, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 0.0027838726568518712, 'feature_fraction': 0.30330552664662813, 'bagging_fraction': 0.9805677235689514, 'bagging_freq': 1, 'lambda_l2': 0.02993467470115463, 'lambda_l1': 0.5502676113750163, 'seed': 1234}\n",
            "[500]\tcv_agg's rmse: 0.493225 + 0.00171951\n",
            "[1000]\tcv_agg's rmse: 0.481638 + 0.00192038\n",
            "[1500]\tcv_agg's rmse: 0.478804 + 0.00200583\n",
            "[2000]\tcv_agg's rmse: 0.477905 + 0.00202181\n",
            "[2500]\tcv_agg's rmse: 0.477369 + 0.00203447\n",
            "[3000]\tcv_agg's rmse: 0.476991 + 0.00202457\n",
            "[3500]\tcv_agg's rmse: 0.476689 + 0.00202678\n",
            "[4000]\tcv_agg's rmse: 0.476468 + 0.00201119\n",
            "[4500]\tcv_agg's rmse: 0.476279 + 0.0020047\n",
            "[5000]\tcv_agg's rmse: 0.476129 + 0.00201517\n",
            "[5500]\tcv_agg's rmse: 0.475983 + 0.0020029\n",
            "[6000]\tcv_agg's rmse: 0.475851 + 0.00201055\n",
            "[6500]\tcv_agg's rmse: 0.475759 + 0.00201582\n",
            "[7000]\tcv_agg's rmse: 0.475687 + 0.00202182\n",
            "[7500]\tcv_agg's rmse: 0.475626 + 0.00201748\n",
            "[8000]\tcv_agg's rmse: 0.475559 + 0.00201591\n",
            ", best_score: 0.475557, best_iteration: 8035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMNbrTDb24rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def lgb_rgr_stack(rgr_params, train_x, train_y, test_x, kfolds, early_stopping_rounds=0, train_y_dummy=None):\n",
        "\n",
        "    skf = KFold(n_splits=kfolds,random_state=1234)\n",
        "    skf_ids = list(skf.split(train_y))\n",
        "\n",
        "\n",
        "    train_blend_x = np.zeros((train_x.shape[0], len(rgr_params)))\n",
        "    test_blend_x = np.zeros((test_x.shape[0], len(rgr_params)))\n",
        "    blend_scores = np.zeros ((kfolds,len(rgr_params)))\n",
        "\n",
        "    print(\"Start stacking.\")\n",
        "    for j, params in enumerate(rgr_params):\n",
        "        print(\"Stacking model\",j+1, params)\n",
        "        test_blend_x_j = np.zeros((test_x.shape[0]))\n",
        "        for i, (train_ids, val_ids) in enumerate(skf_ids):\n",
        "            start = time.time()\n",
        "            print(\"Model %d fold %d\" %(j+1,i+1))\n",
        "            train_x_fold = train_x[train_ids]\n",
        "            train_y_fold = train_y[train_ids]\n",
        "            val_x_fold = train_x[val_ids]\n",
        "            val_y_fold = train_y[val_ids]\n",
        "            print(i, params)\n",
        "            \n",
        "            \n",
        "            if early_stopping_rounds==0:\n",
        "                num_boost_round = copy.deepcopy(params['num_boost_round'])\n",
        "                model = lgb.train(params,\n",
        "                                    lgb.Dataset(train_x_fold, \n",
        "                                                train_y_fold\n",
        "                                               ),\n",
        "                                  num_boost_round=num_boost_round,\n",
        "                                  verbose_eval=500\n",
        "                )\n",
        "                val_y_predict_fold = model.predict(val_x_fold)\n",
        "                score = np.sqrt(mean_squared_error(val_y_fold,val_y_predict_fold))\n",
        "                print(\"Score for Model %d fold %d: %f \" % (j+1,i+1,score))\n",
        "                blend_scores[i,j]=score\n",
        "                train_blend_x[val_ids, j] = val_y_predict_fold\n",
        "                test_blend_x_j = test_blend_x_j + model.predict(test_x)\n",
        "                print(\"Model %d fold %d finished in %d seconds.\" % (j+1,i+1, time.time()-start))\n",
        "            else:\n",
        "                model = lgb.train(params,\n",
        "                                    lgb.Dataset(train_x_fold, \n",
        "                                                train_y_fold\n",
        "                                               ),\n",
        "                                  valid_sets=[lgb.Dataset(val_x_fold, \n",
        "                                                val_y_fold\n",
        "                                                         )],\n",
        "                                  valid_names=['valid'],\n",
        "                                  num_boost_round=10000000,\n",
        "                                  early_stopping_rounds = early_stopping_rounds,\n",
        "                                  verbose_eval=500\n",
        "                                )\n",
        "                best_iteration = model.best_iteration\n",
        "                print(model.best_score['valid']['rmse'])\n",
        "                val_y_predict_fold = model.predict(val_x_fold, num_iteration=best_iteration)\n",
        "                score = np.sqrt(mean_squared_error(val_y_fold,val_y_predict_fold))\n",
        "                print(\"Score for Model %d fold %d: %f \" % (j+1,i+1,score))\n",
        "                blend_scores[i,j]=score\n",
        "                train_blend_x[val_ids, j] = val_y_predict_fold\n",
        "                test_blend_x_j = test_blend_x_j + model.predict(test_x, num_iteration=best_iteration)\n",
        "                print(\"Model %d fold %d finished in %d seconds.\" % (j+1,i+1, time.time()-start))                \n",
        "                \n",
        "        test_blend_x[:,j] = test_blend_x_j/kfolds\n",
        "        print(\"Score for model %d is %f\" % (j+1,np.mean(blend_scores[:,j])))\n",
        "    return train_blend_x, test_blend_x, blend_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKiQHT7b6ngM",
        "colab_type": "code",
        "outputId": "d454ce9f-5e85-45c9-dd05-962d1f420554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "lgb_params = []\n",
        "## Top 5 auto-tuned parameters\n",
        "for i in range(4):\n",
        "    params=dict()\n",
        "    params['num_leaves'] = int(lgb_BO_scores['num_leaves'][i])\n",
        "    params['max_depth'] = int(lgb_BO_scores['max_depth'][i])\n",
        "    params['min_sum_hessian_in_leaf'] = int(lgb_BO_scores['min_sum_hessian_in_leaf'][i])\n",
        "    params['min_gain_to_split'] = lgb_BO_scores['min_gain_to_split'][i]\n",
        "    params['feature_fraction'] = lgb_BO_scores['feature_fraction'][i]\n",
        "    params['bagging_fraction'] = lgb_BO_scores['bagging_fraction'][i]\n",
        "    params['bagging_freq'] = 1\n",
        "    params['lambda_l2'] = lgb_BO_scores['lambda_l2'][i]\n",
        "    params['lambda_l1'] = int(lgb_BO_scores['lambda_l1'][i])\n",
        "    params['objective'] = 'regression'\n",
        "    params['learning_rate'] = 0.05\n",
        "    params['num_boost_round']=best_lgb_iteration\n",
        "    params['seed'] = 1234\n",
        "    params[\"metric\"] = 'rmse'\n",
        "    params['device'] = 'gpu'\n",
        "    lgb_params.append(params)\n",
        "\n",
        "## Best manual-tuned parameters\n",
        "lgb_params.append(best_lgb_params)    \n",
        "print(lgb_params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 17, 'max_depth': 29, 'min_sum_hessian_in_leaf': 1, 'min_gain_to_split': 2.3216378457422437, 'feature_fraction': 0.4315001535921186, 'bagging_fraction': 0.995522553917235, 'bagging_freq': 1, 'lambda_l2': 0.9565641938921855, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}, {'num_leaves': 22, 'max_depth': 12, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 0.7885742928896289, 'feature_fraction': 0.3005860158457473, 'bagging_fraction': 0.9202818625421475, 'bagging_freq': 1, 'lambda_l2': 0.18270342809299156, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}, {'num_leaves': 15, 'max_depth': 26, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 1.9659221466636223, 'feature_fraction': 0.5135254267026184, 'bagging_fraction': 0.9151811663109417, 'bagging_freq': 1, 'lambda_l2': 0.8489393304716284, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}, {'num_leaves': 9, 'max_depth': 20, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 4.535170138877629, 'feature_fraction': 0.5019959113084462, 'bagging_fraction': 0.9594229817959025, 'bagging_freq': 1, 'lambda_l2': 0.48588842619015526, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}, {'learning_rate': 0.05, 'metric': 'rmse', 'bagging_freq': 1, 'seed': 1234, 'objective': 'regression', 'device': 'gpu', 'num_leaves': 127, 'max_depth': -1, 'min_gain_to_split': 0, 'feature_fraction': 0.3, 'bagging_fraction': 0.8, 'min_sum_hessian_in_leaf': 100, 'lambda_l2': 0.1, 'lambda_l1': 0.01}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opOW1TDI6ylI",
        "colab_type": "code",
        "outputId": "4a7ff12f-6764-4ca1-b680-343345e8a763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_blend_x_lgb_ohe, test_blend_x_lgb_ohe, blend_scores_lgb_ohe = lgb_rgr_stack(lgb_params, train_x, train_y, test_x, 4, early_stopping_rounds=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start stacking.\n",
            "Stacking model 1 {'num_leaves': 17, 'max_depth': 29, 'min_sum_hessian_in_leaf': 1, 'min_gain_to_split': 2.3216378457422437, 'feature_fraction': 0.4315001535921186, 'bagging_fraction': 0.995522553917235, 'bagging_freq': 1, 'lambda_l2': 0.9565641938921855, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n",
            "Model 1 fold 1\n",
            "0 {'num_leaves': 17, 'max_depth': 29, 'min_sum_hessian_in_leaf': 1, 'min_gain_to_split': 2.3216378457422437, 'feature_fraction': 0.4315001535921186, 'bagging_fraction': 0.995522553917235, 'bagging_freq': 1, 'lambda_l2': 0.9565641938921855, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.484489\n",
            "Early stopping, best iteration is:\n",
            "[635]\tvalid's rmse: 0.484476\n",
            "0.4844759335605612\n",
            "Score for Model 1 fold 1: 0.484476 \n",
            "Model 1 fold 1 finished in 17 seconds.\n",
            "Model 1 fold 2\n",
            "1 {'num_leaves': 17, 'max_depth': 29, 'min_sum_hessian_in_leaf': 1, 'min_gain_to_split': 2.3216378457422437, 'feature_fraction': 0.4315001535921186, 'bagging_fraction': 0.995522553917235, 'bagging_freq': 1, 'lambda_l2': 0.9565641938921855, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.48404\n",
            "Early stopping, best iteration is:\n",
            "[758]\tvalid's rmse: 0.48402\n",
            "0.4840199362595683\n",
            "Score for Model 1 fold 2: 0.484020 \n",
            "Model 1 fold 2 finished in 19 seconds.\n",
            "Model 1 fold 3\n",
            "2 {'num_leaves': 17, 'max_depth': 29, 'min_sum_hessian_in_leaf': 1, 'min_gain_to_split': 2.3216378457422437, 'feature_fraction': 0.4315001535921186, 'bagging_fraction': 0.995522553917235, 'bagging_freq': 1, 'lambda_l2': 0.9565641938921855, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.486367\n",
            "Early stopping, best iteration is:\n",
            "[938]\tvalid's rmse: 0.48632\n",
            "0.48632006430177704\n",
            "Score for Model 1 fold 3: 0.486320 \n",
            "Model 1 fold 3 finished in 21 seconds.\n",
            "Model 1 fold 4\n",
            "3 {'num_leaves': 17, 'max_depth': 29, 'min_sum_hessian_in_leaf': 1, 'min_gain_to_split': 2.3216378457422437, 'feature_fraction': 0.4315001535921186, 'bagging_fraction': 0.995522553917235, 'bagging_freq': 1, 'lambda_l2': 0.9565641938921855, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.48486\n",
            "Early stopping, best iteration is:\n",
            "[493]\tvalid's rmse: 0.48486\n",
            "0.4848596854356977\n",
            "Score for Model 1 fold 4: 0.484860 \n",
            "Model 1 fold 4 finished in 14 seconds.\n",
            "Score for model 1 is 0.484919\n",
            "Stacking model 2 {'num_leaves': 22, 'max_depth': 12, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 0.7885742928896289, 'feature_fraction': 0.3005860158457473, 'bagging_fraction': 0.9202818625421475, 'bagging_freq': 1, 'lambda_l2': 0.18270342809299156, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n",
            "Model 2 fold 1\n",
            "0 {'num_leaves': 22, 'max_depth': 12, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 0.7885742928896289, 'feature_fraction': 0.3005860158457473, 'bagging_fraction': 0.9202818625421475, 'bagging_freq': 1, 'lambda_l2': 0.18270342809299156, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.480565\n",
            "[1000]\tvalid's rmse: 0.48034\n",
            "[1500]\tvalid's rmse: 0.48026\n",
            "Early stopping, best iteration is:\n",
            "[1772]\tvalid's rmse: 0.480212\n",
            "0.4802123776805648\n",
            "Score for Model 2 fold 1: 0.480214 \n",
            "Model 2 fold 1 finished in 37 seconds.\n",
            "Model 2 fold 2\n",
            "1 {'num_leaves': 22, 'max_depth': 12, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 0.7885742928896289, 'feature_fraction': 0.3005860158457473, 'bagging_fraction': 0.9202818625421475, 'bagging_freq': 1, 'lambda_l2': 0.18270342809299156, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.479946\n",
            "[1000]\tvalid's rmse: 0.479697\n",
            "Early stopping, best iteration is:\n",
            "[1410]\tvalid's rmse: 0.479647\n",
            "0.47964729838958947\n",
            "Score for Model 2 fold 2: 0.479647 \n",
            "Model 2 fold 2 finished in 30 seconds.\n",
            "Model 2 fold 3\n",
            "2 {'num_leaves': 22, 'max_depth': 12, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 0.7885742928896289, 'feature_fraction': 0.3005860158457473, 'bagging_fraction': 0.9202818625421475, 'bagging_freq': 1, 'lambda_l2': 0.18270342809299156, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.48204\n",
            "[1000]\tvalid's rmse: 0.481792\n",
            "[1500]\tvalid's rmse: 0.481711\n",
            "Early stopping, best iteration is:\n",
            "[1625]\tvalid's rmse: 0.481698\n",
            "0.4816984813356717\n",
            "Score for Model 2 fold 3: 0.481701 \n",
            "Model 2 fold 3 finished in 33 seconds.\n",
            "Model 2 fold 4\n",
            "3 {'num_leaves': 22, 'max_depth': 12, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 0.7885742928896289, 'feature_fraction': 0.3005860158457473, 'bagging_fraction': 0.9202818625421475, 'bagging_freq': 1, 'lambda_l2': 0.18270342809299156, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.480486\n",
            "[1000]\tvalid's rmse: 0.480272\n",
            "[1500]\tvalid's rmse: 0.480201\n",
            "Early stopping, best iteration is:\n",
            "[1555]\tvalid's rmse: 0.48019\n",
            "0.4801900756558205\n",
            "Score for Model 2 fold 4: 0.480193 \n",
            "Model 2 fold 4 finished in 31 seconds.\n",
            "Score for model 2 is 0.480439\n",
            "Stacking model 3 {'num_leaves': 15, 'max_depth': 26, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 1.9659221466636223, 'feature_fraction': 0.5135254267026184, 'bagging_fraction': 0.9151811663109417, 'bagging_freq': 1, 'lambda_l2': 0.8489393304716284, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n",
            "Model 3 fold 1\n",
            "0 {'num_leaves': 15, 'max_depth': 26, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 1.9659221466636223, 'feature_fraction': 0.5135254267026184, 'bagging_fraction': 0.9151811663109417, 'bagging_freq': 1, 'lambda_l2': 0.8489393304716284, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.484288\n",
            "[1000]\tvalid's rmse: 0.484129\n",
            "Early stopping, best iteration is:\n",
            "[1309]\tvalid's rmse: 0.484096\n",
            "0.4840959407595176\n",
            "Score for Model 3 fold 1: 0.484099 \n",
            "Model 3 fold 1 finished in 30 seconds.\n",
            "Model 3 fold 2\n",
            "1 {'num_leaves': 15, 'max_depth': 26, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 1.9659221466636223, 'feature_fraction': 0.5135254267026184, 'bagging_fraction': 0.9151811663109417, 'bagging_freq': 1, 'lambda_l2': 0.8489393304716284, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.483331\n",
            "[1000]\tvalid's rmse: 0.483147\n",
            "Early stopping, best iteration is:\n",
            "[1240]\tvalid's rmse: 0.483095\n",
            "0.4830951658169097\n",
            "Score for Model 3 fold 2: 0.483097 \n",
            "Model 3 fold 2 finished in 28 seconds.\n",
            "Model 3 fold 3\n",
            "2 {'num_leaves': 15, 'max_depth': 26, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 1.9659221466636223, 'feature_fraction': 0.5135254267026184, 'bagging_fraction': 0.9151811663109417, 'bagging_freq': 1, 'lambda_l2': 0.8489393304716284, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.485521\n",
            "[1000]\tvalid's rmse: 0.485349\n",
            "Early stopping, best iteration is:\n",
            "[1370]\tvalid's rmse: 0.485256\n",
            "0.4852560881827074\n",
            "Score for Model 3 fold 3: 0.485256 \n",
            "Model 3 fold 3 finished in 29 seconds.\n",
            "Model 3 fold 4\n",
            "3 {'num_leaves': 15, 'max_depth': 26, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 1.9659221466636223, 'feature_fraction': 0.5135254267026184, 'bagging_fraction': 0.9151811663109417, 'bagging_freq': 1, 'lambda_l2': 0.8489393304716284, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.484042\n",
            "[1000]\tvalid's rmse: 0.483867\n",
            "[1500]\tvalid's rmse: 0.483777\n",
            "Early stopping, best iteration is:\n",
            "[1545]\tvalid's rmse: 0.483768\n",
            "0.4837683170783715\n",
            "Score for Model 3 fold 4: 0.483768 \n",
            "Model 3 fold 4 finished in 32 seconds.\n",
            "Score for model 3 is 0.484055\n",
            "Stacking model 4 {'num_leaves': 9, 'max_depth': 20, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 4.535170138877629, 'feature_fraction': 0.5019959113084462, 'bagging_fraction': 0.9594229817959025, 'bagging_freq': 1, 'lambda_l2': 0.48588842619015526, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n",
            "Model 4 fold 1\n",
            "0 {'num_leaves': 9, 'max_depth': 20, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 4.535170138877629, 'feature_fraction': 0.5019959113084462, 'bagging_fraction': 0.9594229817959025, 'bagging_freq': 1, 'lambda_l2': 0.48588842619015526, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.488531\n",
            "Early stopping, best iteration is:\n",
            "[792]\tvalid's rmse: 0.488458\n",
            "0.48845812910045844\n",
            "Score for Model 4 fold 1: 0.488458 \n",
            "Model 4 fold 1 finished in 20 seconds.\n",
            "Model 4 fold 2\n",
            "1 {'num_leaves': 9, 'max_depth': 20, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 4.535170138877629, 'feature_fraction': 0.5019959113084462, 'bagging_fraction': 0.9594229817959025, 'bagging_freq': 1, 'lambda_l2': 0.48588842619015526, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.488168\n",
            "Early stopping, best iteration is:\n",
            "[599]\tvalid's rmse: 0.488121\n",
            "0.4881210653384182\n",
            "Score for Model 4 fold 2: 0.488121 \n",
            "Model 4 fold 2 finished in 16 seconds.\n",
            "Model 4 fold 3\n",
            "2 {'num_leaves': 9, 'max_depth': 20, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 4.535170138877629, 'feature_fraction': 0.5019959113084462, 'bagging_fraction': 0.9594229817959025, 'bagging_freq': 1, 'lambda_l2': 0.48588842619015526, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.490484\n",
            "[1000]\tvalid's rmse: 0.490333\n",
            "Early stopping, best iteration is:\n",
            "[1091]\tvalid's rmse: 0.490313\n",
            "0.4903125258247788\n",
            "Score for Model 4 fold 3: 0.490313 \n",
            "Model 4 fold 3 finished in 23 seconds.\n",
            "Model 4 fold 4\n",
            "3 {'num_leaves': 9, 'max_depth': 20, 'min_sum_hessian_in_leaf': 0, 'min_gain_to_split': 4.535170138877629, 'feature_fraction': 0.5019959113084462, 'bagging_fraction': 0.9594229817959025, 'bagging_freq': 1, 'lambda_l2': 0.48588842619015526, 'lambda_l1': 0, 'objective': 'regression', 'learning_rate': 0.05, 'num_boost_round': 8035, 'seed': 1234, 'metric': 'rmse', 'device': 'gpu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[500]\tvalid's rmse: 0.489242\n",
            "Early stopping, best iteration is:\n",
            "[910]\tvalid's rmse: 0.489083\n",
            "0.48908336297114624\n",
            "Score for Model 4 fold 4: 0.489083 \n",
            "Model 4 fold 4 finished in 20 seconds.\n",
            "Score for model 4 is 0.488994\n",
            "Stacking model 5 {'learning_rate': 0.05, 'metric': 'rmse', 'bagging_freq': 1, 'seed': 1234, 'objective': 'regression', 'device': 'gpu', 'num_leaves': 127, 'max_depth': -1, 'min_gain_to_split': 0, 'feature_fraction': 0.3, 'bagging_fraction': 0.8, 'min_sum_hessian_in_leaf': 100, 'lambda_l2': 0.1, 'lambda_l1': 0.01}\n",
            "Model 5 fold 1\n",
            "0 {'learning_rate': 0.05, 'metric': 'rmse', 'bagging_freq': 1, 'seed': 1234, 'objective': 'regression', 'device': 'gpu', 'num_leaves': 127, 'max_depth': -1, 'min_gain_to_split': 0, 'feature_fraction': 0.3, 'bagging_fraction': 0.8, 'min_sum_hessian_in_leaf': 100, 'lambda_l2': 0.1, 'lambda_l1': 0.01}\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[367]\tvalid's rmse: 0.474741\n",
            "0.474740756224145\n",
            "Score for Model 5 fold 1: 0.474741 \n",
            "Model 5 fold 1 finished in 25 seconds.\n",
            "Model 5 fold 2\n",
            "1 {'learning_rate': 0.05, 'metric': 'rmse', 'bagging_freq': 1, 'seed': 1234, 'objective': 'regression', 'device': 'gpu', 'num_leaves': 127, 'max_depth': -1, 'min_gain_to_split': 0, 'feature_fraction': 0.3, 'bagging_fraction': 0.8, 'min_sum_hessian_in_leaf': 100, 'lambda_l2': 0.1, 'lambda_l1': 0.01}\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[432]\tvalid's rmse: 0.475406\n",
            "0.47540596275215835\n",
            "Score for Model 5 fold 2: 0.475406 \n",
            "Model 5 fold 2 finished in 28 seconds.\n",
            "Model 5 fold 3\n",
            "2 {'learning_rate': 0.05, 'metric': 'rmse', 'bagging_freq': 1, 'seed': 1234, 'objective': 'regression', 'device': 'gpu', 'num_leaves': 127, 'max_depth': -1, 'min_gain_to_split': 0, 'feature_fraction': 0.3, 'bagging_fraction': 0.8, 'min_sum_hessian_in_leaf': 100, 'lambda_l2': 0.1, 'lambda_l1': 0.01}\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[394]\tvalid's rmse: 0.477099\n",
            "0.47709882019331734\n",
            "Score for Model 5 fold 3: 0.477099 \n",
            "Model 5 fold 3 finished in 25 seconds.\n",
            "Model 5 fold 4\n",
            "3 {'learning_rate': 0.05, 'metric': 'rmse', 'bagging_freq': 1, 'seed': 1234, 'objective': 'regression', 'device': 'gpu', 'num_leaves': 127, 'max_depth': -1, 'min_gain_to_split': 0, 'feature_fraction': 0.3, 'bagging_fraction': 0.8, 'min_sum_hessian_in_leaf': 100, 'lambda_l2': 0.1, 'lambda_l1': 0.01}\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[432]\tvalid's rmse: 0.474862\n",
            "0.4748622292548901\n",
            "Score for Model 5 fold 4: 0.474862 \n",
            "Model 5 fold 4 finished in 27 seconds.\n",
            "Score for model 5 is 0.475527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R5MYa8L7KHC",
        "colab_type": "code",
        "outputId": "aff1ded8-2f67-43ac-c0b1-7423c92961be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (np.mean(blend_scores_lgb_ohe,axis=0))\n",
        "np.savetxt(\"/content/drive/My Drive/Kaggle_Allstate/data/train_blend_x_lgb_ohe_gpu.csv\",train_blend_x_lgb_ohe, delimiter=\",\")\n",
        "np.savetxt(\"/content/drive/My Drive/Kaggle_Allstate/data/test_blend_x_lgb_ohe_gpu.csv\",test_blend_x_lgb_ohe, delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.484919   0.48043868 0.48405513 0.48899384 0.47552694]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpX73ZfpALU9",
        "colab_type": "code",
        "outputId": "c496d169-6f6e-4541-df6e-1fa3803843c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "blend_scores_lgb_ohe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.48447593, 0.48021392, 0.48409873, 0.48845813, 0.47474076],\n",
              "       [0.48401994, 0.47964733, 0.48309718, 0.48812107, 0.47540596],\n",
              "       [0.48632006, 0.4817007 , 0.48525631, 0.49031282, 0.47709882],\n",
              "       [0.48486008, 0.48019275, 0.48376832, 0.48908336, 0.47486223]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LBDpEuuAPKm",
        "colab_type": "code",
        "outputId": "ea4df98e-09a9-4de3-bf28-7d01360c2319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_blend_x_lgb_ohe)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdHAk3D9A9sD",
        "colab_type": "text"
      },
      "source": [
        "## stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obk3M4CTA-6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Ridge,ElasticNet, SGDRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from numpy import genfromtxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtBHgxNZBgIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_blend_x_xgb_ohe  = genfromtxt(\"/content/drive/My Drive/Kaggle_Allstate/data/train_blend_x_xgb_ohe_gpu.csv\", delimiter=',')\n",
        "test_blend_x_xgb_ohe  = genfromtxt(\"/content/drive/My Drive/Kaggle_Allstate/data/test_blend_x_xgb_ohe_gpu.csv\", delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNO8P6sXA_fj",
        "colab_type": "code",
        "outputId": "37fdf1c1-0358-4340-e328-e92bac2daff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print  (\"stacking.\")\n",
        "param_grid = {\n",
        "    'alpha':[0,0.00001,0.00003,0.0001,0.0003,0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,15,20,25,30,35,40,45,50,55,60,70,100]\n",
        "              }"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stacking.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbAiT6bdBMhf",
        "colab_type": "code",
        "outputId": "888dda4e-0cbd-4026-db8d-a17771d60926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = search_model(np.hstack((train_blend_x_xgb_ohe,\n",
        "                                train_blend_x_lgb_ohe,\n",
        "                                ))\n",
        "                                , train_y\n",
        "                                , Ridge()\n",
        "                                , param_grid\n",
        "                                , n_jobs=1\n",
        "                                , cv=4\n",
        "                                , refit=True)   \n",
        "\n",
        "print(\"best subsample:\", model.best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 26 candidates, totalling 104 fits\n",
            "[CV] alpha=0 .........................................................\n",
            "[CV] ......................... alpha=0, score=-1299.433, total=   0.1s\n",
            "[CV] alpha=0 .........................................................\n",
            "[CV] ......................... alpha=0, score=-1196.636, total=   0.1s\n",
            "[CV] alpha=0 .........................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ......................... alpha=0, score=-1443.885, total=   0.1s\n",
            "[CV] alpha=0 .........................................................\n",
            "[CV] ......................... alpha=0, score=-1132.179, total=   0.1s\n",
            "[CV] alpha=1e-05 .....................................................\n",
            "[CV] ..................... alpha=1e-05, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=1e-05 .....................................................\n",
            "[CV] ..................... alpha=1e-05, score=-1139.180, total=   0.0s\n",
            "[CV] alpha=1e-05 .....................................................\n",
            "[CV] ..................... alpha=1e-05, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=1e-05 .....................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ..................... alpha=1e-05, score=-1131.525, total=   0.0s\n",
            "[CV] alpha=3e-05 .....................................................\n",
            "[CV] ..................... alpha=3e-05, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=3e-05 .....................................................\n",
            "[CV] ..................... alpha=3e-05, score=-1139.180, total=   0.0s\n",
            "[CV] alpha=3e-05 .....................................................\n",
            "[CV] ..................... alpha=3e-05, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=3e-05 .....................................................\n",
            "[CV] ..................... alpha=3e-05, score=-1131.525, total=   0.0s\n",
            "[CV] alpha=0.0001 ....................................................\n",
            "[CV] .................... alpha=0.0001, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=0.0001 ....................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.5s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .................... alpha=0.0001, score=-1139.180, total=   0.0s\n",
            "[CV] alpha=0.0001 ....................................................\n",
            "[CV] .................... alpha=0.0001, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=0.0001 ....................................................\n",
            "[CV] .................... alpha=0.0001, score=-1131.525, total=   0.0s\n",
            "[CV] alpha=0.0003 ....................................................\n",
            "[CV] .................... alpha=0.0003, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=0.0003 ....................................................\n",
            "[CV] .................... alpha=0.0003, score=-1139.180, total=   0.0s\n",
            "[CV] alpha=0.0003 ....................................................\n",
            "[CV] .................... alpha=0.0003, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=0.0003 ....................................................\n",
            "[CV] .................... alpha=0.0003, score=-1131.525, total=   0.0s\n",
            "[CV] alpha=0.001 .....................................................\n",
            "[CV] ..................... alpha=0.001, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=0.001 .....................................................\n",
            "[CV] ..................... alpha=0.001, score=-1139.180, total=   0.0s\n",
            "[CV] alpha=0.001 .....................................................\n",
            "[CV] ..................... alpha=0.001, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=0.001 .....................................................\n",
            "[CV] ..................... alpha=0.001, score=-1131.525, total=   0.0s\n",
            "[CV] alpha=0.003 .....................................................\n",
            "[CV] ..................... alpha=0.003, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=0.003 .....................................................\n",
            "[CV] ..................... alpha=0.003, score=-1139.180, total=   0.0s\n",
            "[CV] alpha=0.003 .....................................................\n",
            "[CV] ..................... alpha=0.003, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=0.003 .....................................................\n",
            "[CV] ..................... alpha=0.003, score=-1131.525, total=   0.0s\n",
            "[CV] alpha=0.01 ......................................................\n",
            "[CV] ...................... alpha=0.01, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=0.01 ......................................................\n",
            "[CV] ...................... alpha=0.01, score=-1139.180, total=   0.0s\n",
            "[CV] alpha=0.01 ......................................................\n",
            "[CV] ...................... alpha=0.01, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=0.01 ......................................................\n",
            "[CV] ...................... alpha=0.01, score=-1131.525, total=   0.0s\n",
            "[CV] alpha=0.03 ......................................................\n",
            "[CV] ...................... alpha=0.03, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=0.03 ......................................................\n",
            "[CV] ...................... alpha=0.03, score=-1139.180, total=   0.0s\n",
            "[CV] alpha=0.03 ......................................................\n",
            "[CV] ...................... alpha=0.03, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=0.03 ......................................................\n",
            "[CV] ...................... alpha=0.03, score=-1131.525, total=   0.0s\n",
            "[CV] alpha=0.1 .......................................................\n",
            "[CV] ....................... alpha=0.1, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=0.1 .......................................................\n",
            "[CV] ....................... alpha=0.1, score=-1139.180, total=   0.0s\n",
            "[CV] alpha=0.1 .......................................................\n",
            "[CV] ....................... alpha=0.1, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=0.1 .......................................................\n",
            "[CV] ....................... alpha=0.1, score=-1131.525, total=   0.0s\n",
            "[CV] alpha=0.3 .......................................................\n",
            "[CV] ....................... alpha=0.3, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=0.3 .......................................................\n",
            "[CV] ....................... alpha=0.3, score=-1139.179, total=   0.0s\n",
            "[CV] alpha=0.3 .......................................................\n",
            "[CV] ....................... alpha=0.3, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=0.3 .......................................................\n",
            "[CV] ....................... alpha=0.3, score=-1131.525, total=   0.0s\n",
            "[CV] alpha=1 .........................................................\n",
            "[CV] ......................... alpha=1, score=-1133.279, total=   0.0s\n",
            "[CV] alpha=1 .........................................................\n",
            "[CV] ......................... alpha=1, score=-1139.179, total=   0.0s\n",
            "[CV] alpha=1 .........................................................\n",
            "[CV] ......................... alpha=1, score=-1132.719, total=   0.0s\n",
            "[CV] alpha=1 .........................................................\n",
            "[CV] ......................... alpha=1, score=-1131.528, total=   0.0s\n",
            "[CV] alpha=3 .........................................................\n",
            "[CV] ......................... alpha=3, score=-1133.280, total=   0.0s\n",
            "[CV] alpha=3 .........................................................\n",
            "[CV] ......................... alpha=3, score=-1139.176, total=   0.0s\n",
            "[CV] alpha=3 .........................................................\n",
            "[CV] ......................... alpha=3, score=-1132.719, total=   0.0s\n",
            "[CV] alpha=3 .........................................................\n",
            "[CV] ......................... alpha=3, score=-1131.534, total=   0.0s\n",
            "[CV] alpha=10 ........................................................\n",
            "[CV] ........................ alpha=10, score=-1133.283, total=   0.0s\n",
            "[CV] alpha=10 ........................................................\n",
            "[CV] ........................ alpha=10, score=-1139.170, total=   0.0s\n",
            "[CV] alpha=10 ........................................................\n",
            "[CV] ........................ alpha=10, score=-1132.718, total=   0.0s\n",
            "[CV] alpha=10 ........................................................\n",
            "[CV] ........................ alpha=10, score=-1131.555, total=   0.0s\n",
            "[CV] alpha=15 ........................................................\n",
            "[CV] ........................ alpha=15, score=-1133.286, total=   0.0s\n",
            "[CV] alpha=15 ........................................................\n",
            "[CV] ........................ alpha=15, score=-1139.166, total=   0.0s\n",
            "[CV] alpha=15 ........................................................\n",
            "[CV] ........................ alpha=15, score=-1132.718, total=   0.0s\n",
            "[CV] alpha=15 ........................................................\n",
            "[CV] ........................ alpha=15, score=-1131.569, total=   0.0s\n",
            "[CV] alpha=20 ........................................................\n",
            "[CV] ........................ alpha=20, score=-1133.291, total=   0.0s\n",
            "[CV] alpha=20 ........................................................\n",
            "[CV] ........................ alpha=20, score=-1139.162, total=   0.0s\n",
            "[CV] alpha=20 ........................................................\n",
            "[CV] ........................ alpha=20, score=-1132.718, total=   0.0s\n",
            "[CV] alpha=20 ........................................................\n",
            "[CV] ........................ alpha=20, score=-1131.584, total=   0.0s\n",
            "[CV] alpha=25 ........................................................\n",
            "[CV] ........................ alpha=25, score=-1133.296, total=   0.0s\n",
            "[CV] alpha=25 ........................................................\n",
            "[CV] ........................ alpha=25, score=-1139.159, total=   0.0s\n",
            "[CV] alpha=25 ........................................................\n",
            "[CV] ........................ alpha=25, score=-1132.719, total=   0.0s\n",
            "[CV] alpha=25 ........................................................\n",
            "[CV] ........................ alpha=25, score=-1131.600, total=   0.0s\n",
            "[CV] alpha=30 ........................................................\n",
            "[CV] ........................ alpha=30, score=-1133.302, total=   0.0s\n",
            "[CV] alpha=30 ........................................................\n",
            "[CV] ........................ alpha=30, score=-1139.156, total=   0.0s\n",
            "[CV] alpha=30 ........................................................\n",
            "[CV] ........................ alpha=30, score=-1132.720, total=   0.0s\n",
            "[CV] alpha=30 ........................................................\n",
            "[CV] ........................ alpha=30, score=-1131.615, total=   0.0s\n",
            "[CV] alpha=35 ........................................................\n",
            "[CV] ........................ alpha=35, score=-1133.309, total=   0.0s\n",
            "[CV] alpha=35 ........................................................\n",
            "[CV] ........................ alpha=35, score=-1139.154, total=   0.0s\n",
            "[CV] alpha=35 ........................................................\n",
            "[CV] ........................ alpha=35, score=-1132.723, total=   0.0s\n",
            "[CV] alpha=35 ........................................................\n",
            "[CV] ........................ alpha=35, score=-1131.630, total=   0.0s\n",
            "[CV] alpha=40 ........................................................\n",
            "[CV] ........................ alpha=40, score=-1133.317, total=   0.0s\n",
            "[CV] alpha=40 ........................................................\n",
            "[CV] ........................ alpha=40, score=-1139.152, total=   0.0s\n",
            "[CV] alpha=40 ........................................................\n",
            "[CV] ........................ alpha=40, score=-1132.726, total=   0.0s\n",
            "[CV] alpha=40 ........................................................\n",
            "[CV] ........................ alpha=40, score=-1131.645, total=   0.0s\n",
            "[CV] alpha=45 ........................................................\n",
            "[CV] ........................ alpha=45, score=-1133.325, total=   0.0s\n",
            "[CV] alpha=45 ........................................................\n",
            "[CV] ........................ alpha=45, score=-1139.151, total=   0.0s\n",
            "[CV] alpha=45 ........................................................\n",
            "[CV] ........................ alpha=45, score=-1132.729, total=   0.0s\n",
            "[CV] alpha=45 ........................................................\n",
            "[CV] ........................ alpha=45, score=-1131.660, total=   0.0s\n",
            "[CV] alpha=50 ........................................................\n",
            "[CV] ........................ alpha=50, score=-1133.334, total=   0.0s\n",
            "[CV] alpha=50 ........................................................\n",
            "[CV] ........................ alpha=50, score=-1139.151, total=   0.0s\n",
            "[CV] alpha=50 ........................................................\n",
            "[CV] ........................ alpha=50, score=-1132.734, total=   0.0s\n",
            "[CV] alpha=50 ........................................................\n",
            "[CV] ........................ alpha=50, score=-1131.675, total=   0.0s\n",
            "[CV] alpha=55 ........................................................\n",
            "[CV] ........................ alpha=55, score=-1133.343, total=   0.0s\n",
            "[CV] alpha=55 ........................................................\n",
            "[CV] ........................ alpha=55, score=-1139.150, total=   0.0s\n",
            "[CV] alpha=55 ........................................................\n",
            "[CV] ........................ alpha=55, score=-1132.738, total=   0.0s\n",
            "[CV] alpha=55 ........................................................\n",
            "[CV] ........................ alpha=55, score=-1131.689, total=   0.0s\n",
            "[CV] alpha=60 ........................................................\n",
            "[CV] ........................ alpha=60, score=-1133.353, total=   0.0s\n",
            "[CV] alpha=60 ........................................................\n",
            "[CV] ........................ alpha=60, score=-1139.151, total=   0.0s\n",
            "[CV] alpha=60 ........................................................\n",
            "[CV] ........................ alpha=60, score=-1132.744, total=   0.0s\n",
            "[CV] alpha=60 ........................................................\n",
            "[CV] ........................ alpha=60, score=-1131.704, total=   0.0s\n",
            "[CV] alpha=70 ........................................................\n",
            "[CV] ........................ alpha=70, score=-1133.373, total=   0.0s\n",
            "[CV] alpha=70 ........................................................\n",
            "[CV] ........................ alpha=70, score=-1139.152, total=   0.0s\n",
            "[CV] alpha=70 ........................................................\n",
            "[CV] ........................ alpha=70, score=-1132.756, total=   0.0s\n",
            "[CV] alpha=70 ........................................................\n",
            "[CV] ........................ alpha=70, score=-1131.734, total=   0.0s\n",
            "[CV] alpha=100 .......................................................\n",
            "[CV] ....................... alpha=100, score=-1133.442, total=   0.0s\n",
            "[CV] alpha=100 .......................................................\n",
            "[CV] ....................... alpha=100, score=-1139.166, total=   0.0s\n",
            "[CV] alpha=100 .......................................................\n",
            "[CV] ....................... alpha=100, score=-1132.799, total=   0.0s\n",
            "[CV] alpha=100 .......................................................\n",
            "[CV] ....................... alpha=100, score=-1131.828, total=   0.0s\n",
            "params:\n",
            "\n",
            "[{'alpha': 0}, {'alpha': 1e-05}, {'alpha': 3e-05}, {'alpha': 0.0001}, {'alpha': 0.0003}, {'alpha': 0.001}, {'alpha': 0.003}, {'alpha': 0.01}, {'alpha': 0.03}, {'alpha': 0.1}, {'alpha': 0.3}, {'alpha': 1}, {'alpha': 3}, {'alpha': 10}, {'alpha': 15}, {'alpha': 20}, {'alpha': 25}, {'alpha': 30}, {'alpha': 35}, {'alpha': 40}, {'alpha': 45}, {'alpha': 50}, {'alpha': 55}, {'alpha': 60}, {'alpha': 70}, {'alpha': 100}]\n",
            "mean test scores:\n",
            "\n",
            "[-1268.03304875 -1134.1758033  -1134.1758033  -1134.17580333\n",
            " -1134.17580341 -1134.17580367 -1134.17580443 -1134.17580709\n",
            " -1134.17581475 -1134.17584162 -1134.17591863 -1134.17620129\n",
            " -1134.177128   -1134.18122205 -1134.1847643  -1134.18879893\n",
            " -1134.19327643 -1134.19827292 -1134.20383686 -1134.20986627\n",
            " -1134.21633422 -1134.2231839  -1134.23031141 -1134.23785171\n",
            " -1134.25373889 -1134.30915464]\n",
            "std test scores:\n",
            "\n",
            "[117.75337108   2.95776186   2.95776184   2.95776176   2.95776153\n",
            "   2.95776072   2.9577584    2.95775029   2.95772723   2.95764661\n",
            "   2.95741621   2.95661503   2.95437041   2.94671928   2.94144269\n",
            "   2.93626709   2.93106958   2.92585947   2.9208199    2.91592734\n",
            "   2.91106146   2.90629736   2.90159805   2.89699643   2.88810376\n",
            "   2.86266385]\n",
            "Best score: -1134.176\n",
            "Best parameters set: {'alpha': 1e-05}\n",
            "**********************************************\n",
            "best subsample: {'alpha': 1e-05}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 104 out of 104 | elapsed:    3.9s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
            "  \"removed in 0.24.\", FutureWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEIpw_CPBVRv",
        "colab_type": "code",
        "outputId": "447428d0-c537-4a5a-c74b-6cc589b1c1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "lgb_params = {'learning_rate': 0.05, 'metric': 'rmse',\n",
        "              'bagging_freq': 1, 'seed': 1234, 'objective': 'regression',\n",
        "              'num_leaves': 7, 'verbose': 1,\n",
        "              'max_depth': 5, 'min_gain_to_split': 0,\n",
        "              'feature_fraction': 0.1,\n",
        "              'bagging_fraction': 0.9,\n",
        "              'min_sum_hessian_in_leaf': 1,\n",
        "              'lambda_l2': 0, 'lambda_l1': 0,\n",
        "              'device':'gpu'\n",
        "              }\n",
        "\n",
        "train_blend_x = np.hstack((train_blend_x_xgb_ohe,\n",
        "                            train_blend_x_lgb_ohe,\n",
        "                            ))\n",
        "\n",
        "lgb_cv = lgb.cv(lgb_params,\n",
        "                lgb.Dataset(train_blend_x,\n",
        "                            label=train_y\n",
        "                            ),\n",
        "                num_boost_round=100000,\n",
        "                nfold=kfolds,\n",
        "                folds=skf.split(train_blend_x, train_y),\n",
        "                early_stopping_rounds=50,\n",
        "                verbose_eval=500)\n",
        "\n",
        "best_lgb_score = min(lgb_cv['rmse-mean'])\n",
        "best_lgb_iteration = len(lgb_cv['rmse-mean'])\n",
        "print(', best_score: %f, best_iteration: %d' %\n",
        "      (best_lgb_score, best_lgb_iteration))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[500]\tcv_agg's rmse: 0.476172 + 0.00237683\n",
            ", best_score: 0.476038, best_iteration: 674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLM19QPjEZpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdDks8UuFB7t",
        "colab_type": "text"
      },
      "source": [
        "## submission "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xojyny0MFDbz",
        "colab_type": "code",
        "outputId": "92826454-32f1-47de-f682-e74eff0ae928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_y_ridge = np.exp(model.predict(np.hstack((\n",
        "                                test_blend_x_xgb_ohe,\n",
        "                                test_blend_x_lgb_ohe,\n",
        ")))) - 200\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['id'] = full_data[train_size:].id\n",
        "results['loss'] = pred_y_ridge\n",
        "results.to_csv(\"/content/drive/My Drive/Kaggle_Allstate/data/sub_ridge_blended_gpu.csv\", index=False)\n",
        "print (\"Submission created.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submission created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqf3_Qp6FVqu",
        "colab_type": "code",
        "outputId": "3d98214c-235d-4b46-d4e4-5aa58e6f3a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_blend_x = np.hstack((test_blend_x_xgb_ohe,\n",
        "                          test_blend_x_lgb_ohe,\n",
        "                          ))\n",
        "\n",
        "model = lgb.train(lgb_params,\n",
        "            lgb.Dataset(train_blend_x,\n",
        "                        label=train_y\n",
        "                        ),\n",
        "            num_boost_round=best_lgb_iteration)\n",
        "preds_lgb = np.expm1(model.predict(test_blend_x))\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['id'] = full_data[train_size:].id\n",
        "results['loss'] = preds_lgb\n",
        "results.to_csv('/content/drive/My Drive/Kaggle_Allstate/data/sub_stacking_lgb_gpu.csv', index=False)\n",
        "print (\"Submission created.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submission created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV8OeBDvGQ7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUseTT6nGSI8",
        "colab_type": "text"
      },
      "source": [
        "## Final submisstion "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXIhX3UPGDG5",
        "colab_type": "code",
        "outputId": "69258239-4312-4c79-893e-d60ec5b85fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_y = pred_y_ridge*0.5 + preds_lgb*0.5\n",
        "\n",
        "results = pd.DataFrame()\n",
        "results['id'] = full_data[train_size:].id\n",
        "results['loss'] = pred_y\n",
        "results.to_csv(\"/content/drive/My Drive/Kaggle_Allstate/data/sub_final_gpu.csv\", index=False)\n",
        "print (\"Submission created.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submission created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1DC_iu0Gsvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}